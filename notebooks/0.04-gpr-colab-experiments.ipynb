{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dftDiCQg3vYy"
      },
      "source": [
        "# <font color='#4C5FDA'>**Breast Cancer Detection Based on CNNs Using Thermal Imaging** </font>\n",
        "\n",
        "Original paper by Juan Pablo Zuluaga, Zeina Al Masry, Khaled Benaggoune, Safa Meraghni & Noureddine Zerhouni: [A CNN-based methodology for breast cancer diagnosis using thermal images](https://www.tandfonline.com/doi/full/10.1080/21681163.2020.1824685)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uraq36CkXB1T",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Instalar paquetes necesarios**\n",
        "\n",
        "%%capture\n",
        "! pip install torchmetrics\n",
        "! pip install wandb -Uq\n",
        "# ! pip install onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#ECA702'>**Clonamos nuestro repo**</font>\n",
        "\n",
        "Esto con el fin de traer todos los .py para poder entrenar 'localmente' en Colab y registrar las métricas en wandb."
      ],
      "metadata": {
        "id": "3eUcjX2Lv5g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/gpintoruiz/Thermal-Imaging-Breast-Cancer-Detection.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miS2W9Tdwbxq",
        "outputId": "e0ce4052-32b8-4657-cc81-4b59a8f6d2f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Thermal-Imaging-Breast-Cancer-Detection'...\n",
            "remote: Enumerating objects: 216, done.\u001b[K\n",
            "remote: Counting objects: 100% (216/216), done.\u001b[K\n",
            "remote: Compressing objects: 100% (169/169), done.\u001b[K\n",
            "remote: Total 216 (delta 123), reused 113 (delta 45), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (216/216), 5.33 MiB | 22.74 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Thermal-Imaging-Breast-Cancer-Detection/notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMuOAySxx1Z1",
        "outputId": "6092390a-2041-4398-ec53-8680a254661c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssFb9A5GyA_u",
        "outputId": "a4fb6a16-2057-4626-d1c6-9b5014cdaa3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01-gpr-data-exploration-cv2.ipynb   alexnet.py       train_gkfold.py   validation.py\n",
            "0.01-gpr-data-exploration-pil.ipynb   make_dataset.py  train_one_run.py  vgg.py\n",
            "0.04-gpr-colab-experiments.ipynb      preprocess.py    train.py          xception-one-run.yaml\n",
            "1.00-gpr-xception-from-scratch.ipynb  test.py          utils.py          xception.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkrHeEps3vY3"
      },
      "source": [
        "## <font color='#ECA702'>**Configuración inicial para conectarnos con Kaggle**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJFg6k1x3vY5"
      },
      "source": [
        "1. Instalamos kaggle. Para poder usar comandos de Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hwqionQb3vY6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP3nl2Et3vY7"
      },
      "source": [
        "Subimos nuestro token de autenticación de Kaggle (si estamos en colab, sino colocarlo en la carpeta del proyecto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ARP6wZsb3vY8",
        "outputId": "504ed74c-4524-430a-95fb-898729c924b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3d4a4647-3b49-4e38-a666-29f632c52228\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3d4a4647-3b49-4e38-a666-29f632c52228\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"majinka10\",\"key\":\"3d279682cba1b9e369b0c57794d7135e\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0VPkGso3vY9"
      },
      "source": [
        "1. Creamos los directorios de Kaggle\n",
        "2. Copiamos nuestro token en .kaggle\n",
        "3. Con `chmod 600` establecemos los permitos del token en 600, es decir, que solo yo tengo permisos de lectura y escritura sobre el archivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ed2nCVTu3vY-"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UhQ_SI9x3vZA"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGkGIazz3vZB"
      },
      "source": [
        "## <font color='#ECA702'>**Carga del dataset**</font>\n",
        "\n",
        "Traemos el dataset [Thermal Images for Breast Cancer Diagnosis DMR-IR](https://www.kaggle.com/datasets/asdeepak/thermal-images-for-breast-cancer-diagnosis-dmrir) desde kaggle.\n",
        "\n",
        "This dataset is a methodology for breast disease computer-aided diagnosis using dynamic thermography. The thermal images for breast tumors are classified according to DMR-IR standards.\n",
        "\n",
        "Two types of tumors are classified in this dataset one is benign another is malignant.\n",
        "- Benign: This type of tumor is usually well-defined and round or oval in shape. (non-cancerous tumor)\n",
        "- Malignant: This type of tumor is usually poorly defined and irregular with lobules. (cancerous tumor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "lmT0aOvG3vZD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! kaggle datasets download -d asdeepak/thermal-images-for-breast-cancer-diagnosis-dmrir\n",
        "! unzip thermal-images-for-breast-cancer-diagnosis-dmrir.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QToIa8zB3vZE"
      },
      "source": [
        "Después de descargar los datos. Debemos entender la estructura de las carpetas para poder trabajar con ellas de una mejor manera.\n",
        "1. La carpeta principal `Imagens e Matrizes da Tese de Thiago Alves Elias da Silva` son todos los datos `data`.\n",
        "2. La carpeta `12 Novos Casos de Testes` la podemos tomar como nuestro conjunto de prueba (`test`).\n",
        "3. Mientras que la carpeta `Desenvolvimento da Metodologia` será nuestro conjunto de entrenamiento (`train`).\n",
        "\n",
        "Luego dentro de nuestras carpetas de `train` y `test` encontramos dos categorías `DOENTES`y `SAUDAтХа├╝VEIS` o SAUDÁVEI. Los primeros son los casos malignos y los segundos benignos.\n",
        "\n",
        "Dentro de cada una de las carpetas de pacientes saludables y enfermos se encuentran carpetas con números, cada número representa un paciente. Y para cada paciente tendremos dos carpetas más, una para las imágenes **segmentadas** en escala de grises y la otra para la matrix o mapa de calor.\n",
        "\n",
        "Algo bueno de este dataset es que ya está dividido por pacientes, es decir, no tendremos imagenes del mismo paciente en el conjunto de entrenamiento y testeo. Por lo tanto, vamos a entrenar con N pacientes, y testear con K pacientes, que no son los mismos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='#52F17F'>**Partición de los datos**</font>"
      ],
      "metadata": {
        "id": "gvB-mHCPqELO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este comando nos permite cargar funciones de un .py en el entorno local de Colab. [Fuente](https://stackoverflow.com/questions/47345004/in-googles-colab-notebook-how-do-i-call-a-function-from-a-python-file)"
      ],
      "metadata": {
        "id": "753-jlzfrNz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "execfile('make_dataset.py')"
      ],
      "metadata": {
        "id": "0jIUw7ntqHQU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2024)\n",
        "\n",
        "def print_fold_patients(folds: dict, data: pd.DataFrame):\n",
        "    for fold_name, indices in folds.items():\n",
        "        train_patients = data.iloc[indices['train']]['patient'].unique()\n",
        "        val_patients = data.iloc[indices['val']]['patient'].unique()\n",
        "        test_patients = data.iloc[indices['test']]['patient'].unique()\n",
        "\n",
        "        print(f\"{fold_name}:\\n\")\n",
        "        print(f\"Train patients: {train_patients}\")\n",
        "        print(f\"Validation patients: {val_patients}\")\n",
        "        print(f\"Test patients: {test_patients}\\n\")\n",
        "\n",
        "# Generar los datos\n",
        "data = make_dataframe()\n",
        "\n",
        "# Generar los folds\n",
        "folds = make_folds(data)\n",
        "\n",
        "# Imprimir los pacientes por cada fold\n",
        "print_fold_patients(folds, data)"
      ],
      "metadata": {
        "id": "wI2woWQCqxcO",
        "outputId": "e3cbba05-165a-450d-b46b-18cba03b5ce8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold_1:\n",
            "\n",
            "Train patients: ['48' '69' '14' '63' '65' '62' '66' '61' '64' '45' '44' '43' '11' '46'\n",
            " '15' '38' '42' '37' '13' '51' '28' '01' '07' '31' '00' '09' '53' '36'\n",
            " '59' '54' '35' '55' '49' '58' '05' '08' '32' '25' '26' '02']\n",
            "Validation patients: ['16' '40' '41' '39' '04' '24' '29' '56']\n",
            "Test patients: ['17' '34' '06' '52' '27' '03' '60' '30']\n",
            "\n",
            "fold_2:\n",
            "\n",
            "Train patients: ['14' '63' '65' '62' '66' '61' '64' '16' '45' '40' '17' '11' '46' '39'\n",
            " '15' '34' '38' '42' '13' '28' '01' '06' '31' '00' '09' '53' '36' '24'\n",
            " '54' '52' '55' '58' '05' '08' '32' '03' '60' '25' '30' '02']\n",
            "Validation patients: ['44' '43' '37' '04' '35' '49' '27' '29']\n",
            "Test patients: ['48' '69' '41' '51' '07' '59' '56' '26']\n",
            "\n",
            "fold_3:\n",
            "\n",
            "Train patients: ['48' '69' '14' '63' '62' '66' '61' '64' '45' '44' '17' '43' '41' '38'\n",
            " '37' '13' '51' '28' '06' '04' '07' '31' '00' '09' '53' '36' '24' '59'\n",
            " '35' '52' '55' '27' '05' '29' '32' '03' '60' '25' '26' '30']\n",
            "Validation patients: ['40' '11' '46' '39' '34' '54' '56' '02']\n",
            "Test patients: ['65' '16' '15' '42' '01' '49' '58' '08']\n",
            "\n",
            "fold_4:\n",
            "\n",
            "Train patients: ['69' '14' '63' '62' '61' '16' '44' '40' '17' '11' '46' '39' '15' '34'\n",
            " '38' '42' '51' '28' '01' '06' '04' '07' '00' '36' '24' '59' '35' '52'\n",
            " '49' '58' '05' '08' '29' '32' '03' '60' '56' '26' '30' '02']\n",
            "Validation patients: ['48' '65' '45' '41' '31' '53' '54' '27']\n",
            "Test patients: ['66' '64' '43' '37' '13' '09' '55' '25']\n",
            "\n",
            "fold_5:\n",
            "\n",
            "Train patients: ['48' '69' '14' '65' '66' '61' '64' '16' '45' '40' '17' '41' '39' '15'\n",
            " '34' '42' '37' '13' '51' '28' '01' '06' '04' '07' '00' '09' '53' '52'\n",
            " '55' '49' '27' '58' '05' '32' '03' '60' '56' '26' '30' '02']\n",
            "Validation patients: ['62' '43' '46' '36' '59' '54' '08' '25']\n",
            "Test patients: ['63' '44' '11' '38' '31' '24' '35' '29']\n",
            "\n",
            "fold_6:\n",
            "\n",
            "Train patients: ['48' '69' '63' '66' '61' '64' '16' '44' '17' '43' '11' '46' '41' '15'\n",
            " '38' '42' '37' '13' '01' '06' '31' '00' '09' '53' '36' '24' '59' '35'\n",
            " '52' '55' '27' '58' '08' '29' '60' '56' '25' '26' '30' '02']\n",
            "Validation patients: ['65' '40' '34' '51' '04' '07' '49' '03']\n",
            "Test patients: ['14' '62' '45' '39' '28' '54' '05' '32']\n",
            "\n",
            "fold_7:\n",
            "\n",
            "Train patients: ['48' '69' '14' '63' '65' '62' '66' '16' '45' '17' '43' '11' '41' '39'\n",
            " '15' '34' '38' '42' '37' '13' '06' '07' '31' '09' '24' '59' '54' '52'\n",
            " '55' '49' '27' '58' '05' '08' '29' '03' '60' '25' '26' '30']\n",
            "Validation patients: ['64' '44' '51' '28' '01' '35' '32' '56']\n",
            "Test patients: ['61' '40' '46' '04' '00' '53' '36' '02']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#ECA702'>**Inicializamos el agende de wandb**</font>"
      ],
      "metadata": {
        "id": "5YTKbsdDyqSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='#52F17F'>**1. Nos logeamos en nuestra cuenta**</font>"
      ],
      "metadata": {
        "id": "nIskv2fHy4Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "St3agN3Rypzq",
        "outputId": "f2cfd437-0a23-4399-c65e-b0e2a89d916b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='#52F17F'>**2. Hacemos call del agente**</font>\n"
      ],
      "metadata": {
        "id": "w0r-E8Ufy9TQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El sweep que estoy probando acá es el [siguiente](https://github.com/gpintoruiz/Thermal-Imaging-Breast-Cancer-Detection/blob/main/notebooks/xception-one-run.yaml). Se pueden cambiar los parámetros a probar como tú quieras de acuerdo con la [documentación](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration) (recomiendo solo cambiar la arquitectura para que las comparaciones entre modelos sean equivalentes).\n",
        "\n",
        "Si no tienes ni idea qué es un sweep mira el siguiente [tutorial](https://www.youtube.com/watch?v=9zrmUIlScdY&t=1361s&ab_channel=Weights%26Biases).\n",
        "\n",
        "El comando `--count` sirve para decirle al agente cuántos runs hacer, aplica especialmente cuando el método del sweep es `bayes` o `random`\n"
      ],
      "metadata": {
        "id": "LxtyncVoCVL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wandb agent aiuis/dip-project/27e1qtt0 --count 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpG6dTZdy0R3",
        "outputId": "5f5065e3-c9e7-4aba-ede8-36c98a3abb7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
            "2024-06-10 19:56:19,681 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2024-06-10 19:56:20,735 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 19:56:20,735 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 8\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.006106816662230798\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: 150\n",
            "2024-06-10 19:56:20,737 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=8 --crop=True --learning_rate=0.006106816662230798 --normalize=True --optimizer=sgd --resize=150\n",
            "2024-06-10 19:56:25,748 - wandb.wandb_agent - INFO - Running runs: ['i159n1q3']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_195630-i159n1q3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcelestial-sweep-28\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/i159n1q3\u001b[0m\n",
            "FOLD 2\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "train loss: 0.783 accuracy: 0.507 [after 33 batches]\n",
            "train loss: 0.714 accuracy: 0.577 [after 67 batches]\n",
            "train loss: 0.637 accuracy: 0.637 [after 101 batches]\n",
            "train loss: 0.573 accuracy: 0.689 [after 135 batches]\n",
            "val loss: 0.314 accuracy: 0.902 [after 28 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.379 accuracy: 0.853 [after 33 batches]\n",
            "train loss: 0.367 accuracy: 0.851 [after 67 batches]\n",
            "train loss: 0.387 accuracy: 0.854 [after 101 batches]\n",
            "train loss: 0.402 accuracy: 0.856 [after 135 batches]\n",
            "val loss: 0.253 accuracy: 0.902 [after 28 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.405 accuracy: 0.842 [after 33 batches]\n",
            "train loss: 0.417 accuracy: 0.858 [after 67 batches]\n",
            "train loss: 0.416 accuracy: 0.854 [after 101 batches]\n",
            "train loss: 0.397 accuracy: 0.855 [after 135 batches]\n",
            "val loss: 0.313 accuracy: 0.897 [after 28 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.289 accuracy: 0.890 [after 33 batches]\n",
            "train loss: 0.349 accuracy: 0.888 [after 67 batches]\n",
            "train loss: 0.314 accuracy: 0.896 [after 101 batches]\n",
            "train loss: 0.298 accuracy: 0.895 [after 135 batches]\n",
            "val loss: 0.314 accuracy: 0.924 [after 28 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.319 accuracy: 0.886 [after 33 batches]\n",
            "train loss: 0.331 accuracy: 0.888 [after 67 batches]\n",
            "train loss: 0.320 accuracy: 0.890 [after 101 batches]\n",
            "train loss: 0.295 accuracy: 0.894 [after 135 batches]\n",
            "val loss: 0.213 accuracy: 0.946 [after 28 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.180 accuracy: 0.952 [after 33 batches]\n",
            "train loss: 0.242 accuracy: 0.934 [after 67 batches]\n",
            "train loss: 0.246 accuracy: 0.931 [after 101 batches]\n",
            "train loss: 0.252 accuracy: 0.924 [after 135 batches]\n",
            "val loss: 0.525 accuracy: 0.920 [after 28 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.236 accuracy: 0.923 [after 33 batches]\n",
            "train loss: 0.231 accuracy: 0.926 [after 67 batches]\n",
            "train loss: 0.210 accuracy: 0.935 [after 101 batches]\n",
            "train loss: 0.217 accuracy: 0.943 [after 135 batches]\n",
            "val loss: 0.060 accuracy: 0.982 [after 28 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.158 accuracy: 0.960 [after 33 batches]\n",
            "train loss: 0.124 accuracy: 0.963 [after 67 batches]\n",
            "train loss: 0.155 accuracy: 0.947 [after 101 batches]\n",
            "train loss: 0.147 accuracy: 0.951 [after 135 batches]\n",
            "val loss: 0.225 accuracy: 0.879 [after 28 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.268 accuracy: 0.893 [after 33 batches]\n",
            "train loss: 0.241 accuracy: 0.915 [after 67 batches]\n",
            "train loss: 0.207 accuracy: 0.929 [after 101 batches]\n",
            "train loss: 0.188 accuracy: 0.941 [after 135 batches]\n",
            "val loss: 0.117 accuracy: 0.982 [after 28 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.056 accuracy: 0.982 [after 33 batches]\n",
            "train loss: 0.063 accuracy: 0.982 [after 67 batches]\n",
            "train loss: 0.081 accuracy: 0.978 [after 101 batches]\n",
            "train loss: 0.074 accuracy: 0.980 [after 135 batches]\n",
            "val loss: 0.228 accuracy: 0.946 [after 28 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.020 accuracy: 0.996 [after 33 batches]\n",
            "train loss: 0.021 accuracy: 0.994 [after 67 batches]\n",
            "train loss: 0.057 accuracy: 0.983 [after 101 batches]\n",
            "train loss: 0.082 accuracy: 0.972 [after 135 batches]\n",
            "val loss: 0.035 accuracy: 1.000 [after 28 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.193 accuracy: 0.967 [after 33 batches]\n",
            "train loss: 0.123 accuracy: 0.978 [after 67 batches]\n",
            "train loss: 0.093 accuracy: 0.982 [after 101 batches]\n",
            "train loss: 0.078 accuracy: 0.985 [after 135 batches]\n",
            "val loss: 0.293 accuracy: 0.888 [after 28 batches]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "train loss: 0.140 accuracy: 0.974 [after 33 batches]\n",
            "train loss: 0.091 accuracy: 0.982 [after 67 batches]\n",
            "train loss: 0.074 accuracy: 0.984 [after 101 batches]\n",
            "train loss: 0.081 accuracy: 0.978 [after 135 batches]\n",
            "val loss: 0.445 accuracy: 0.853 [after 28 batches]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "train loss: 0.221 accuracy: 0.971 [after 33 batches]\n",
            "train loss: 0.160 accuracy: 0.963 [after 67 batches]\n",
            "train loss: 0.132 accuracy: 0.969 [after 101 batches]\n",
            "train loss: 0.113 accuracy: 0.971 [after 135 batches]\n",
            "val loss: 0.095 accuracy: 0.942 [after 28 batches]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "train loss: 0.047 accuracy: 0.989 [after 33 batches]\n",
            "train loss: 0.119 accuracy: 0.985 [after 67 batches]\n",
            "train loss: 0.122 accuracy: 0.979 [after 101 batches]\n",
            "train loss: 0.129 accuracy: 0.977 [after 135 batches]\n",
            "val loss: 0.138 accuracy: 0.938 [after 28 batches]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "train loss: 0.202 accuracy: 0.952 [after 33 batches]\n",
            "train loss: 0.148 accuracy: 0.967 [after 67 batches]\n",
            "train loss: 0.104 accuracy: 0.978 [after 101 batches]\n",
            "train loss: 0.115 accuracy: 0.980 [after 135 batches]\n",
            "val loss: 0.004 accuracy: 1.000 [after 28 batches]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "train loss: 0.064 accuracy: 0.974 [after 33 batches]\n",
            "train loss: 0.057 accuracy: 0.982 [after 67 batches]\n",
            "train loss: 0.055 accuracy: 0.984 [after 101 batches]\n",
            "train loss: 0.052 accuracy: 0.982 [after 135 batches]\n",
            "val loss: 0.232 accuracy: 0.924 [after 28 batches]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "train loss: 0.028 accuracy: 0.996 [after 33 batches]\n",
            "train loss: 0.025 accuracy: 0.994 [after 67 batches]\n",
            "train loss: 0.018 accuracy: 0.996 [after 101 batches]\n",
            "train loss: 0.070 accuracy: 0.994 [after 135 batches]\n",
            "val loss: 0.131 accuracy: 0.960 [after 28 batches]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "train loss: 0.040 accuracy: 0.989 [after 33 batches]\n",
            "train loss: 0.022 accuracy: 0.994 [after 67 batches]\n",
            "train loss: 0.017 accuracy: 0.996 [after 101 batches]\n",
            "train loss: 0.029 accuracy: 0.994 [after 135 batches]\n",
            "val loss: 0.011 accuracy: 0.991 [after 28 batches]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "train loss: 0.018 accuracy: 0.993 [after 33 batches]\n",
            "train loss: 0.037 accuracy: 0.989 [after 67 batches]\n",
            "train loss: 0.074 accuracy: 0.982 [after 101 batches]\n",
            "train loss: 0.067 accuracy: 0.983 [after 135 batches]\n",
            "val loss: 0.102 accuracy: 0.946 [after 28 batches]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "train loss: 0.029 accuracy: 0.989 [after 33 batches]\n",
            "train loss: 0.021 accuracy: 0.993 [after 67 batches]\n",
            "train loss: 0.016 accuracy: 0.995 [after 101 batches]\n",
            "train loss: 0.014 accuracy: 0.996 [after 135 batches]\n",
            "val loss: 0.008 accuracy: 0.996 [after 28 batches]\n",
            "test accuracy: 0.911 recall: 0.801 precision: 1.000 f1: 0.878 [after 28 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▆▆▆▆▆▇▆▇▇▇▇▇█▇▇███████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▄▄▅▅▄▄▄▄▃▃▃▃▂▂▃▁▂▁▁▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▃▃▃▄▅▄▇▂▇▅█▃▁▅▅█▄▆█▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▅▄▅▅▄█▂▄▃▄▁▅▇▂▃▁▄▃▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 83\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.91071\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.87754\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.80119\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.99632\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.01418\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.99554\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.00806\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcelestial-sweep-28\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/i159n1q3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_195630-i159n1q3/logs\u001b[0m\n",
            "2024-06-10 20:17:32,447 - wandb.wandb_agent - INFO - Cleaning up finished run: i159n1q3\n",
            "2024-06-10 20:17:34,757 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 20:17:34,757 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 64\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.0026621555913423646\n",
            "\tnormalize: False\n",
            "\toptimizer: sgd\n",
            "\tresize: 50\n",
            "2024-06-10 20:17:34,760 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=64 --crop=True --learning_rate=0.0026621555913423646 --normalize=False --optimizer=sgd --resize=50\n",
            "2024-06-10 20:17:39,773 - wandb.wandb_agent - INFO - Running runs: ['28grinqn']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_201740-28grinqn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcolorful-sweep-35\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/28grinqn\u001b[0m\n",
            "FOLD 2\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.695 accuracy: 0.500 [after 3 batches]\n",
            "train loss: 0.681 accuracy: 0.521 [after 7 batches]\n",
            "train loss: 0.687 accuracy: 0.512 [after 11 batches]\n",
            "train loss: 0.675 accuracy: 0.546 [after 15 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "val loss: 0.689 accuracy: 0.554 [after 4 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.592 accuracy: 0.652 [after 3 batches]\n",
            "train loss: 0.578 accuracy: 0.660 [after 7 batches]\n",
            "train loss: 0.552 accuracy: 0.682 [after 11 batches]\n",
            "train loss: 0.544 accuracy: 0.675 [after 15 batches]\n",
            "val loss: 0.694 accuracy: 0.564 [after 4 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.488 accuracy: 0.691 [after 3 batches]\n",
            "train loss: 0.476 accuracy: 0.695 [after 7 batches]\n",
            "train loss: 0.462 accuracy: 0.702 [after 11 batches]\n",
            "train loss: 0.460 accuracy: 0.707 [after 15 batches]\n",
            "val loss: 0.800 accuracy: 0.549 [after 4 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.465 accuracy: 0.727 [after 3 batches]\n",
            "train loss: 0.489 accuracy: 0.701 [after 7 batches]\n",
            "train loss: 0.517 accuracy: 0.688 [after 11 batches]\n",
            "train loss: 0.507 accuracy: 0.688 [after 15 batches]\n",
            "val loss: 0.218 accuracy: 0.919 [after 4 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.492 accuracy: 0.668 [after 3 batches]\n",
            "train loss: 0.509 accuracy: 0.686 [after 7 batches]\n",
            "train loss: 0.509 accuracy: 0.682 [after 11 batches]\n",
            "train loss: 0.512 accuracy: 0.674 [after 15 batches]\n",
            "val loss: 0.229 accuracy: 0.894 [after 4 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.464 accuracy: 0.695 [after 3 batches]\n",
            "train loss: 0.452 accuracy: 0.721 [after 7 batches]\n",
            "train loss: 0.431 accuracy: 0.734 [after 11 batches]\n",
            "train loss: 0.431 accuracy: 0.744 [after 15 batches]\n",
            "val loss: 0.243 accuracy: 0.908 [after 4 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.510 accuracy: 0.664 [after 3 batches]\n",
            "train loss: 0.468 accuracy: 0.709 [after 7 batches]\n",
            "train loss: 0.472 accuracy: 0.712 [after 11 batches]\n",
            "train loss: 0.457 accuracy: 0.721 [after 15 batches]\n",
            "val loss: 0.285 accuracy: 0.909 [after 4 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.410 accuracy: 0.742 [after 3 batches]\n",
            "train loss: 0.423 accuracy: 0.738 [after 7 batches]\n",
            "train loss: 0.446 accuracy: 0.723 [after 11 batches]\n",
            "train loss: 0.437 accuracy: 0.725 [after 15 batches]\n",
            "val loss: 0.267 accuracy: 0.911 [after 4 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.454 accuracy: 0.719 [after 3 batches]\n",
            "train loss: 0.456 accuracy: 0.703 [after 7 batches]\n",
            "train loss: 0.444 accuracy: 0.708 [after 11 batches]\n",
            "train loss: 0.467 accuracy: 0.699 [after 15 batches]\n",
            "val loss: 0.293 accuracy: 0.922 [after 4 batches]\n",
            "test accuracy: 0.820 recall: 0.597 precision: 1.000 f1: 0.742 [after 4 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▄▅▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▁▂▅▆▆▆▆▇▇▇▇▇▆▆▆▆▆▆▇▇██▆▇▇▇██▇▇▇▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ████▅▅▄▄▃▃▂▂▂▃▄▃▃▃▃▃▂▂▂▂▃▂▃▂▁▁▂▂▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▁█▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▇▇█▁▁▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.81975\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.74193\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.59731\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.69922\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.46652\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.92188\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.29284\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcolorful-sweep-35\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/28grinqn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_201740-28grinqn/logs\u001b[0m\n",
            "2024-06-10 20:25:16,117 - wandb.wandb_agent - INFO - Cleaning up finished run: 28grinqn\n",
            "2024-06-10 20:25:17,699 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 20:25:17,699 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 16\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.0036878942619798087\n",
            "\tnormalize: False\n",
            "\toptimizer: adam\n",
            "\tresize: 50\n",
            "2024-06-10 20:25:17,701 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=16 --crop=True --learning_rate=0.0036878942619798087 --normalize=False --optimizer=adam --resize=50\n",
            "2024-06-10 20:25:22,711 - wandb.wandb_agent - INFO - Running runs: ['f4rdqov2']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_202524-f4rdqov2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mearnest-sweep-37\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/f4rdqov2\u001b[0m\n",
            "FOLD 4\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "train loss: 1.144 accuracy: 0.445 [after 16 batches]\n",
            "train loss: 0.967 accuracy: 0.469 [after 33 batches]\n",
            "train loss: 0.885 accuracy: 0.490 [after 50 batches]\n",
            "train loss: 0.868 accuracy: 0.482 [after 67 batches]\n",
            "val loss: 0.687 accuracy: 0.542 [after 14 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.774 accuracy: 0.485 [after 16 batches]\n",
            "train loss: 0.775 accuracy: 0.485 [after 33 batches]\n",
            "train loss: 0.751 accuracy: 0.493 [after 50 batches]\n",
            "train loss: 0.742 accuracy: 0.491 [after 67 batches]\n",
            "val loss: 0.724 accuracy: 0.545 [after 14 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.719 accuracy: 0.533 [after 16 batches]\n",
            "train loss: 0.709 accuracy: 0.517 [after 33 batches]\n",
            "train loss: 0.701 accuracy: 0.520 [after 50 batches]\n",
            "train loss: 0.707 accuracy: 0.522 [after 67 batches]\n",
            "val loss: 0.695 accuracy: 0.573 [after 14 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.693 accuracy: 0.533 [after 16 batches]\n",
            "train loss: 0.699 accuracy: 0.524 [after 33 batches]\n",
            "train loss: 0.705 accuracy: 0.504 [after 50 batches]\n",
            "train loss: 0.701 accuracy: 0.503 [after 67 batches]\n",
            "val loss: 0.687 accuracy: 0.543 [after 14 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.713 accuracy: 0.500 [after 16 batches]\n",
            "train loss: 0.706 accuracy: 0.513 [after 33 batches]\n",
            "train loss: 0.704 accuracy: 0.498 [after 50 batches]\n",
            "train loss: 0.705 accuracy: 0.513 [after 67 batches]\n",
            "val loss: 0.697 accuracy: 0.557 [after 14 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.699 accuracy: 0.562 [after 16 batches]\n",
            "train loss: 0.698 accuracy: 0.537 [after 33 batches]\n",
            "train loss: 0.697 accuracy: 0.523 [after 50 batches]\n",
            "train loss: 0.697 accuracy: 0.515 [after 67 batches]\n",
            "val loss: 0.700 accuracy: 0.542 [after 14 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.692 accuracy: 0.518 [after 16 batches]\n",
            "train loss: 0.691 accuracy: 0.542 [after 33 batches]\n",
            "train loss: 0.690 accuracy: 0.544 [after 50 batches]\n",
            "train loss: 0.691 accuracy: 0.542 [after 67 batches]\n",
            "val loss: 0.698 accuracy: 0.549 [after 14 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.684 accuracy: 0.559 [after 16 batches]\n",
            "train loss: 0.682 accuracy: 0.570 [after 33 batches]\n",
            "train loss: 0.687 accuracy: 0.558 [after 50 batches]\n",
            "train loss: 0.685 accuracy: 0.564 [after 67 batches]\n",
            "val loss: 0.967 accuracy: 0.552 [after 14 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.655 accuracy: 0.621 [after 16 batches]\n",
            "train loss: 0.648 accuracy: 0.625 [after 33 batches]\n",
            "train loss: 0.662 accuracy: 0.607 [after 50 batches]\n",
            "train loss: 0.652 accuracy: 0.624 [after 67 batches]\n",
            "val loss: 0.627 accuracy: 0.786 [after 14 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.717 accuracy: 0.651 [after 16 batches]\n",
            "train loss: 0.667 accuracy: 0.665 [after 33 batches]\n",
            "train loss: 0.659 accuracy: 0.643 [after 50 batches]\n",
            "train loss: 0.652 accuracy: 0.651 [after 67 batches]\n",
            "val loss: 0.509 accuracy: 0.798 [after 14 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.598 accuracy: 0.702 [after 16 batches]\n",
            "train loss: 0.608 accuracy: 0.682 [after 33 batches]\n",
            "train loss: 0.611 accuracy: 0.680 [after 50 batches]\n",
            "train loss: 0.600 accuracy: 0.690 [after 67 batches]\n",
            "val loss: 0.324 accuracy: 0.996 [after 14 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.597 accuracy: 0.695 [after 16 batches]\n",
            "train loss: 0.593 accuracy: 0.691 [after 33 batches]\n",
            "train loss: 0.603 accuracy: 0.689 [after 50 batches]\n",
            "train loss: 0.613 accuracy: 0.673 [after 67 batches]\n",
            "val loss: 0.357 accuracy: 0.982 [after 14 batches]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "train loss: 0.601 accuracy: 0.684 [after 16 batches]\n",
            "train loss: 0.594 accuracy: 0.682 [after 33 batches]\n",
            "train loss: 0.590 accuracy: 0.686 [after 50 batches]\n",
            "train loss: 0.582 accuracy: 0.688 [after 67 batches]\n",
            "val loss: 0.586 accuracy: 0.638 [after 14 batches]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "train loss: 0.582 accuracy: 0.669 [after 16 batches]\n",
            "train loss: 0.611 accuracy: 0.664 [after 33 batches]\n",
            "train loss: 0.609 accuracy: 0.676 [after 50 batches]\n",
            "train loss: 0.604 accuracy: 0.681 [after 67 batches]\n",
            "val loss: 0.398 accuracy: 0.882 [after 14 batches]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "train loss: 0.571 accuracy: 0.713 [after 16 batches]\n",
            "train loss: 0.577 accuracy: 0.710 [after 33 batches]\n",
            "train loss: 0.570 accuracy: 0.707 [after 50 batches]\n",
            "train loss: 0.562 accuracy: 0.719 [after 67 batches]\n",
            "val loss: 0.402 accuracy: 0.905 [after 14 batches]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "train loss: 0.598 accuracy: 0.691 [after 16 batches]\n",
            "train loss: 0.607 accuracy: 0.676 [after 33 batches]\n",
            "train loss: 0.596 accuracy: 0.690 [after 50 batches]\n",
            "train loss: 0.581 accuracy: 0.704 [after 67 batches]\n",
            "val loss: 0.291 accuracy: 0.927 [after 14 batches]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "train loss: 0.546 accuracy: 0.713 [after 16 batches]\n",
            "train loss: 0.573 accuracy: 0.693 [after 33 batches]\n",
            "train loss: 0.585 accuracy: 0.680 [after 50 batches]\n",
            "train loss: 0.590 accuracy: 0.678 [after 67 batches]\n",
            "val loss: 0.355 accuracy: 0.911 [after 14 batches]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "train loss: 0.595 accuracy: 0.688 [after 16 batches]\n",
            "train loss: 0.613 accuracy: 0.664 [after 33 batches]\n",
            "train loss: 0.593 accuracy: 0.681 [after 50 batches]\n",
            "train loss: 0.592 accuracy: 0.688 [after 67 batches]\n",
            "val loss: 0.403 accuracy: 0.905 [after 14 batches]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "train loss: 0.546 accuracy: 0.735 [after 16 batches]\n",
            "train loss: 0.570 accuracy: 0.700 [after 33 batches]\n",
            "train loss: 0.572 accuracy: 0.697 [after 50 batches]\n",
            "train loss: 0.568 accuracy: 0.703 [after 67 batches]\n",
            "val loss: 1.791 accuracy: 0.570 [after 14 batches]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "train loss: 0.607 accuracy: 0.713 [after 16 batches]\n",
            "train loss: 0.600 accuracy: 0.693 [after 33 batches]\n",
            "train loss: 0.605 accuracy: 0.683 [after 50 batches]\n",
            "train loss: 0.597 accuracy: 0.689 [after 67 batches]\n",
            "val loss: 0.471 accuracy: 0.795 [after 14 batches]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "train loss: 0.582 accuracy: 0.699 [after 16 batches]\n",
            "train loss: 0.588 accuracy: 0.693 [after 33 batches]\n",
            "train loss: 0.599 accuracy: 0.685 [after 50 batches]\n",
            "train loss: 0.602 accuracy: 0.678 [after 67 batches]\n",
            "val loss: 0.313 accuracy: 0.978 [after 14 batches]\n",
            "test accuracy: 0.737 recall: 0.798 precision: 0.686 f1: 0.725 [after 14 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▂▂▃▃▃▂▃▃▃▃▃▃▄▄▅▆▆▇▇▇▇▇▇▆▇▇█▇▇▇▇▇█▇▇▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▁▂▂▁▁▂▁▁▂▂▁▁▂▂▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▁▁▁▁▁▁▅▅██▂▆▇▇▇▇▁▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▃▃▃▃▃▃▃▄▃▂▁▁▂▁▂▁▁▂█▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 83\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.73661\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.72528\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.68616\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.79807\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.67794\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.60227\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.97768\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.31332\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mearnest-sweep-37\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/f4rdqov2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_202524-f4rdqov2/logs\u001b[0m\n",
            "2024-06-10 20:44:24,790 - wandb.wandb_agent - INFO - Cleaning up finished run: f4rdqov2\n",
            "2024-06-10 20:44:27,179 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 20:44:27,179 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 64\n",
            "\tcrop: False\n",
            "\tlearning_rate: 0.002687398380446804\n",
            "\tnormalize: False\n",
            "\toptimizer: sgd\n",
            "\tresize: 150\n",
            "2024-06-10 20:44:27,181 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=64 --crop=False --learning_rate=0.002687398380446804 --normalize=False --optimizer=sgd --resize=150\n",
            "2024-06-10 20:44:32,191 - wandb.wandb_agent - INFO - Running runs: ['kfdbbtyr']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_204433-kfdbbtyr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meager-sweep-42\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/kfdbbtyr\u001b[0m\n",
            "FOLD 7\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.692 accuracy: 0.523 [after 3 batches]\n",
            "train loss: 0.687 accuracy: 0.514 [after 7 batches]\n",
            "train loss: 0.681 accuracy: 0.514 [after 11 batches]\n",
            "train loss: 0.668 accuracy: 0.547 [after 15 batches]\n",
            "val loss: 0.685 accuracy: 0.663 [after 4 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.619 accuracy: 0.613 [after 3 batches]\n",
            "train loss: 0.624 accuracy: 0.588 [after 7 batches]\n",
            "train loss: 0.619 accuracy: 0.598 [after 11 batches]\n",
            "train loss: 0.619 accuracy: 0.604 [after 15 batches]\n",
            "val loss: 0.752 accuracy: 0.668 [after 4 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.547 accuracy: 0.680 [after 3 batches]\n",
            "train loss: 0.539 accuracy: 0.678 [after 7 batches]\n",
            "train loss: 0.561 accuracy: 0.635 [after 11 batches]\n",
            "train loss: 0.553 accuracy: 0.651 [after 15 batches]\n",
            "val loss: 0.907 accuracy: 0.092 [after 4 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.551 accuracy: 0.648 [after 3 batches]\n",
            "train loss: 0.530 accuracy: 0.648 [after 7 batches]\n",
            "train loss: 0.542 accuracy: 0.648 [after 11 batches]\n",
            "train loss: 0.527 accuracy: 0.668 [after 15 batches]\n",
            "val loss: 3.231 accuracy: 0.396 [after 4 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.486 accuracy: 0.711 [after 3 batches]\n",
            "train loss: 0.513 accuracy: 0.684 [after 7 batches]\n",
            "train loss: 0.504 accuracy: 0.681 [after 11 batches]\n",
            "train loss: 0.497 accuracy: 0.688 [after 15 batches]\n",
            "val loss: 0.290 accuracy: 0.862 [after 4 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.556 accuracy: 0.637 [after 3 batches]\n",
            "train loss: 0.555 accuracy: 0.646 [after 7 batches]\n",
            "train loss: 0.523 accuracy: 0.671 [after 11 batches]\n",
            "train loss: 0.524 accuracy: 0.674 [after 15 batches]\n",
            "val loss: 0.440 accuracy: 0.828 [after 4 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.430 accuracy: 0.730 [after 3 batches]\n",
            "train loss: 0.460 accuracy: 0.699 [after 7 batches]\n",
            "train loss: 0.487 accuracy: 0.682 [after 11 batches]\n",
            "train loss: 0.474 accuracy: 0.699 [after 15 batches]\n",
            "val loss: 4.841 accuracy: 0.411 [after 4 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.467 accuracy: 0.707 [after 3 batches]\n",
            "train loss: 0.452 accuracy: 0.729 [after 7 batches]\n",
            "train loss: 0.453 accuracy: 0.733 [after 11 batches]\n",
            "train loss: 0.450 accuracy: 0.729 [after 15 batches]\n",
            "val loss: 1.711 accuracy: 0.508 [after 4 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.435 accuracy: 0.719 [after 3 batches]\n",
            "train loss: 0.438 accuracy: 0.711 [after 7 batches]\n",
            "train loss: 0.434 accuracy: 0.711 [after 11 batches]\n",
            "train loss: 0.430 accuracy: 0.709 [after 15 batches]\n",
            "val loss: 0.405 accuracy: 0.858 [after 4 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.391 accuracy: 0.703 [after 3 batches]\n",
            "train loss: 0.400 accuracy: 0.703 [after 7 batches]\n",
            "train loss: 0.411 accuracy: 0.688 [after 11 batches]\n",
            "train loss: 0.413 accuracy: 0.705 [after 15 batches]\n",
            "val loss: 0.258 accuracy: 0.915 [after 4 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.399 accuracy: 0.746 [after 3 batches]\n",
            "train loss: 0.404 accuracy: 0.752 [after 7 batches]\n",
            "train loss: 0.393 accuracy: 0.742 [after 11 batches]\n",
            "train loss: 0.380 accuracy: 0.757 [after 15 batches]\n",
            "val loss: 0.313 accuracy: 0.918 [after 4 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.386 accuracy: 0.758 [after 3 batches]\n",
            "train loss: 0.393 accuracy: 0.750 [after 7 batches]\n",
            "train loss: 0.395 accuracy: 0.746 [after 11 batches]\n",
            "train loss: 0.398 accuracy: 0.735 [after 15 batches]\n",
            "val loss: 0.415 accuracy: 0.917 [after 4 batches]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "train loss: 0.423 accuracy: 0.707 [after 3 batches]\n",
            "train loss: 0.412 accuracy: 0.697 [after 7 batches]\n",
            "train loss: 0.408 accuracy: 0.707 [after 11 batches]\n",
            "train loss: 0.420 accuracy: 0.700 [after 15 batches]\n",
            "val loss: 0.565 accuracy: 0.913 [after 4 batches]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "train loss: 0.393 accuracy: 0.750 [after 3 batches]\n",
            "train loss: 0.398 accuracy: 0.730 [after 7 batches]\n",
            "train loss: 0.393 accuracy: 0.719 [after 11 batches]\n",
            "train loss: 0.393 accuracy: 0.723 [after 15 batches]\n",
            "val loss: 0.507 accuracy: 0.814 [after 4 batches]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "train loss: 0.413 accuracy: 0.719 [after 3 batches]\n",
            "train loss: 0.399 accuracy: 0.738 [after 7 batches]\n",
            "train loss: 0.409 accuracy: 0.738 [after 11 batches]\n",
            "train loss: 0.416 accuracy: 0.725 [after 15 batches]\n",
            "val loss: 1.284 accuracy: 0.812 [after 4 batches]\n",
            "test accuracy: 0.386 recall: 0.022 precision: 0.500 f1: 0.041 [after 4 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▂▄▃▄▆▅▅▅▅▇▆▆▅▆▇▆▆▇▇▇▇▇▆▆▇█████▇▆▆█▇▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ██▇▆▆▆▅▅▅▄▄▃▄▄▅▄▂▃▃▃▃▃▂▂▁▁▂▁▁▁▁▁▂▂▂▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▆▆▁▄█▇▄▅▇████▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▂▂▂▆▁▁█▃▁▁▁▁▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 59\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.3856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.04132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.02172\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.72461\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.41636\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.8125\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.28399\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33meager-sweep-42\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/kfdbbtyr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_204433-kfdbbtyr/logs\u001b[0m\n",
            "2024-06-10 21:00:25,525 - wandb.wandb_agent - INFO - Cleaning up finished run: kfdbbtyr\n",
            "2024-06-10 21:00:27,856 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 21:00:27,856 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 16\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.003652677745567213\n",
            "\tnormalize: False\n",
            "\toptimizer: sgd\n",
            "\tresize: 300\n",
            "2024-06-10 21:00:27,859 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=16 --crop=True --learning_rate=0.003652677745567213 --normalize=False --optimizer=sgd --resize=300\n",
            "2024-06-10 21:00:32,871 - wandb.wandb_agent - INFO - Running runs: ['o4exlvbe']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_210034-o4exlvbe\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mskilled-sweep-44\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/o4exlvbe\u001b[0m\n",
            "FOLD 5\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.697 accuracy: 0.535 [after 15 batches]\n",
            "train loss: 0.709 accuracy: 0.555 [after 31 batches]\n",
            "train loss: 0.717 accuracy: 0.555 [after 47 batches]\n",
            "train loss: 0.692 accuracy: 0.575 [after 63 batches]\n",
            "val loss: 0.303 accuracy: 0.913 [after 15 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.602 accuracy: 0.660 [after 15 batches]\n",
            "train loss: 0.575 accuracy: 0.680 [after 31 batches]\n",
            "train loss: 0.586 accuracy: 0.661 [after 47 batches]\n",
            "train loss: 0.619 accuracy: 0.641 [after 63 batches]\n",
            "val loss: 0.378 accuracy: 0.917 [after 15 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.574 accuracy: 0.715 [after 15 batches]\n",
            "train loss: 0.545 accuracy: 0.676 [after 31 batches]\n",
            "train loss: 0.581 accuracy: 0.663 [after 47 batches]\n",
            "train loss: 0.586 accuracy: 0.673 [after 63 batches]\n",
            "val loss: 0.368 accuracy: 0.904 [after 15 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.654 accuracy: 0.641 [after 15 batches]\n",
            "train loss: 0.601 accuracy: 0.637 [after 31 batches]\n",
            "train loss: 0.571 accuracy: 0.661 [after 47 batches]\n",
            "train loss: 0.563 accuracy: 0.668 [after 63 batches]\n",
            "val loss: 0.320 accuracy: 0.883 [after 15 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.528 accuracy: 0.727 [after 15 batches]\n",
            "train loss: 0.562 accuracy: 0.682 [after 31 batches]\n",
            "train loss: 0.552 accuracy: 0.667 [after 47 batches]\n",
            "train loss: 0.540 accuracy: 0.683 [after 63 batches]\n",
            "val loss: 0.238 accuracy: 0.913 [after 15 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.495 accuracy: 0.734 [after 15 batches]\n",
            "train loss: 0.530 accuracy: 0.719 [after 31 batches]\n",
            "train loss: 0.520 accuracy: 0.720 [after 47 batches]\n",
            "train loss: 0.529 accuracy: 0.716 [after 63 batches]\n",
            "val loss: 0.280 accuracy: 0.900 [after 15 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.533 accuracy: 0.680 [after 15 batches]\n",
            "train loss: 0.548 accuracy: 0.678 [after 31 batches]\n",
            "train loss: 0.522 accuracy: 0.702 [after 47 batches]\n",
            "train loss: 0.502 accuracy: 0.707 [after 63 batches]\n",
            "val loss: 0.353 accuracy: 0.892 [after 15 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.504 accuracy: 0.684 [after 15 batches]\n",
            "train loss: 0.506 accuracy: 0.703 [after 31 batches]\n",
            "train loss: 0.495 accuracy: 0.694 [after 47 batches]\n",
            "train loss: 0.502 accuracy: 0.696 [after 63 batches]\n",
            "val loss: 0.354 accuracy: 0.858 [after 15 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.515 accuracy: 0.695 [after 15 batches]\n",
            "train loss: 0.536 accuracy: 0.664 [after 31 batches]\n",
            "train loss: 0.559 accuracy: 0.660 [after 47 batches]\n",
            "train loss: 0.552 accuracy: 0.657 [after 63 batches]\n",
            "val loss: 0.358 accuracy: 0.925 [after 15 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.577 accuracy: 0.660 [after 15 batches]\n",
            "train loss: 0.573 accuracy: 0.662 [after 31 batches]\n",
            "train loss: 0.555 accuracy: 0.673 [after 47 batches]\n",
            "train loss: 0.535 accuracy: 0.687 [after 63 batches]\n",
            "val loss: 0.295 accuracy: 0.900 [after 15 batches]\n",
            "test accuracy: 0.991 recall: 0.973 precision: 1.000 f1: 0.985 [after 14 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▂▂▅▆▅▅▇▆▅▆▅▅▅▆█▆▆▆█▇▇▇▆▆▇▇▆▇▇▇▇▆▅▅▅▅▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▇██▇▄▄▄▅▃▃▄▄▆▄▃▃▂▃▃▂▁▂▂▂▂▃▂▁▁▁▁▁▂▂▃▃▄▃▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▇▇▆▄▇▅▅▁█▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▄██▅▁▃▇▇▇▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 39\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.99107\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.98503\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.97321\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.68652\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.53491\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.29525\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mskilled-sweep-44\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/o4exlvbe\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_210034-o4exlvbe/logs\u001b[0m\n",
            "2024-06-10 21:11:16,118 - wandb.wandb_agent - INFO - Cleaning up finished run: o4exlvbe\n",
            "2024-06-10 21:11:17,973 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 21:11:17,973 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 64\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.007212349792452739\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: 300\n",
            "2024-06-10 21:11:17,975 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=64 --crop=True --learning_rate=0.007212349792452739 --normalize=True --optimizer=sgd --resize=300\n",
            "2024-06-10 21:11:22,985 - wandb.wandb_agent - INFO - Running runs: ['x2pljpos']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_211123-x2pljpos\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrose-sweep-45\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/x2pljpos\u001b[0m\n",
            "FOLD 2\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 52, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 188, in forward\n",
            "    x = self.exit_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 171, in forward\n",
            "    x = self.block1(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 149, in forward\n",
            "    x2 =self.double_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\", line 175, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2509, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 92.00 MiB. GPU \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mrose-sweep-45\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/x2pljpos\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_211123-x2pljpos/logs\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 62, in <module>\n",
            "    model_pipeline(default_config)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 52, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 188, in forward\n",
            "    x = self.exit_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 171, in forward\n",
            "    x = self.block1(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 149, in forward\n",
            "    x2 =self.double_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\", line 175, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2509, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 92.00 MiB. GPU \n",
            "2024-06-10 21:11:48,300 - wandb.wandb_agent - INFO - Cleaning up finished run: x2pljpos\n",
            "2024-06-10 21:11:50,825 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 21:11:50,825 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 16\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.003891776473826622\n",
            "\tnormalize: False\n",
            "\toptimizer: adam\n",
            "\tresize: False\n",
            "2024-06-10 21:11:50,827 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=16 --crop=True --learning_rate=0.003891776473826622 --normalize=False --optimizer=adam --resize=False\n",
            "2024-06-10 21:11:55,836 - wandb.wandb_agent - INFO - Running runs: ['duw3blyg']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_211156-duw3blyg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfaithful-sweep-46\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/duw3blyg\u001b[0m\n",
            "FOLD 2\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 1.161 accuracy: 0.504 [after 16 batches]\n",
            "train loss: 0.943 accuracy: 0.502 [after 33 batches]\n",
            "train loss: 0.865 accuracy: 0.499 [after 50 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "train loss: 0.833 accuracy: 0.507 [after 67 batches]\n",
            "val loss: 0.644 accuracy: 0.674 [after 14 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.645 accuracy: 0.511 [after 16 batches]\n",
            "train loss: 0.636 accuracy: 0.572 [after 33 batches]\n",
            "train loss: 0.634 accuracy: 0.580 [after 50 batches]\n",
            "train loss: 0.634 accuracy: 0.570 [after 67 batches]\n",
            "val loss: 0.479 accuracy: 0.885 [after 14 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.606 accuracy: 0.632 [after 16 batches]\n",
            "train loss: 0.632 accuracy: 0.627 [after 33 batches]\n",
            "train loss: 0.629 accuracy: 0.623 [after 50 batches]\n",
            "train loss: 0.624 accuracy: 0.624 [after 67 batches]\n",
            "val loss: 0.630 accuracy: 0.705 [after 14 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.520 accuracy: 0.710 [after 16 batches]\n",
            "train loss: 0.512 accuracy: 0.744 [after 33 batches]\n",
            "train loss: 0.556 accuracy: 0.710 [after 50 batches]\n",
            "train loss: 0.565 accuracy: 0.699 [after 67 batches]\n",
            "val loss: 0.342 accuracy: 0.896 [after 14 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.407 accuracy: 0.846 [after 16 batches]\n",
            "train loss: 0.424 accuracy: 0.833 [after 33 batches]\n",
            "train loss: 0.430 accuracy: 0.847 [after 50 batches]\n",
            "train loss: 0.397 accuracy: 0.861 [after 67 batches]\n",
            "val loss: 0.166 accuracy: 0.955 [after 14 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.363 accuracy: 0.820 [after 16 batches]\n",
            "train loss: 0.380 accuracy: 0.807 [after 33 batches]\n",
            "train loss: 0.333 accuracy: 0.846 [after 50 batches]\n",
            "train loss: 0.310 accuracy: 0.862 [after 67 batches]\n",
            "val loss: 0.182 accuracy: 0.952 [after 14 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.354 accuracy: 0.882 [after 16 batches]\n",
            "train loss: 0.334 accuracy: 0.893 [after 33 batches]\n",
            "train loss: 0.314 accuracy: 0.895 [after 50 batches]\n",
            "train loss: 0.305 accuracy: 0.900 [after 67 batches]\n",
            "val loss: 0.264 accuracy: 0.906 [after 14 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.273 accuracy: 0.908 [after 16 batches]\n",
            "train loss: 0.271 accuracy: 0.904 [after 33 batches]\n",
            "train loss: 0.285 accuracy: 0.884 [after 50 batches]\n",
            "train loss: 0.269 accuracy: 0.893 [after 67 batches]\n",
            "val loss: 0.168 accuracy: 0.951 [after 14 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.196 accuracy: 0.915 [after 16 batches]\n",
            "train loss: 0.204 accuracy: 0.915 [after 33 batches]\n",
            "train loss: 0.223 accuracy: 0.912 [after 50 batches]\n",
            "train loss: 0.248 accuracy: 0.895 [after 67 batches]\n",
            "val loss: 0.303 accuracy: 0.896 [after 14 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.192 accuracy: 0.934 [after 16 batches]\n",
            "train loss: 0.232 accuracy: 0.926 [after 33 batches]\n",
            "train loss: 0.203 accuracy: 0.942 [after 50 batches]\n",
            "train loss: 0.204 accuracy: 0.939 [after 67 batches]\n",
            "val loss: 0.157 accuracy: 0.963 [after 14 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.179 accuracy: 0.923 [after 16 batches]\n",
            "train loss: 0.145 accuracy: 0.949 [after 33 batches]\n",
            "train loss: 0.178 accuracy: 0.939 [after 50 batches]\n",
            "train loss: 0.195 accuracy: 0.933 [after 67 batches]\n",
            "val loss: 0.272 accuracy: 0.899 [after 14 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.246 accuracy: 0.901 [after 16 batches]\n",
            "train loss: 0.219 accuracy: 0.917 [after 33 batches]\n",
            "train loss: 0.232 accuracy: 0.922 [after 50 batches]\n",
            "train loss: 0.229 accuracy: 0.924 [after 67 batches]\n",
            "val loss: 0.420 accuracy: 0.899 [after 14 batches]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "train loss: 0.180 accuracy: 0.941 [after 16 batches]\n",
            "train loss: 0.201 accuracy: 0.926 [after 33 batches]\n",
            "train loss: 0.219 accuracy: 0.922 [after 50 batches]\n",
            "train loss: 0.208 accuracy: 0.926 [after 67 batches]\n",
            "val loss: 0.169 accuracy: 0.960 [after 14 batches]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "train loss: 0.201 accuracy: 0.919 [after 16 batches]\n",
            "train loss: 0.193 accuracy: 0.926 [after 33 batches]\n",
            "train loss: 0.214 accuracy: 0.922 [after 50 batches]\n",
            "train loss: 0.190 accuracy: 0.933 [after 67 batches]\n",
            "val loss: 0.177 accuracy: 0.955 [after 14 batches]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "train loss: 0.218 accuracy: 0.923 [after 16 batches]\n",
            "train loss: 0.203 accuracy: 0.936 [after 33 batches]\n",
            "train loss: 0.187 accuracy: 0.945 [after 50 batches]\n",
            "train loss: 0.183 accuracy: 0.946 [after 67 batches]\n",
            "val loss: 0.200 accuracy: 0.920 [after 14 batches]\n",
            "test accuracy: 0.768 recall: 0.608 precision: 0.833 f1: 0.693 [after 14 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▁▁▂▂▃▃▄▅▄▆▆▇▆▆▇▇▇▇▇▇█▇████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▆▆▄▄▄▄▄▃▃▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▆▂▆██▇█▆█▆▆██▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆█▄▁▁▃▁▃▁▃▅▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 59\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.76786\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.69284\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.83333\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.60845\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.94577\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.18326\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.91964\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.20003\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfaithful-sweep-46\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/duw3blyg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_211156-duw3blyg/logs\u001b[0m\n",
            "2024-06-10 21:25:21,104 - wandb.wandb_agent - INFO - Cleaning up finished run: duw3blyg\n",
            "2024-06-10 21:25:23,302 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 21:25:23,302 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 32\n",
            "\tcrop: False\n",
            "\tlearning_rate: 0.005611100566613726\n",
            "\tnormalize: True\n",
            "\toptimizer: adam\n",
            "\tresize: 150\n",
            "2024-06-10 21:25:23,304 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=32 --crop=False --learning_rate=0.005611100566613726 --normalize=True --optimizer=adam --resize=150\n",
            "2024-06-10 21:25:28,315 - wandb.wandb_agent - INFO - Running runs: ['3dfgocpf']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_212528-3dfgocpf\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpleasant-sweep-50\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/3dfgocpf\u001b[0m\n",
            "FOLD 4\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 1.187 accuracy: 0.531 [after 7 batches]\n",
            "train loss: 0.962 accuracy: 0.537 [after 15 batches]\n",
            "train loss: 0.869 accuracy: 0.533 [after 23 batches]\n",
            "train loss: 0.825 accuracy: 0.537 [after 31 batches]\n",
            "val loss: 0.692 accuracy: 0.543 [after 7 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.650 accuracy: 0.621 [after 7 batches]\n",
            "train loss: 0.675 accuracy: 0.605 [after 15 batches]\n",
            "train loss: 0.666 accuracy: 0.598 [after 23 batches]\n",
            "train loss: 0.675 accuracy: 0.607 [after 31 batches]\n",
            "val loss: 1.064 accuracy: 0.601 [after 7 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.757 accuracy: 0.512 [after 7 batches]\n",
            "train loss: 0.690 accuracy: 0.572 [after 15 batches]\n",
            "train loss: 0.682 accuracy: 0.581 [after 23 batches]\n",
            "train loss: 0.675 accuracy: 0.582 [after 31 batches]\n",
            "val loss: 3.102 accuracy: 0.609 [after 7 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.651 accuracy: 0.617 [after 7 batches]\n",
            "train loss: 0.635 accuracy: 0.570 [after 15 batches]\n",
            "train loss: 0.619 accuracy: 0.608 [after 23 batches]\n",
            "train loss: 0.588 accuracy: 0.645 [after 31 batches]\n",
            "val loss: 6.008 accuracy: 0.533 [after 7 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.498 accuracy: 0.773 [after 7 batches]\n",
            "train loss: 0.477 accuracy: 0.779 [after 15 batches]\n",
            "train loss: 0.430 accuracy: 0.812 [after 23 batches]\n",
            "train loss: 0.441 accuracy: 0.811 [after 31 batches]\n",
            "val loss: 6.287 accuracy: 0.699 [after 7 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.474 accuracy: 0.777 [after 7 batches]\n",
            "train loss: 0.437 accuracy: 0.793 [after 15 batches]\n",
            "train loss: 0.378 accuracy: 0.832 [after 23 batches]\n",
            "train loss: 0.358 accuracy: 0.846 [after 31 batches]\n",
            "val loss: 0.499 accuracy: 0.800 [after 7 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.246 accuracy: 0.902 [after 7 batches]\n",
            "train loss: 0.212 accuracy: 0.924 [after 15 batches]\n",
            "train loss: 0.208 accuracy: 0.928 [after 23 batches]\n",
            "train loss: 0.192 accuracy: 0.936 [after 31 batches]\n",
            "val loss: 0.761 accuracy: 0.734 [after 7 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.200 accuracy: 0.934 [after 7 batches]\n",
            "train loss: 0.141 accuracy: 0.957 [after 15 batches]\n",
            "train loss: 0.131 accuracy: 0.960 [after 23 batches]\n",
            "train loss: 0.123 accuracy: 0.960 [after 31 batches]\n",
            "val loss: 5.353 accuracy: 0.638 [after 7 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.168 accuracy: 0.965 [after 7 batches]\n",
            "train loss: 0.121 accuracy: 0.977 [after 15 batches]\n",
            "train loss: 0.120 accuracy: 0.970 [after 23 batches]\n",
            "train loss: 0.108 accuracy: 0.972 [after 31 batches]\n",
            "val loss: 1.524 accuracy: 0.754 [after 7 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.269 accuracy: 0.914 [after 7 batches]\n",
            "train loss: 0.204 accuracy: 0.926 [after 15 batches]\n",
            "train loss: 0.171 accuracy: 0.940 [after 23 batches]\n",
            "train loss: 0.142 accuracy: 0.951 [after 31 batches]\n",
            "val loss: 14.536 accuracy: 0.586 [after 7 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.095 accuracy: 0.973 [after 7 batches]\n",
            "train loss: 0.116 accuracy: 0.963 [after 15 batches]\n",
            "train loss: 0.099 accuracy: 0.969 [after 23 batches]\n",
            "train loss: 0.107 accuracy: 0.966 [after 31 batches]\n",
            "val loss: 1.368 accuracy: 0.818 [after 7 batches]\n",
            "test accuracy: 0.619 recall: 0.960 precision: 0.546 f1: 0.695 [after 7 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▂▃▄▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▁▁▃▂▂▂▁▂▂▃▂▂▃▅▅▆▅▅▆▆▇▇▇▇▇██████▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▆▅▅▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▃▃▁▅█▆▄▆▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁▁▂▄▄▁▁▃▂█▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 43\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.61926\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.69474\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.54588\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.96044\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.96582\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.10696\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.81824\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.36768\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpleasant-sweep-50\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/3dfgocpf\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_212528-3dfgocpf/logs\u001b[0m\n",
            "2024-06-10 21:35:16,219 - wandb.wandb_agent - INFO - Cleaning up finished run: 3dfgocpf\n",
            "2024-06-10 21:35:18,428 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 21:35:18,428 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 64\n",
            "\tcrop: False\n",
            "\tlearning_rate: 0.005825162054177245\n",
            "\tnormalize: True\n",
            "\toptimizer: adam\n",
            "\tresize: 50\n",
            "2024-06-10 21:35:18,430 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=64 --crop=False --learning_rate=0.005825162054177245 --normalize=True --optimizer=adam --resize=50\n",
            "2024-06-10 21:35:23,439 - wandb.wandb_agent - INFO - Running runs: ['vfuqnxt2']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_213525-vfuqnxt2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlaced-sweep-52\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/vfuqnxt2\u001b[0m\n",
            "FOLD 5\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 1.619 accuracy: 0.496 [after 3 batches]\n",
            "train loss: 1.309 accuracy: 0.500 [after 7 batches]\n",
            "train loss: 1.127 accuracy: 0.479 [after 11 batches]\n",
            "train loss: 1.022 accuracy: 0.484 [after 15 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "val loss: 0.675 accuracy: 0.659 [after 4 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.724 accuracy: 0.492 [after 3 batches]\n",
            "train loss: 0.717 accuracy: 0.516 [after 7 batches]\n",
            "train loss: 0.712 accuracy: 0.521 [after 11 batches]\n",
            "train loss: 0.712 accuracy: 0.520 [after 15 batches]\n",
            "val loss: 0.736 accuracy: 0.667 [after 4 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.802 accuracy: 0.555 [after 3 batches]\n",
            "train loss: 0.767 accuracy: 0.537 [after 7 batches]\n",
            "train loss: 0.743 accuracy: 0.547 [after 11 batches]\n",
            "train loss: 0.746 accuracy: 0.542 [after 15 batches]\n",
            "val loss: 0.729 accuracy: 0.671 [after 4 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.743 accuracy: 0.512 [after 3 batches]\n",
            "train loss: 0.743 accuracy: 0.531 [after 7 batches]\n",
            "train loss: 0.725 accuracy: 0.527 [after 11 batches]\n",
            "train loss: 0.721 accuracy: 0.546 [after 15 batches]\n",
            "val loss: 1.302 accuracy: 0.336 [after 4 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.678 accuracy: 0.578 [after 3 batches]\n",
            "train loss: 0.681 accuracy: 0.557 [after 7 batches]\n",
            "train loss: 0.678 accuracy: 0.565 [after 11 batches]\n",
            "train loss: 0.690 accuracy: 0.544 [after 15 batches]\n",
            "val loss: 0.717 accuracy: 0.669 [after 4 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.691 accuracy: 0.508 [after 3 batches]\n",
            "train loss: 0.691 accuracy: 0.535 [after 7 batches]\n",
            "train loss: 0.692 accuracy: 0.529 [after 11 batches]\n",
            "train loss: 0.688 accuracy: 0.531 [after 15 batches]\n",
            "val loss: 0.752 accuracy: 0.594 [after 4 batches]\n",
            "test accuracy: 0.525 recall: 0.750 precision: 0.355 f1: 0.481 [after 4 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▂▂▁▁▂▄▄▄▆▅▆▅▃▅▄▆█▆▇▆▃▅▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▆▄▄▁▁▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ███▁█▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁▂▂█▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.52511\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.4814\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.35547\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.75\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.53125\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.68787\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.59375\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.75183\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlaced-sweep-52\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/vfuqnxt2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_213525-vfuqnxt2/logs\u001b[0m\n",
            "2024-06-10 21:41:38,210 - wandb.wandb_agent - INFO - Cleaning up finished run: vfuqnxt2\n",
            "2024-06-10 21:41:39,822 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 21:41:39,822 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 64\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.0012248844908352666\n",
            "\tnormalize: False\n",
            "\toptimizer: adam\n",
            "\tresize: 150\n",
            "2024-06-10 21:41:39,824 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=64 --crop=True --learning_rate=0.0012248844908352666 --normalize=False --optimizer=adam --resize=150\n",
            "2024-06-10 21:41:44,833 - wandb.wandb_agent - INFO - Running runs: ['uu6h7gwk']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_214146-uu6h7gwk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwoven-sweep-54\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/uu6h7gwk\u001b[0m\n",
            "FOLD 6\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "train loss: 1.179 accuracy: 0.477 [after 3 batches]\n",
            "train loss: 0.894 accuracy: 0.555 [after 7 batches]\n",
            "train loss: 0.795 accuracy: 0.585 [after 11 batches]\n",
            "train loss: 0.752 accuracy: 0.600 [after 15 batches]\n",
            "val loss: 0.637 accuracy: 0.665 [after 4 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.444 accuracy: 0.844 [after 3 batches]\n",
            "train loss: 0.391 accuracy: 0.867 [after 7 batches]\n",
            "train loss: 0.364 accuracy: 0.874 [after 11 batches]\n",
            "train loss: 0.347 accuracy: 0.871 [after 15 batches]\n",
            "val loss: 0.434 accuracy: 0.828 [after 4 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.231 accuracy: 0.891 [after 3 batches]\n",
            "train loss: 0.182 accuracy: 0.920 [after 7 batches]\n",
            "train loss: 0.164 accuracy: 0.928 [after 11 batches]\n",
            "train loss: 0.142 accuracy: 0.942 [after 15 batches]\n",
            "val loss: 0.785 accuracy: 0.750 [after 4 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.079 accuracy: 0.977 [after 3 batches]\n",
            "train loss: 0.183 accuracy: 0.951 [after 7 batches]\n",
            "train loss: 0.152 accuracy: 0.953 [after 11 batches]\n",
            "train loss: 0.146 accuracy: 0.951 [after 15 batches]\n",
            "val loss: 0.542 accuracy: 0.867 [after 4 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.152 accuracy: 0.957 [after 3 batches]\n",
            "train loss: 0.191 accuracy: 0.934 [after 7 batches]\n",
            "train loss: 0.178 accuracy: 0.939 [after 11 batches]\n",
            "train loss: 0.178 accuracy: 0.945 [after 15 batches]\n",
            "val loss: 0.441 accuracy: 0.835 [after 4 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.039 accuracy: 0.992 [after 3 batches]\n",
            "train loss: 0.062 accuracy: 0.984 [after 7 batches]\n",
            "train loss: 0.075 accuracy: 0.975 [after 11 batches]\n",
            "train loss: 0.090 accuracy: 0.970 [after 15 batches]\n",
            "val loss: 0.494 accuracy: 0.909 [after 4 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.103 accuracy: 0.953 [after 3 batches]\n",
            "train loss: 0.092 accuracy: 0.961 [after 7 batches]\n",
            "train loss: 0.084 accuracy: 0.966 [after 11 batches]\n",
            "train loss: 0.078 accuracy: 0.968 [after 15 batches]\n",
            "val loss: 0.432 accuracy: 0.902 [after 4 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.048 accuracy: 0.980 [after 3 batches]\n",
            "train loss: 0.043 accuracy: 0.982 [after 7 batches]\n",
            "train loss: 0.036 accuracy: 0.984 [after 11 batches]\n",
            "train loss: 0.037 accuracy: 0.986 [after 15 batches]\n",
            "val loss: 0.609 accuracy: 0.887 [after 4 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.024 accuracy: 0.996 [after 3 batches]\n",
            "train loss: 0.024 accuracy: 0.996 [after 7 batches]\n",
            "train loss: 0.036 accuracy: 0.991 [after 11 batches]\n",
            "train loss: 0.040 accuracy: 0.990 [after 15 batches]\n",
            "val loss: 0.577 accuracy: 0.863 [after 4 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.024 accuracy: 0.996 [after 3 batches]\n",
            "train loss: 0.032 accuracy: 0.988 [after 7 batches]\n",
            "train loss: 0.026 accuracy: 0.991 [after 11 batches]\n",
            "train loss: 0.026 accuracy: 0.990 [after 15 batches]\n",
            "val loss: 0.758 accuracy: 0.811 [after 4 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.059 accuracy: 0.977 [after 3 batches]\n",
            "train loss: 0.034 accuracy: 0.988 [after 7 batches]\n",
            "train loss: 0.038 accuracy: 0.987 [after 11 batches]\n",
            "train loss: 0.041 accuracy: 0.986 [after 15 batches]\n",
            "val loss: 0.680 accuracy: 0.776 [after 4 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.031 accuracy: 0.984 [after 3 batches]\n",
            "train loss: 0.036 accuracy: 0.984 [after 7 batches]\n",
            "train loss: 0.033 accuracy: 0.984 [after 11 batches]\n",
            "train loss: 0.030 accuracy: 0.986 [after 15 batches]\n",
            "val loss: 0.624 accuracy: 0.822 [after 4 batches]\n",
            "test accuracy: 0.871 recall: 0.984 precision: 0.795 f1: 0.878 [after 4 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▂▃▄▄▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▂▃▆▆▆▇▇▇█▇▇▇▇▇▇███▇███████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▆▆▅▄▃▃▂▂▂▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▆▃▇▆██▇▇▅▄▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▅▁█▃▁▂▁▅▄▇▆▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.87054\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.87764\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.79507\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.98387\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.98633\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.02987\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.82161\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.62387\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwoven-sweep-54\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/uu6h7gwk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_214146-uu6h7gwk/logs\u001b[0m\n",
            "2024-06-10 21:52:02,652 - wandb.wandb_agent - INFO - Cleaning up finished run: uu6h7gwk\n",
            "2024-06-10 21:52:05,331 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 21:52:05,331 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 64\n",
            "\tcrop: False\n",
            "\tlearning_rate: 0.0035674504929040507\n",
            "\tnormalize: True\n",
            "\toptimizer: adam\n",
            "\tresize: 300\n",
            "2024-06-10 21:52:05,333 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=64 --crop=False --learning_rate=0.0035674504929040507 --normalize=True --optimizer=adam --resize=300\n",
            "2024-06-10 21:52:10,342 - wandb.wandb_agent - INFO - Running runs: ['n2fmuh8q']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_215210-n2fmuh8q\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfragrant-sweep-58\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/n2fmuh8q\u001b[0m\n",
            "FOLD 5\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 52, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 187, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 124, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 113, in forward\n",
            "    x = self.triple_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\", line 175, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2509, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfragrant-sweep-58\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/n2fmuh8q\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_215210-n2fmuh8q/logs\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 62, in <module>\n",
            "    model_pipeline(default_config)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 52, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 187, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 124, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 113, in forward\n",
            "    x = self.triple_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\", line 175, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2509, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU \n",
            "2024-06-10 21:52:35,672 - wandb.wandb_agent - INFO - Cleaning up finished run: n2fmuh8q\n",
            "2024-06-10 21:52:37,645 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 21:52:37,646 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 32\n",
            "\tcrop: False\n",
            "\tlearning_rate: 0.006280738040427414\n",
            "\tnormalize: False\n",
            "\toptimizer: sgd\n",
            "\tresize: 150\n",
            "2024-06-10 21:52:37,648 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=32 --crop=False --learning_rate=0.006280738040427414 --normalize=False --optimizer=sgd --resize=150\n",
            "2024-06-10 21:52:42,659 - wandb.wandb_agent - INFO - Running runs: ['x6pm0h26']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_215244-x6pm0h26\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdrawn-sweep-59\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/x6pm0h26\u001b[0m\n",
            "FOLD 2\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.681 accuracy: 0.574 [after 7 batches]\n",
            "train loss: 0.659 accuracy: 0.600 [after 15 batches]\n",
            "train loss: 0.615 accuracy: 0.629 [after 23 batches]\n",
            "train loss: 0.610 accuracy: 0.634 [after 31 batches]\n",
            "val loss: 0.782 accuracy: 0.456 [after 7 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.604 accuracy: 0.633 [after 7 batches]\n",
            "train loss: 0.577 accuracy: 0.658 [after 15 batches]\n",
            "train loss: 0.569 accuracy: 0.659 [after 23 batches]\n",
            "train loss: 0.573 accuracy: 0.653 [after 31 batches]\n",
            "val loss: 0.293 accuracy: 0.909 [after 7 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.603 accuracy: 0.617 [after 7 batches]\n",
            "train loss: 0.584 accuracy: 0.645 [after 15 batches]\n",
            "train loss: 0.585 accuracy: 0.647 [after 23 batches]\n",
            "train loss: 0.562 accuracy: 0.659 [after 31 batches]\n",
            "val loss: 0.239 accuracy: 0.909 [after 7 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.514 accuracy: 0.707 [after 7 batches]\n",
            "train loss: 0.524 accuracy: 0.689 [after 15 batches]\n",
            "train loss: 0.516 accuracy: 0.694 [after 23 batches]\n",
            "train loss: 0.496 accuracy: 0.700 [after 31 batches]\n",
            "val loss: 0.199 accuracy: 0.909 [after 7 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.527 accuracy: 0.707 [after 7 batches]\n",
            "train loss: 0.519 accuracy: 0.699 [after 15 batches]\n",
            "train loss: 0.503 accuracy: 0.691 [after 23 batches]\n",
            "train loss: 0.494 accuracy: 0.683 [after 31 batches]\n",
            "val loss: 0.087 accuracy: 0.996 [after 7 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.466 accuracy: 0.723 [after 7 batches]\n",
            "train loss: 0.485 accuracy: 0.725 [after 15 batches]\n",
            "train loss: 0.490 accuracy: 0.703 [after 23 batches]\n",
            "train loss: 0.491 accuracy: 0.701 [after 31 batches]\n",
            "val loss: 0.168 accuracy: 0.908 [after 7 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.432 accuracy: 0.758 [after 7 batches]\n",
            "train loss: 0.461 accuracy: 0.723 [after 15 batches]\n",
            "train loss: 0.455 accuracy: 0.723 [after 23 batches]\n",
            "train loss: 0.456 accuracy: 0.715 [after 31 batches]\n",
            "val loss: 0.116 accuracy: 0.972 [after 7 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.506 accuracy: 0.656 [after 7 batches]\n",
            "train loss: 0.492 accuracy: 0.697 [after 15 batches]\n",
            "train loss: 0.471 accuracy: 0.706 [after 23 batches]\n",
            "train loss: 0.456 accuracy: 0.701 [after 31 batches]\n",
            "val loss: 0.110 accuracy: 0.973 [after 7 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.438 accuracy: 0.699 [after 7 batches]\n",
            "train loss: 0.454 accuracy: 0.713 [after 15 batches]\n",
            "train loss: 0.467 accuracy: 0.701 [after 23 batches]\n",
            "train loss: 0.464 accuracy: 0.701 [after 31 batches]\n",
            "val loss: 0.070 accuracy: 0.982 [after 7 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.424 accuracy: 0.742 [after 7 batches]\n",
            "train loss: 0.477 accuracy: 0.719 [after 15 batches]\n",
            "train loss: 0.478 accuracy: 0.707 [after 23 batches]\n",
            "train loss: 0.466 accuracy: 0.719 [after 31 batches]\n",
            "val loss: 0.076 accuracy: 0.987 [after 7 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.414 accuracy: 0.738 [after 7 batches]\n",
            "train loss: 0.400 accuracy: 0.727 [after 15 batches]\n",
            "train loss: 0.397 accuracy: 0.737 [after 23 batches]\n",
            "train loss: 0.407 accuracy: 0.734 [after 31 batches]\n",
            "val loss: 0.082 accuracy: 0.950 [after 7 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.471 accuracy: 0.680 [after 7 batches]\n",
            "train loss: 0.476 accuracy: 0.703 [after 15 batches]\n",
            "train loss: 0.465 accuracy: 0.716 [after 23 batches]\n",
            "train loss: 0.456 accuracy: 0.717 [after 31 batches]\n",
            "val loss: 0.062 accuracy: 0.996 [after 7 batches]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "train loss: 0.435 accuracy: 0.695 [after 7 batches]\n",
            "train loss: 0.422 accuracy: 0.721 [after 15 batches]\n",
            "train loss: 0.422 accuracy: 0.720 [after 23 batches]\n",
            "train loss: 0.431 accuracy: 0.712 [after 31 batches]\n",
            "val loss: 0.038 accuracy: 1.000 [after 7 batches]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "train loss: 0.498 accuracy: 0.684 [after 7 batches]\n",
            "train loss: 0.496 accuracy: 0.699 [after 15 batches]\n",
            "train loss: 0.454 accuracy: 0.719 [after 23 batches]\n",
            "train loss: 0.459 accuracy: 0.711 [after 31 batches]\n",
            "val loss: 0.033 accuracy: 1.000 [after 7 batches]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "train loss: 0.465 accuracy: 0.699 [after 7 batches]\n",
            "train loss: 0.470 accuracy: 0.688 [after 15 batches]\n",
            "train loss: 0.447 accuracy: 0.715 [after 23 batches]\n",
            "train loss: 0.465 accuracy: 0.714 [after 31 batches]\n",
            "val loss: 0.068 accuracy: 0.972 [after 7 batches]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "train loss: 0.431 accuracy: 0.750 [after 7 batches]\n",
            "train loss: 0.459 accuracy: 0.719 [after 15 batches]\n",
            "train loss: 0.474 accuracy: 0.694 [after 23 batches]\n",
            "train loss: 0.455 accuracy: 0.704 [after 31 batches]\n",
            "val loss: 0.058 accuracy: 0.982 [after 7 batches]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "train loss: 0.395 accuracy: 0.730 [after 7 batches]\n",
            "train loss: 0.408 accuracy: 0.730 [after 15 batches]\n",
            "train loss: 0.416 accuracy: 0.714 [after 23 batches]\n",
            "train loss: 0.416 accuracy: 0.732 [after 31 batches]\n",
            "val loss: 0.044 accuracy: 0.991 [after 7 batches]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "train loss: 0.443 accuracy: 0.684 [after 7 batches]\n",
            "train loss: 0.433 accuracy: 0.705 [after 15 batches]\n",
            "train loss: 0.460 accuracy: 0.704 [after 23 batches]\n",
            "train loss: 0.441 accuracy: 0.708 [after 31 batches]\n",
            "val loss: 0.094 accuracy: 0.912 [after 7 batches]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "train loss: 0.515 accuracy: 0.648 [after 7 batches]\n",
            "train loss: 0.534 accuracy: 0.648 [after 15 batches]\n",
            "train loss: 0.500 accuracy: 0.672 [after 23 batches]\n",
            "train loss: 0.490 accuracy: 0.681 [after 31 batches]\n",
            "val loss: 0.270 accuracy: 0.895 [after 7 batches]\n",
            "test accuracy: 0.729 recall: 0.408 precision: 1.000 f1: 0.568 [after 7 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▅▄▄▅▆▆▆▆▇▆▇▇▄▆▆▆█▇██▅▇▆▇▇▆▇▆▇▇▆██▆▇▄▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▅▅▆▅▄▃▄▃▃▃▃▂▄▃▂▃▂▃▁▁▃▃▂▂▂▃▃▃▃▃▂▁▁▂▂▄▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▇▇▇█▇████▇██████▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 75\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.72895\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.56802\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.40807\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.68066\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.48974\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.89477\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.27014\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdrawn-sweep-59\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/x6pm0h26\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_215244-x6pm0h26/logs\u001b[0m\n",
            "2024-06-10 22:13:09,281 - wandb.wandb_agent - INFO - Cleaning up finished run: x6pm0h26\n",
            "2024-06-10 22:13:11,649 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 22:13:11,649 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 8\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.0006460220176883009\n",
            "\tnormalize: False\n",
            "\toptimizer: adam\n",
            "\tresize: 50\n",
            "2024-06-10 22:13:11,650 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=8 --crop=True --learning_rate=0.0006460220176883009 --normalize=False --optimizer=adam --resize=50\n",
            "2024-06-10 22:13:16,660 - wandb.wandb_agent - INFO - Running runs: ['c3y9n39u']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_221317-c3y9n39u\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdivine-sweep-60\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/c3y9n39u\u001b[0m\n",
            "FOLD 1\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "train loss: 0.669 accuracy: 0.663 [after 32 batches]\n",
            "train loss: 0.576 accuracy: 0.744 [after 65 batches]\n",
            "train loss: 0.500 accuracy: 0.787 [after 98 batches]\n",
            "train loss: 0.459 accuracy: 0.809 [after 131 batches]\n",
            "val loss: 0.246 accuracy: 0.917 [after 30 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.140 accuracy: 0.955 [after 32 batches]\n",
            "train loss: 0.144 accuracy: 0.951 [after 65 batches]\n",
            "train loss: 0.131 accuracy: 0.953 [after 98 batches]\n",
            "train loss: 0.143 accuracy: 0.950 [after 131 batches]\n",
            "val loss: 0.601 accuracy: 0.604 [after 30 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.247 accuracy: 0.936 [after 32 batches]\n",
            "train loss: 0.179 accuracy: 0.949 [after 65 batches]\n",
            "train loss: 0.141 accuracy: 0.957 [after 98 batches]\n",
            "train loss: 0.119 accuracy: 0.963 [after 131 batches]\n",
            "val loss: 0.491 accuracy: 0.917 [after 30 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.289 accuracy: 0.947 [after 32 batches]\n",
            "train loss: 0.223 accuracy: 0.943 [after 65 batches]\n",
            "train loss: 0.214 accuracy: 0.944 [after 98 batches]\n",
            "train loss: 0.176 accuracy: 0.955 [after 131 batches]\n",
            "val loss: 0.370 accuracy: 0.917 [after 30 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.111 accuracy: 0.977 [after 32 batches]\n",
            "train loss: 0.114 accuracy: 0.966 [after 65 batches]\n",
            "train loss: 0.108 accuracy: 0.971 [after 98 batches]\n",
            "train loss: 0.094 accuracy: 0.975 [after 131 batches]\n",
            "val loss: 0.365 accuracy: 0.883 [after 30 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.012 accuracy: 1.000 [after 32 batches]\n",
            "train loss: 0.010 accuracy: 1.000 [after 65 batches]\n",
            "train loss: 0.035 accuracy: 0.991 [after 98 batches]\n",
            "train loss: 0.049 accuracy: 0.986 [after 131 batches]\n",
            "val loss: 0.604 accuracy: 0.825 [after 30 batches]\n",
            "test accuracy: 0.947 recall: 0.952 precision: 0.971 f1: 0.955 [after 26 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▄▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▆▂▂▂▂▄▃▂▂▄▃▃▃▂▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy █▁██▇▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁█▆▃▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.94712\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.95482\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.97115\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.95192\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.9858\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.04873\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.825\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.60396\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdivine-sweep-60\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/c3y9n39u\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_221317-c3y9n39u/logs\u001b[0m\n",
            "2024-06-10 22:19:36,713 - wandb.wandb_agent - INFO - Cleaning up finished run: c3y9n39u\n",
            "2024-06-10 22:19:38,872 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 22:19:38,872 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 32\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.003322995268602219\n",
            "\tnormalize: False\n",
            "\toptimizer: sgd\n",
            "\tresize: 50\n",
            "2024-06-10 22:19:38,874 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=32 --crop=True --learning_rate=0.003322995268602219 --normalize=False --optimizer=sgd --resize=50\n",
            "2024-06-10 22:19:43,884 - wandb.wandb_agent - INFO - Running runs: ['eof50elh']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_221944-eof50elh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhonest-sweep-61\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/eof50elh\u001b[0m\n",
            "FOLD 1\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "train loss: 0.705 accuracy: 0.531 [after 7 batches]\n",
            "train loss: 0.702 accuracy: 0.539 [after 15 batches]\n",
            "train loss: 0.690 accuracy: 0.542 [after 23 batches]\n",
            "train loss: 0.675 accuracy: 0.562 [after 31 batches]\n",
            "val loss: 0.668 accuracy: 0.668 [after 8 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.536 accuracy: 0.691 [after 7 batches]\n",
            "train loss: 0.545 accuracy: 0.678 [after 15 batches]\n",
            "train loss: 0.536 accuracy: 0.682 [after 23 batches]\n",
            "train loss: 0.528 accuracy: 0.679 [after 31 batches]\n",
            "val loss: 0.378 accuracy: 0.844 [after 8 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.484 accuracy: 0.730 [after 7 batches]\n",
            "train loss: 0.507 accuracy: 0.699 [after 15 batches]\n",
            "train loss: 0.515 accuracy: 0.682 [after 23 batches]\n",
            "train loss: 0.513 accuracy: 0.687 [after 31 batches]\n",
            "val loss: 0.816 accuracy: 0.727 [after 8 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.521 accuracy: 0.672 [after 7 batches]\n",
            "train loss: 0.515 accuracy: 0.701 [after 15 batches]\n",
            "train loss: 0.508 accuracy: 0.698 [after 23 batches]\n",
            "train loss: 0.508 accuracy: 0.698 [after 31 batches]\n",
            "val loss: 0.286 accuracy: 0.910 [after 8 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.510 accuracy: 0.699 [after 7 batches]\n",
            "train loss: 0.502 accuracy: 0.678 [after 15 batches]\n",
            "train loss: 0.499 accuracy: 0.680 [after 23 batches]\n",
            "train loss: 0.506 accuracy: 0.680 [after 31 batches]\n",
            "val loss: 0.668 accuracy: 0.477 [after 8 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.482 accuracy: 0.734 [after 7 batches]\n",
            "train loss: 0.506 accuracy: 0.715 [after 15 batches]\n",
            "train loss: 0.493 accuracy: 0.716 [after 23 batches]\n",
            "train loss: 0.484 accuracy: 0.723 [after 31 batches]\n",
            "val loss: 0.635 accuracy: 0.762 [after 8 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.535 accuracy: 0.711 [after 7 batches]\n",
            "train loss: 0.483 accuracy: 0.725 [after 15 batches]\n",
            "train loss: 0.490 accuracy: 0.708 [after 23 batches]\n",
            "train loss: 0.489 accuracy: 0.710 [after 31 batches]\n",
            "val loss: 0.713 accuracy: 0.441 [after 8 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.490 accuracy: 0.703 [after 7 batches]\n",
            "train loss: 0.480 accuracy: 0.693 [after 15 batches]\n",
            "train loss: 0.471 accuracy: 0.691 [after 23 batches]\n",
            "train loss: 0.490 accuracy: 0.690 [after 31 batches]\n",
            "val loss: 0.865 accuracy: 0.543 [after 8 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.478 accuracy: 0.699 [after 7 batches]\n",
            "train loss: 0.492 accuracy: 0.676 [after 15 batches]\n",
            "train loss: 0.506 accuracy: 0.677 [after 23 batches]\n",
            "train loss: 0.487 accuracy: 0.695 [after 31 batches]\n",
            "val loss: 0.318 accuracy: 0.910 [after 8 batches]\n",
            "test accuracy: 0.969 recall: 0.957 precision: 0.992 f1: 0.974 [after 7 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▄▅▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▁▂▇▆▆▆█▇▆▆▆▇▇▇▇▆▆▆█▇▇█▇█▇▇▇▇▇▆▇▆▆▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ███▇▃▃▃▃▁▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▃▁▂▂▂▁▁▂▁▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▄▇▅█▂▆▁▃█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▆▂▇▁▆▅▆█▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.96875\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.97357\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.9916\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.95709\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.69531\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.4874\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.91016\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.31766\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhonest-sweep-61\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/eof50elh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_221944-eof50elh/logs\u001b[0m\n",
            "2024-06-10 22:27:55,311 - wandb.wandb_agent - INFO - Cleaning up finished run: eof50elh\n",
            "2024-06-10 22:27:57,364 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 22:27:57,364 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 16\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.005156345070314441\n",
            "\tnormalize: False\n",
            "\toptimizer: adam\n",
            "\tresize: 150\n",
            "2024-06-10 22:27:57,366 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=16 --crop=True --learning_rate=0.005156345070314441 --normalize=False --optimizer=adam --resize=150\n",
            "2024-06-10 22:28:02,375 - wandb.wandb_agent - INFO - Running runs: ['8fs4g501']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_222803-8fs4g501\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrevived-sweep-62\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/8fs4g501\u001b[0m\n",
            "FOLD 3\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 1.347 accuracy: 0.426 [after 16 batches]\n",
            "train loss: 1.029 accuracy: 0.463 [after 33 batches]\n",
            "train loss: 0.921 accuracy: 0.479 [after 50 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "train loss: 0.878 accuracy: 0.497 [after 67 batches]\n",
            "val loss: 0.939 accuracy: 0.452 [after 14 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.740 accuracy: 0.460 [after 16 batches]\n",
            "train loss: 0.741 accuracy: 0.491 [after 33 batches]\n",
            "train loss: 0.733 accuracy: 0.493 [after 50 batches]\n",
            "train loss: 0.727 accuracy: 0.493 [after 67 batches]\n",
            "val loss: 2.249 accuracy: 0.452 [after 14 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.751 accuracy: 0.555 [after 16 batches]\n",
            "train loss: 0.726 accuracy: 0.535 [after 33 batches]\n",
            "train loss: 0.716 accuracy: 0.529 [after 50 batches]\n",
            "train loss: 0.711 accuracy: 0.526 [after 67 batches]\n",
            "val loss: 5.423 accuracy: 0.676 [after 14 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.702 accuracy: 0.485 [after 16 batches]\n",
            "train loss: 0.697 accuracy: 0.513 [after 33 batches]\n",
            "train loss: 0.696 accuracy: 0.511 [after 50 batches]\n",
            "train loss: 0.696 accuracy: 0.506 [after 67 batches]\n",
            "val loss: 0.689 accuracy: 0.579 [after 14 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.698 accuracy: 0.489 [after 16 batches]\n",
            "train loss: 0.697 accuracy: 0.493 [after 33 batches]\n",
            "train loss: 0.696 accuracy: 0.495 [after 50 batches]\n",
            "train loss: 0.695 accuracy: 0.491 [after 67 batches]\n",
            "val loss: 0.681 accuracy: 0.564 [after 14 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.694 accuracy: 0.500 [after 16 batches]\n",
            "train loss: 0.694 accuracy: 0.507 [after 33 batches]\n",
            "train loss: 0.693 accuracy: 0.491 [after 50 batches]\n",
            "train loss: 0.693 accuracy: 0.489 [after 67 batches]\n",
            "val loss: 0.909 accuracy: 0.454 [after 14 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.695 accuracy: 0.500 [after 16 batches]\n",
            "train loss: 0.693 accuracy: 0.460 [after 33 batches]\n",
            "train loss: 0.694 accuracy: 0.500 [after 50 batches]\n",
            "train loss: 0.694 accuracy: 0.502 [after 67 batches]\n",
            "val loss: 0.647 accuracy: 0.458 [after 14 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.692 accuracy: 0.500 [after 16 batches]\n",
            "train loss: 0.694 accuracy: 0.489 [after 33 batches]\n",
            "train loss: 0.694 accuracy: 0.495 [after 50 batches]\n",
            "train loss: 0.694 accuracy: 0.483 [after 67 batches]\n",
            "val loss: 0.683 accuracy: 0.545 [after 14 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.691 accuracy: 0.449 [after 16 batches]\n",
            "train loss: 0.693 accuracy: 0.480 [after 33 batches]\n",
            "train loss: 0.693 accuracy: 0.489 [after 50 batches]\n",
            "train loss: 0.693 accuracy: 0.482 [after 67 batches]\n",
            "val loss: 0.674 accuracy: 0.625 [after 14 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.694 accuracy: 0.507 [after 16 batches]\n",
            "train loss: 0.694 accuracy: 0.500 [after 33 batches]\n",
            "train loss: 0.693 accuracy: 0.489 [after 50 batches]\n",
            "train loss: 0.693 accuracy: 0.483 [after 67 batches]\n",
            "val loss: 0.725 accuracy: 0.356 [after 14 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.691 accuracy: 0.452 [after 16 batches]\n",
            "train loss: 0.692 accuracy: 0.491 [after 33 batches]\n",
            "train loss: 0.692 accuracy: 0.499 [after 50 batches]\n",
            "train loss: 0.693 accuracy: 0.502 [after 67 batches]\n",
            "val loss: 0.851 accuracy: 0.326 [after 14 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.691 accuracy: 0.500 [after 16 batches]\n",
            "train loss: 0.693 accuracy: 0.498 [after 33 batches]\n",
            "train loss: 0.693 accuracy: 0.491 [after 50 batches]\n",
            "train loss: 0.693 accuracy: 0.492 [after 67 batches]\n",
            "val loss: 0.872 accuracy: 0.452 [after 14 batches]\n",
            "test accuracy: 0.423 recall: 0.714 precision: 0.298 f1: 0.415 [after 14 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▂▃▄▄▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▄▅▃▅▅█▇▇▄▆▆▅▄▅▄▅▅▅▅▃▅▅▅▅▄▂▄▄▅▅▄▄▂▅▅▅▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▅▃▃▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▄▄█▆▆▄▄▅▇▂▁▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁▃█▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.42262\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.4146\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.29762\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.71429\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.4921\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.69277\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.45238\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.87226\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mrevived-sweep-62\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/8fs4g501\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_222803-8fs4g501/logs\u001b[0m\n",
            "2024-06-10 22:39:36,662 - wandb.wandb_agent - INFO - Cleaning up finished run: 8fs4g501\n",
            "2024-06-10 22:39:38,979 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 22:39:38,979 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 64\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.0066627696263693295\n",
            "\tnormalize: False\n",
            "\toptimizer: sgd\n",
            "\tresize: 50\n",
            "2024-06-10 22:39:38,981 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=64 --crop=True --learning_rate=0.0066627696263693295 --normalize=False --optimizer=sgd --resize=50\n",
            "2024-06-10 22:39:43,991 - wandb.wandb_agent - INFO - Running runs: ['z3nk6mlh']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_223945-z3nk6mlh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhonest-sweep-63\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/z3nk6mlh\u001b[0m\n",
            "FOLD 6\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.699 accuracy: 0.488 [after 3 batches]\n",
            "train loss: 0.698 accuracy: 0.520 [after 7 batches]\n",
            "train loss: 0.696 accuracy: 0.510 [after 11 batches]\n",
            "train loss: 0.694 accuracy: 0.526 [after 15 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "val loss: 0.693 accuracy: 0.668 [after 4 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.655 accuracy: 0.539 [after 3 batches]\n",
            "train loss: 0.612 accuracy: 0.584 [after 7 batches]\n",
            "train loss: 0.589 accuracy: 0.618 [after 11 batches]\n",
            "train loss: 0.578 accuracy: 0.625 [after 15 batches]\n",
            "val loss: 0.687 accuracy: 0.669 [after 4 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.545 accuracy: 0.660 [after 3 batches]\n",
            "train loss: 0.537 accuracy: 0.658 [after 7 batches]\n",
            "train loss: 0.527 accuracy: 0.668 [after 11 batches]\n",
            "train loss: 0.522 accuracy: 0.659 [after 15 batches]\n",
            "val loss: 0.339 accuracy: 0.884 [after 4 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.505 accuracy: 0.688 [after 3 batches]\n",
            "train loss: 0.479 accuracy: 0.701 [after 7 batches]\n",
            "train loss: 0.465 accuracy: 0.697 [after 11 batches]\n",
            "train loss: 0.473 accuracy: 0.709 [after 15 batches]\n",
            "val loss: 0.282 accuracy: 0.911 [after 4 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.467 accuracy: 0.738 [after 3 batches]\n",
            "train loss: 0.474 accuracy: 0.697 [after 7 batches]\n",
            "train loss: 0.475 accuracy: 0.706 [after 11 batches]\n",
            "train loss: 0.482 accuracy: 0.704 [after 15 batches]\n",
            "val loss: 0.269 accuracy: 0.914 [after 4 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.438 accuracy: 0.734 [after 3 batches]\n",
            "train loss: 0.442 accuracy: 0.717 [after 7 batches]\n",
            "train loss: 0.444 accuracy: 0.728 [after 11 batches]\n",
            "train loss: 0.439 accuracy: 0.735 [after 15 batches]\n",
            "val loss: 0.330 accuracy: 0.913 [after 4 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.461 accuracy: 0.742 [after 3 batches]\n",
            "train loss: 0.447 accuracy: 0.732 [after 7 batches]\n",
            "train loss: 0.455 accuracy: 0.723 [after 11 batches]\n",
            "train loss: 0.449 accuracy: 0.730 [after 15 batches]\n",
            "val loss: 0.291 accuracy: 0.914 [after 4 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.491 accuracy: 0.723 [after 3 batches]\n",
            "train loss: 0.473 accuracy: 0.727 [after 7 batches]\n",
            "train loss: 0.463 accuracy: 0.716 [after 11 batches]\n",
            "train loss: 0.470 accuracy: 0.703 [after 15 batches]\n",
            "val loss: 0.293 accuracy: 0.915 [after 4 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.467 accuracy: 0.715 [after 3 batches]\n",
            "train loss: 0.484 accuracy: 0.732 [after 7 batches]\n",
            "train loss: 0.472 accuracy: 0.745 [after 11 batches]\n",
            "train loss: 0.489 accuracy: 0.725 [after 15 batches]\n",
            "val loss: 0.310 accuracy: 0.896 [after 4 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.465 accuracy: 0.699 [after 3 batches]\n",
            "train loss: 0.437 accuracy: 0.742 [after 7 batches]\n",
            "train loss: 0.456 accuracy: 0.725 [after 11 batches]\n",
            "train loss: 0.446 accuracy: 0.723 [after 15 batches]\n",
            "val loss: 0.323 accuracy: 0.908 [after 4 batches]\n",
            "test accuracy: 0.992 recall: 0.979 precision: 1.000 f1: 0.989 [after 4 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▂▂▂▄▅▅▆▆▆▆▆▇▇▇█▇▇▇█▇████▇█▇█▇▇▇██▇▇█▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ████▇▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▂▂▂▂▂▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▇█████▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ██▂▁▁▂▁▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 39\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.99219\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.98927\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.97903\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.72266\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.44627\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.90755\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.3233\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhonest-sweep-63\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/z3nk6mlh\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_223945-z3nk6mlh/logs\u001b[0m\n",
            "2024-06-10 22:48:25,803 - wandb.wandb_agent - INFO - Cleaning up finished run: z3nk6mlh\n",
            "2024-06-10 22:48:28,815 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 22:48:28,815 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 64\n",
            "\tcrop: False\n",
            "\tlearning_rate: 0.007951651155408301\n",
            "\tnormalize: False\n",
            "\toptimizer: adam\n",
            "\tresize: 150\n",
            "2024-06-10 22:48:28,817 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=64 --crop=False --learning_rate=0.007951651155408301 --normalize=False --optimizer=adam --resize=150\n",
            "2024-06-10 22:48:33,826 - wandb.wandb_agent - INFO - Running runs: ['cmsn6rnc']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_224834-cmsn6rnc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33melated-sweep-64\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/cmsn6rnc\u001b[0m\n",
            "FOLD 2\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 1.517 accuracy: 0.527 [after 3 batches]\n",
            "train loss: 1.087 accuracy: 0.541 [after 7 batches]\n",
            "train loss: 0.933 accuracy: 0.569 [after 11 batches]\n",
            "train loss: 0.857 accuracy: 0.577 [after 15 batches]\n",
            "val loss: 561485.680 accuracy: 0.466 [after 4 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.617 accuracy: 0.613 [after 3 batches]\n",
            "train loss: 0.603 accuracy: 0.643 [after 7 batches]\n",
            "train loss: 0.615 accuracy: 0.656 [after 11 batches]\n",
            "train loss: 0.607 accuracy: 0.660 [after 15 batches]\n",
            "val loss: 159.963 accuracy: 0.446 [after 4 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.580 accuracy: 0.684 [after 3 batches]\n",
            "train loss: 0.596 accuracy: 0.641 [after 7 batches]\n",
            "train loss: 0.580 accuracy: 0.658 [after 11 batches]\n",
            "train loss: 0.582 accuracy: 0.650 [after 15 batches]\n",
            "val loss: 0.632 accuracy: 0.617 [after 4 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.567 accuracy: 0.648 [after 3 batches]\n",
            "train loss: 0.554 accuracy: 0.672 [after 7 batches]\n",
            "train loss: 0.571 accuracy: 0.667 [after 11 batches]\n",
            "train loss: 0.570 accuracy: 0.671 [after 15 batches]\n",
            "val loss: 2.301 accuracy: 0.795 [after 4 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.595 accuracy: 0.625 [after 3 batches]\n",
            "train loss: 0.582 accuracy: 0.662 [after 7 batches]\n",
            "train loss: 0.565 accuracy: 0.674 [after 11 batches]\n",
            "train loss: 0.558 accuracy: 0.677 [after 15 batches]\n",
            "val loss: 22.825 accuracy: 0.461 [after 4 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.553 accuracy: 0.684 [after 3 batches]\n",
            "train loss: 0.542 accuracy: 0.695 [after 7 batches]\n",
            "train loss: 0.550 accuracy: 0.678 [after 11 batches]\n",
            "train loss: 0.540 accuracy: 0.694 [after 15 batches]\n",
            "val loss: 4.962 accuracy: 0.436 [after 4 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.488 accuracy: 0.746 [after 3 batches]\n",
            "train loss: 0.623 accuracy: 0.684 [after 7 batches]\n",
            "train loss: 0.620 accuracy: 0.668 [after 11 batches]\n",
            "train loss: 0.624 accuracy: 0.646 [after 15 batches]\n",
            "val loss: 0.545 accuracy: 0.744 [after 4 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.616 accuracy: 0.637 [after 3 batches]\n",
            "train loss: 0.611 accuracy: 0.615 [after 7 batches]\n",
            "train loss: 0.608 accuracy: 0.613 [after 11 batches]\n",
            "train loss: 0.619 accuracy: 0.620 [after 15 batches]\n",
            "val loss: 47.853 accuracy: 0.441 [after 4 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.591 accuracy: 0.613 [after 3 batches]\n",
            "train loss: 0.582 accuracy: 0.629 [after 7 batches]\n",
            "train loss: 0.576 accuracy: 0.635 [after 11 batches]\n",
            "train loss: 0.569 accuracy: 0.650 [after 15 batches]\n",
            "val loss: 0.693 accuracy: 0.850 [after 4 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.548 accuracy: 0.684 [after 3 batches]\n",
            "train loss: 0.541 accuracy: 0.684 [after 7 batches]\n",
            "train loss: 0.552 accuracy: 0.686 [after 11 batches]\n",
            "train loss: 0.548 accuracy: 0.689 [after 15 batches]\n",
            "val loss: 0.695 accuracy: 0.554 [after 4 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.555 accuracy: 0.691 [after 3 batches]\n",
            "train loss: 0.578 accuracy: 0.650 [after 7 batches]\n",
            "train loss: 0.573 accuracy: 0.656 [after 11 batches]\n",
            "train loss: 0.565 accuracy: 0.665 [after 15 batches]\n",
            "val loss: 30.461 accuracy: 0.471 [after 4 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.549 accuracy: 0.691 [after 3 batches]\n",
            "train loss: 0.541 accuracy: 0.689 [after 7 batches]\n",
            "train loss: 0.537 accuracy: 0.693 [after 11 batches]\n",
            "train loss: 0.527 accuracy: 0.703 [after 15 batches]\n",
            "val loss: 4.399 accuracy: 0.461 [after 4 batches]\n",
            "test accuracy: 0.451 recall: 1.000 precision: 0.451 f1: 0.620 [after 4 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▂▃▄▄▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▂▃▄▅▅▆▅▅▅▆▅▆▄▆▆▆▆▆█▆▅▅▅▄▄▄▄▄▆▆▆▆▆▅▅▆▆▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▅▄▄▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▂▁▄▇▁▁▆▁█▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.45089\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.61962\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.45089\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.70312\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.52689\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.46094\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 4.39864\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33melated-sweep-64\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/cmsn6rnc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_224834-cmsn6rnc/logs\u001b[0m\n",
            "2024-06-10 23:01:44,108 - wandb.wandb_agent - INFO - Cleaning up finished run: cmsn6rnc\n",
            "2024-06-10 23:01:46,621 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 23:01:46,621 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 64\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.00667567388316583\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: False\n",
            "2024-06-10 23:01:46,623 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=64 --crop=True --learning_rate=0.00667567388316583 --normalize=True --optimizer=sgd --resize=False\n",
            "2024-06-10 23:01:51,635 - wandb.wandb_agent - INFO - Running runs: ['iwg2ire3']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_230153-iwg2ire3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfresh-sweep-65\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/iwg2ire3\u001b[0m\n",
            "FOLD 7\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "train loss: 0.695 accuracy: 0.531 [after 3 batches]\n",
            "train loss: 0.699 accuracy: 0.533 [after 7 batches]\n",
            "train loss: 0.714 accuracy: 0.521 [after 11 batches]\n",
            "train loss: 0.719 accuracy: 0.512 [after 15 batches]\n",
            "val loss: 0.686 accuracy: 0.665 [after 4 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.732 accuracy: 0.484 [after 3 batches]\n",
            "train loss: 0.723 accuracy: 0.477 [after 7 batches]\n",
            "train loss: 0.730 accuracy: 0.520 [after 11 batches]\n",
            "train loss: 0.722 accuracy: 0.520 [after 15 batches]\n",
            "val loss: 0.725 accuracy: 0.668 [after 4 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.664 accuracy: 0.582 [after 3 batches]\n",
            "train loss: 0.692 accuracy: 0.586 [after 7 batches]\n",
            "train loss: 0.698 accuracy: 0.564 [after 11 batches]\n",
            "train loss: 0.697 accuracy: 0.575 [after 15 batches]\n",
            "val loss: 0.773 accuracy: 0.667 [after 4 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.680 accuracy: 0.535 [after 3 batches]\n",
            "train loss: 0.671 accuracy: 0.596 [after 7 batches]\n",
            "train loss: 0.660 accuracy: 0.630 [after 11 batches]\n",
            "train loss: 0.640 accuracy: 0.645 [after 15 batches]\n",
            "val loss: 0.487 accuracy: 0.854 [after 4 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.550 accuracy: 0.742 [after 3 batches]\n",
            "train loss: 0.572 accuracy: 0.732 [after 7 batches]\n",
            "train loss: 0.565 accuracy: 0.736 [after 11 batches]\n",
            "train loss: 0.565 accuracy: 0.734 [after 15 batches]\n",
            "val loss: 0.331 accuracy: 0.918 [after 4 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.600 accuracy: 0.727 [after 3 batches]\n",
            "train loss: 0.583 accuracy: 0.730 [after 7 batches]\n",
            "train loss: 0.557 accuracy: 0.749 [after 11 batches]\n",
            "train loss: 0.568 accuracy: 0.733 [after 15 batches]\n",
            "val loss: 0.388 accuracy: 0.917 [after 4 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.464 accuracy: 0.797 [after 3 batches]\n",
            "train loss: 0.489 accuracy: 0.781 [after 7 batches]\n",
            "train loss: 0.481 accuracy: 0.793 [after 11 batches]\n",
            "train loss: 0.469 accuracy: 0.798 [after 15 batches]\n",
            "val loss: 0.367 accuracy: 0.908 [after 4 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.496 accuracy: 0.754 [after 3 batches]\n",
            "train loss: 0.494 accuracy: 0.768 [after 7 batches]\n",
            "train loss: 0.541 accuracy: 0.753 [after 11 batches]\n",
            "train loss: 0.542 accuracy: 0.747 [after 15 batches]\n",
            "val loss: 0.279 accuracy: 0.913 [after 4 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.438 accuracy: 0.820 [after 3 batches]\n",
            "train loss: 0.447 accuracy: 0.803 [after 7 batches]\n",
            "train loss: 0.449 accuracy: 0.807 [after 11 batches]\n",
            "train loss: 0.440 accuracy: 0.808 [after 15 batches]\n",
            "val loss: 0.351 accuracy: 0.913 [after 4 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.430 accuracy: 0.793 [after 3 batches]\n",
            "train loss: 0.427 accuracy: 0.795 [after 7 batches]\n",
            "train loss: 0.423 accuracy: 0.805 [after 11 batches]\n",
            "train loss: 0.402 accuracy: 0.821 [after 15 batches]\n",
            "val loss: 0.389 accuracy: 0.913 [after 4 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.402 accuracy: 0.832 [after 3 batches]\n",
            "train loss: 0.388 accuracy: 0.838 [after 7 batches]\n",
            "train loss: 0.367 accuracy: 0.846 [after 11 batches]\n",
            "train loss: 0.370 accuracy: 0.842 [after 15 batches]\n",
            "val loss: 0.614 accuracy: 0.682 [after 4 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.386 accuracy: 0.816 [after 3 batches]\n",
            "train loss: 0.388 accuracy: 0.809 [after 7 batches]\n",
            "train loss: 0.347 accuracy: 0.840 [after 11 batches]\n",
            "train loss: 0.333 accuracy: 0.846 [after 15 batches]\n",
            "val loss: 0.860 accuracy: 0.828 [after 4 batches]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "train loss: 0.440 accuracy: 0.859 [after 3 batches]\n",
            "train loss: 0.364 accuracy: 0.869 [after 7 batches]\n",
            "train loss: 0.358 accuracy: 0.862 [after 11 batches]\n",
            "train loss: 0.333 accuracy: 0.869 [after 15 batches]\n",
            "val loss: 0.507 accuracy: 0.865 [after 4 batches]\n",
            "test accuracy: 0.772 recall: 0.766 precision: 0.876 f1: 0.804 [after 4 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▂▃▃▄▅▅▆▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▂▂▂▂▁▂▂▃▃▃▃▄▄▆▆▆▅▆▆▇▇▇▆▆▆▇▇▇▇▇▇▇▇█▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▇▇█████▇▇▇▇▇▆▅▅▅▆▅▅▃▄▃▄▅▅▃▃▃▃▃▂▂▂▂▂▂▁▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▁▆██████▁▆▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▆▆▇▄▂▂▂▁▂▂▅█▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 51\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.77232\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.80439\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.87601\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.76602\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.86914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.33253\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.86458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.50748\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfresh-sweep-65\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/iwg2ire3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_230153-iwg2ire3/logs\u001b[0m\n",
            "2024-06-10 23:13:10,650 - wandb.wandb_agent - INFO - Cleaning up finished run: iwg2ire3\n",
            "2024-06-10 23:13:13,420 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 23:13:13,420 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 16\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.006351489923009888\n",
            "\tnormalize: False\n",
            "\toptimizer: adam\n",
            "\tresize: 150\n",
            "2024-06-10 23:13:13,422 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=16 --crop=True --learning_rate=0.006351489923009888 --normalize=False --optimizer=adam --resize=150\n",
            "2024-06-10 23:13:18,431 - wandb.wandb_agent - INFO - Running runs: ['h5nkvuvo']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_231318-h5nkvuvo\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglamorous-sweep-66\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/h5nkvuvo\u001b[0m\n",
            "FOLD 6\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 1.029 accuracy: 0.562 [after 15 batches]\n",
            "train loss: 0.875 accuracy: 0.568 [after 31 batches]\n",
            "train loss: 0.800 accuracy: 0.547 [after 47 batches]\n",
            "train loss: 0.763 accuracy: 0.546 [after 63 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "val loss: 0.629 accuracy: 0.375 [after 15 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.615 accuracy: 0.605 [after 15 batches]\n",
            "train loss: 0.592 accuracy: 0.633 [after 31 batches]\n",
            "train loss: 0.594 accuracy: 0.618 [after 47 batches]\n",
            "train loss: 0.597 accuracy: 0.625 [after 63 batches]\n",
            "val loss: 0.535 accuracy: 0.808 [after 15 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.440 accuracy: 0.754 [after 15 batches]\n",
            "train loss: 0.449 accuracy: 0.754 [after 31 batches]\n",
            "train loss: 0.486 accuracy: 0.763 [after 47 batches]\n",
            "train loss: 0.472 accuracy: 0.787 [after 63 batches]\n",
            "val loss: 0.291 accuracy: 0.904 [after 15 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.369 accuracy: 0.848 [after 15 batches]\n",
            "train loss: 0.342 accuracy: 0.873 [after 31 batches]\n",
            "train loss: 0.333 accuracy: 0.878 [after 47 batches]\n",
            "train loss: 0.310 accuracy: 0.888 [after 63 batches]\n",
            "val loss: 0.265 accuracy: 0.917 [after 15 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.272 accuracy: 0.906 [after 15 batches]\n",
            "train loss: 0.282 accuracy: 0.895 [after 31 batches]\n",
            "train loss: 0.305 accuracy: 0.902 [after 47 batches]\n",
            "train loss: 0.323 accuracy: 0.886 [after 63 batches]\n",
            "val loss: 0.447 accuracy: 0.917 [after 15 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.454 accuracy: 0.789 [after 15 batches]\n",
            "train loss: 0.372 accuracy: 0.844 [after 31 batches]\n",
            "train loss: 0.392 accuracy: 0.836 [after 47 batches]\n",
            "train loss: 0.407 accuracy: 0.831 [after 63 batches]\n",
            "val loss: 0.381 accuracy: 0.858 [after 15 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.395 accuracy: 0.805 [after 15 batches]\n",
            "train loss: 0.355 accuracy: 0.855 [after 31 batches]\n",
            "train loss: 0.345 accuracy: 0.859 [after 47 batches]\n",
            "train loss: 0.331 accuracy: 0.868 [after 63 batches]\n",
            "val loss: 0.295 accuracy: 0.913 [after 15 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.291 accuracy: 0.879 [after 15 batches]\n",
            "train loss: 0.355 accuracy: 0.863 [after 31 batches]\n",
            "train loss: 0.348 accuracy: 0.867 [after 47 batches]\n",
            "train loss: 0.336 accuracy: 0.873 [after 63 batches]\n",
            "val loss: 0.274 accuracy: 0.913 [after 15 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.278 accuracy: 0.895 [after 15 batches]\n",
            "train loss: 0.271 accuracy: 0.904 [after 31 batches]\n",
            "train loss: 0.272 accuracy: 0.904 [after 47 batches]\n",
            "train loss: 0.257 accuracy: 0.913 [after 63 batches]\n",
            "val loss: 0.719 accuracy: 0.775 [after 15 batches]\n",
            "test accuracy: 0.766 recall: 0.986 precision: 0.670 f1: 0.791 [after 14 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▄▅▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▁▁▂▃▂▃▅▅▅▆▇▇▇████▇▆▇▇▆▆▇▇▇▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▆▄▄▄▄▃▃▃▃▂▂▂▁▁▁▁▂▃▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▇███▇██▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▇▅▁▁▄▃▁▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.76637\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.79069\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.67046\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.98571\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.91309\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.25651\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.775\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.71856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mglamorous-sweep-66\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/h5nkvuvo\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_231318-h5nkvuvo/logs\u001b[0m\n",
            "2024-06-10 23:21:50,224 - wandb.wandb_agent - INFO - Cleaning up finished run: h5nkvuvo\n",
            "2024-06-10 23:21:52,813 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 23:21:52,814 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 64\n",
            "\tcrop: False\n",
            "\tlearning_rate: 0.0077422563580061745\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: False\n",
            "2024-06-10 23:21:52,816 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=64 --crop=False --learning_rate=0.0077422563580061745 --normalize=True --optimizer=sgd --resize=False\n",
            "2024-06-10 23:21:57,826 - wandb.wandb_agent - INFO - Running runs: ['d9dnrwuz']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_232158-d9dnrwuz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdainty-sweep-67\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/d9dnrwuz\u001b[0m\n",
            "FOLD 3\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 52, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 186, in forward\n",
            "    x = self.entry_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 90, in forward\n",
            "    x = self.block1(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 73, in forward\n",
            "    x2 =self.double_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 38, in forward\n",
            "    x = self.one_by_one(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 460, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.29 GiB. GPU \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdainty-sweep-67\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/d9dnrwuz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_232158-d9dnrwuz/logs\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 62, in <module>\n",
            "    model_pipeline(default_config)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 52, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 186, in forward\n",
            "    x = self.entry_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 90, in forward\n",
            "    x = self.block1(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 73, in forward\n",
            "    x2 =self.double_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 38, in forward\n",
            "    x = self.one_by_one(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 460, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.29 GiB. GPU \n",
            "2024-06-10 23:22:18,081 - wandb.wandb_agent - INFO - Cleaning up finished run: d9dnrwuz\n",
            "2024-06-10 23:22:20,826 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 23:22:20,826 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 8\n",
            "\tcrop: True\n",
            "\tlearning_rate: 0.004158907507926989\n",
            "\tnormalize: False\n",
            "\toptimizer: sgd\n",
            "\tresize: False\n",
            "2024-06-10 23:22:20,828 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=8 --crop=True --learning_rate=0.004158907507926989 --normalize=False --optimizer=sgd --resize=False\n",
            "2024-06-10 23:22:25,837 - wandb.wandb_agent - INFO - Running runs: ['ajy7okbw']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_232227-ajy7okbw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstill-sweep-68\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/27e1qtt0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/ajy7okbw\u001b[0m\n",
            "FOLD 2\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.840 accuracy: 0.496 [after 33 batches]\n",
            "train loss: 0.772 accuracy: 0.548 [after 67 batches]\n",
            "train loss: 0.777 accuracy: 0.565 [after 101 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "train loss: 0.748 accuracy: 0.574 [after 135 batches]\n",
            "val loss: 0.870 accuracy: 0.540 [after 28 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.726 accuracy: 0.526 [after 33 batches]\n",
            "train loss: 0.716 accuracy: 0.551 [after 67 batches]\n",
            "train loss: 0.677 accuracy: 0.585 [after 101 batches]\n",
            "train loss: 0.678 accuracy: 0.591 [after 135 batches]\n",
            "val loss: 0.306 accuracy: 0.893 [after 28 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.666 accuracy: 0.632 [after 33 batches]\n",
            "train loss: 0.800 accuracy: 0.586 [after 67 batches]\n",
            "train loss: 0.777 accuracy: 0.580 [after 101 batches]\n",
            "train loss: 0.744 accuracy: 0.583 [after 135 batches]\n",
            "val loss: 0.304 accuracy: 0.902 [after 28 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.721 accuracy: 0.529 [after 33 batches]\n",
            "train loss: 0.695 accuracy: 0.572 [after 67 batches]\n",
            "train loss: 0.675 accuracy: 0.574 [after 101 batches]\n",
            "train loss: 0.677 accuracy: 0.585 [after 135 batches]\n",
            "val loss: 0.473 accuracy: 0.857 [after 28 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.631 accuracy: 0.636 [after 33 batches]\n",
            "train loss: 0.642 accuracy: 0.623 [after 67 batches]\n",
            "train loss: 0.622 accuracy: 0.625 [after 101 batches]\n",
            "train loss: 0.634 accuracy: 0.631 [after 135 batches]\n",
            "val loss: 0.339 accuracy: 0.804 [after 28 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.825 accuracy: 0.566 [after 33 batches]\n",
            "train loss: 0.747 accuracy: 0.557 [after 67 batches]\n",
            "train loss: 0.695 accuracy: 0.588 [after 101 batches]\n",
            "train loss: 0.674 accuracy: 0.602 [after 135 batches]\n",
            "val loss: 0.236 accuracy: 0.879 [after 28 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.656 accuracy: 0.680 [after 33 batches]\n",
            "train loss: 0.646 accuracy: 0.638 [after 67 batches]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}