{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dftDiCQg3vYy"
      },
      "source": [
        "# <font color='#4C5FDA'>**Breast Cancer Detection Based on CNNs Using Thermal Imaging** </font>\n",
        "\n",
        "Original paper by Juan Pablo Zuluaga, Zeina Al Masry, Khaled Benaggoune, Safa Meraghni & Noureddine Zerhouni: [A CNN-based methodology for breast cancer diagnosis using thermal images](https://www.tandfonline.com/doi/full/10.1080/21681163.2020.1824685)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uraq36CkXB1T",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Instalar paquetes necesarios**\n",
        "\n",
        "%%capture\n",
        "! pip install torchmetrics\n",
        "! pip install wandb -Uq\n",
        "# ! pip install onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#ECA702'>**Clonamos nuestro repo**</font>\n",
        "\n",
        "Esto con el fin de traer todos los .py para poder entrenar 'localmente' en Colab y registrar las métricas en wandb."
      ],
      "metadata": {
        "id": "3eUcjX2Lv5g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/gpintoruiz/Thermal-Imaging-Breast-Cancer-Detection.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miS2W9Tdwbxq",
        "outputId": "5886cc4f-e0e4-40bb-e412-bc5b800c0930"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Thermal-Imaging-Breast-Cancer-Detection'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 121 (delta 57), reused 70 (delta 24), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (121/121), 2.27 MiB | 16.76 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Thermal-Imaging-Breast-Cancer-Detection/notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMuOAySxx1Z1",
        "outputId": "49d2fbeb-1083-4547-d477-7de805fe5957"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssFb9A5GyA_u",
        "outputId": "76cb4a4a-c214-4122-acd8-a524bc925684"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01-gpr-data-exploration-cv2.ipynb  1.00-gpr-xception-from-scratch.ipynb  utils.py\n",
            "0.01-gpr-data-exploration-pil.ipynb  make_dataset.py                       validation.py\n",
            "0.02-gpr-experiments-base.ipynb      test.py                               xception-one-run.yaml\n",
            "0.03-gpr-initial-experiment.ipynb    train_one_run.py                      xception.py\n",
            "0.04-gpr-colab-experiments.ipynb     train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkrHeEps3vY3"
      },
      "source": [
        "## <font color='#ECA702'>**Configuración inicial para conectarnos con Kaggle**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJFg6k1x3vY5"
      },
      "source": [
        "1. Instalamos kaggle. Para poder usar comandos de Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hwqionQb3vY6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP3nl2Et3vY7"
      },
      "source": [
        "Subimos nuestro token de autenticación de Kaggle (si estamos en colab, sino colocarlo en la carpeta del proyecto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARP6wZsb3vY8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0VPkGso3vY9"
      },
      "source": [
        "1. Creamos los directorios de Kaggle\n",
        "2. Copiamos nuestro token en .kaggle\n",
        "3. Con `chmod 600` establecemos los permitos del token en 600, es decir, que solo yo tengo permisos de lectura y escritura sobre el archivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ed2nCVTu3vY-"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UhQ_SI9x3vZA"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGkGIazz3vZB"
      },
      "source": [
        "## <font color='#ECA702'>**Carga del dataset**</font>\n",
        "\n",
        "Traemos el dataset [Thermal Images for Breast Cancer Diagnosis DMR-IR](https://www.kaggle.com/datasets/asdeepak/thermal-images-for-breast-cancer-diagnosis-dmrir) desde kaggle.\n",
        "\n",
        "This dataset is a methodology for breast disease computer-aided diagnosis using dynamic thermography. The thermal images for breast tumors are classified according to DMR-IR standards.\n",
        "\n",
        "Two types of tumors are classified in this dataset one is benign another is malignant.\n",
        "- Benign: This type of tumor is usually well-defined and round or oval in shape. (non-cancerous tumor)\n",
        "- Malignant: This type of tumor is usually poorly defined and irregular with lobules. (cancerous tumor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "lmT0aOvG3vZD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! kaggle datasets download -d asdeepak/thermal-images-for-breast-cancer-diagnosis-dmrir\n",
        "! unzip thermal-images-for-breast-cancer-diagnosis-dmrir.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QToIa8zB3vZE"
      },
      "source": [
        "Después de descargar los datos. Debemos entender la estructura de las carpetas para poder trabajar con ellas de una mejor manera.\n",
        "1. La carpeta principal `Imagens e Matrizes da Tese de Thiago Alves Elias da Silva` son todos los datos `data`.\n",
        "2. La carpeta `12 Novos Casos de Testes` la podemos tomar como nuestro conjunto de prueba (`test`).\n",
        "3. Mientras que la carpeta `Desenvolvimento da Metodologia` será nuestro conjunto de entrenamiento (`train`).\n",
        "\n",
        "Luego dentro de nuestras carpetas de `train` y `test` encontramos dos categorías `DOENTES`y `SAUDAтХа├╝VEIS` o SAUDÁVEI. Los primeros son los casos malignos y los segundos benignos.\n",
        "\n",
        "Dentro de cada una de las carpetas de pacientes saludables y enfermos se encuentran carpetas con números, cada número representa un paciente. Y para cada paciente tendremos dos carpetas más, una para las imágenes **segmentadas** en escala de grises y la otra para la matrix o mapa de calor.\n",
        "\n",
        "Algo bueno de este dataset es que ya está dividido por pacientes, es decir, no tendremos imagenes del mismo paciente en el conjunto de entrenamiento y testeo. Por lo tanto, vamos a entrenar con N pacientes, y testear con K pacientes, que no son los mismos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#ECA702'>**Inicializamos el agende de wandb**</font>"
      ],
      "metadata": {
        "id": "5YTKbsdDyqSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='#52F17F'>**1. Nos logeamos en nuestra cuenta**</font>"
      ],
      "metadata": {
        "id": "nIskv2fHy4Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "St3agN3Rypzq",
        "outputId": "ed0e999d-b990-416a-c79d-fc718b01ad00"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='#52F17F'>**2. Hacemos call del agente**</font>\n"
      ],
      "metadata": {
        "id": "w0r-E8Ufy9TQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El sweep que estoy probando acá es el [siguiente](https://github.com/gpintoruiz/Thermal-Imaging-Breast-Cancer-Detection/blob/main/notebooks/xception-one-run.yaml). Se pueden cambiar los parámetros a probar como tú quieras de acuerdo con la [documentación](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration) (recomiendo solo cambiar la arquitectura para que las comparaciones entre modelos sean equivalentes).\n",
        "\n",
        "Si no tienes ni idea qué es un sweep mira el siguiente [tutorial](https://www.youtube.com/watch?v=9zrmUIlScdY&t=1361s&ab_channel=Weights%26Biases).\n",
        "\n",
        "El comando `--count` sirve para decirle al agente cuántos runs hacer, aplica especialmente cuando el método del sweep es `bayes` o `random`\n"
      ],
      "metadata": {
        "id": "LxtyncVoCVL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wandb agent aiuis/dip-project/epbt9jh6 --count 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpG6dTZdy0R3",
        "outputId": "c4134fe9-6578-45dc-dc12-6c2d3cbc8210"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
            "2024-06-09 18:36:23,481 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2024-06-09 18:36:23,755 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 18:36:23,755 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 8\n",
            "\tfold: 3\n",
            "\tlearning_rate: 0.0015528833190894193\n",
            "\tnormalize: False\n",
            "\toptimizer: adam\n",
            "\tresize: 300\n",
            "2024-06-09 18:36:23,758 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=8 --fold=3 --learning_rate=0.0015528833190894193 --normalize=False --optimizer=adam --resize=300\n",
            "2024-06-09 18:36:28,773 - wandb.wandb_agent - INFO - Running runs: ['psadoyj7']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_183633-psadoyj7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdandy-sweep-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/psadoyj7\u001b[0m\n",
            "FOLD 3\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.802 accuracy: 0.492 [after 32 batches]\n",
            "train loss: 0.763 accuracy: 0.504 [after 65 batches]\n",
            "train loss: 0.746 accuracy: 0.514 [after 98 batches]\n",
            "train loss: 0.735 accuracy: 0.498 [after 131 batches]\n",
            "val loss: 0.705 accuracy: 0.500 [after 30 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.701 accuracy: 0.477 [after 32 batches]\n",
            "train loss: 0.693 accuracy: 0.545 [after 65 batches]\n",
            "train loss: 0.703 accuracy: 0.540 [after 98 batches]\n",
            "train loss: 0.705 accuracy: 0.524 [after 131 batches]\n",
            "val loss: 0.740 accuracy: 0.663 [after 30 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.698 accuracy: 0.504 [after 32 batches]\n",
            "train loss: 0.688 accuracy: 0.500 [after 65 batches]\n",
            "train loss: 0.695 accuracy: 0.508 [after 98 batches]\n",
            "train loss: 0.695 accuracy: 0.510 [after 131 batches]\n",
            "val loss: 0.734 accuracy: 0.542 [after 30 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.679 accuracy: 0.568 [after 32 batches]\n",
            "train loss: 0.684 accuracy: 0.585 [after 65 batches]\n",
            "train loss: 0.685 accuracy: 0.576 [after 98 batches]\n",
            "train loss: 0.680 accuracy: 0.584 [after 131 batches]\n",
            "val loss: 0.633 accuracy: 0.708 [after 30 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.685 accuracy: 0.614 [after 32 batches]\n",
            "train loss: 0.685 accuracy: 0.581 [after 65 batches]\n",
            "train loss: 0.687 accuracy: 0.577 [after 98 batches]\n",
            "train loss: 0.692 accuracy: 0.562 [after 131 batches]\n",
            "val loss: 0.666 accuracy: 0.646 [after 30 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.674 accuracy: 0.606 [after 32 batches]\n",
            "train loss: 0.678 accuracy: 0.597 [after 65 batches]\n",
            "train loss: 0.689 accuracy: 0.585 [after 98 batches]\n",
            "train loss: 0.683 accuracy: 0.597 [after 131 batches]\n",
            "val loss: 0.746 accuracy: 0.667 [after 30 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.692 accuracy: 0.553 [after 32 batches]\n",
            "train loss: 0.691 accuracy: 0.509 [after 65 batches]\n",
            "train loss: 0.694 accuracy: 0.504 [after 98 batches]\n",
            "train loss: 0.686 accuracy: 0.528 [after 131 batches]\n",
            "val loss: 0.651 accuracy: 0.671 [after 30 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.700 accuracy: 0.561 [after 32 batches]\n",
            "train loss: 0.701 accuracy: 0.545 [after 65 batches]\n",
            "train loss: 0.697 accuracy: 0.554 [after 98 batches]\n",
            "train loss: 0.692 accuracy: 0.560 [after 131 batches]\n",
            "val loss: 0.777 accuracy: 0.667 [after 30 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.692 accuracy: 0.557 [after 32 batches]\n",
            "train loss: 0.692 accuracy: 0.559 [after 65 batches]\n",
            "train loss: 0.687 accuracy: 0.578 [after 98 batches]\n",
            "train loss: 0.689 accuracy: 0.560 [after 131 batches]\n",
            "val loss: 0.654 accuracy: 0.704 [after 30 batches]\n",
            "test accuracy: 0.714 recall: 0.838 precision: 0.655 f1: 0.709 [after 28 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▄▅▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▂▂▃▂▁▅▄▃▂▂▃▃▆▇▆▆█▆▆▅█▇▇▇▅▃▂▄▅▅▅▅▅▅▆▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▆▅▄▃▂▃▃▂▂▂▂▁▂▂▁▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▆▂█▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▅▆▆▁▃▇▂█▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.71429\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.70928\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.65476\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.8375\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.55966\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.68917\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.70417\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.65405\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdandy-sweep-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/psadoyj7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_183633-psadoyj7/logs\u001b[0m\n",
            "2024-06-09 18:49:35,592 - wandb.wandb_agent - INFO - Cleaning up finished run: psadoyj7\n",
            "2024-06-09 18:49:36,026 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 18:49:36,026 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 55\n",
            "\tfold: 6\n",
            "\tlearning_rate: 0.0012656963553523929\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: 50\n",
            "2024-06-09 18:49:36,028 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=55 --fold=6 --learning_rate=0.0012656963553523929 --normalize=True --optimizer=sgd --resize=50\n",
            "2024-06-09 18:49:41,037 - wandb.wandb_agent - INFO - Running runs: ['qb6bpq52']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_184942-qb6bpq52\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwise-sweep-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/qb6bpq52\u001b[0m\n",
            "FOLD 6\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "train loss: 0.684 accuracy: 0.531 [after 4 batches]\n",
            "train loss: 0.683 accuracy: 0.562 [after 9 batches]\n",
            "train loss: 0.674 accuracy: 0.594 [after 14 batches]\n",
            "train loss: 0.671 accuracy: 0.597 [after 19 batches]\n",
            "val loss: 0.690 accuracy: 0.545 [after 4 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.699 accuracy: 0.600 [after 4 batches]\n",
            "train loss: 0.661 accuracy: 0.655 [after 9 batches]\n",
            "train loss: 0.638 accuracy: 0.667 [after 14 batches]\n",
            "train loss: 0.631 accuracy: 0.669 [after 19 batches]\n",
            "val loss: 0.716 accuracy: 0.545 [after 4 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.622 accuracy: 0.676 [after 4 batches]\n",
            "train loss: 0.609 accuracy: 0.680 [after 9 batches]\n",
            "train loss: 0.607 accuracy: 0.674 [after 14 batches]\n",
            "train loss: 0.601 accuracy: 0.677 [after 19 batches]\n",
            "val loss: 0.775 accuracy: 0.545 [after 4 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.577 accuracy: 0.705 [after 4 batches]\n",
            "train loss: 0.569 accuracy: 0.718 [after 9 batches]\n",
            "train loss: 0.576 accuracy: 0.708 [after 14 batches]\n",
            "train loss: 0.552 accuracy: 0.730 [after 19 batches]\n",
            "val loss: 0.517 accuracy: 0.759 [after 4 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.516 accuracy: 0.731 [after 4 batches]\n",
            "train loss: 0.501 accuracy: 0.751 [after 9 batches]\n",
            "train loss: 0.484 accuracy: 0.764 [after 14 batches]\n",
            "train loss: 0.499 accuracy: 0.757 [after 19 batches]\n",
            "val loss: 0.471 accuracy: 0.814 [after 4 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.514 accuracy: 0.742 [after 4 batches]\n",
            "train loss: 0.506 accuracy: 0.747 [after 9 batches]\n",
            "train loss: 0.515 accuracy: 0.742 [after 14 batches]\n",
            "train loss: 0.526 accuracy: 0.730 [after 19 batches]\n",
            "val loss: 0.598 accuracy: 0.750 [after 4 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.537 accuracy: 0.745 [after 4 batches]\n",
            "train loss: 0.487 accuracy: 0.775 [after 9 batches]\n",
            "train loss: 0.462 accuracy: 0.789 [after 14 batches]\n",
            "train loss: 0.473 accuracy: 0.786 [after 19 batches]\n",
            "val loss: 0.501 accuracy: 0.845 [after 4 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.507 accuracy: 0.767 [after 4 batches]\n",
            "train loss: 0.490 accuracy: 0.780 [after 9 batches]\n",
            "train loss: 0.473 accuracy: 0.787 [after 14 batches]\n",
            "train loss: 0.479 accuracy: 0.781 [after 19 batches]\n",
            "val loss: 0.529 accuracy: 0.705 [after 4 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.466 accuracy: 0.778 [after 4 batches]\n",
            "train loss: 0.476 accuracy: 0.780 [after 9 batches]\n",
            "train loss: 0.462 accuracy: 0.793 [after 14 batches]\n",
            "train loss: 0.452 accuracy: 0.795 [after 19 batches]\n",
            "val loss: 0.730 accuracy: 0.618 [after 4 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.464 accuracy: 0.804 [after 4 batches]\n",
            "train loss: 0.477 accuracy: 0.796 [after 9 batches]\n",
            "train loss: 0.467 accuracy: 0.795 [after 14 batches]\n",
            "train loss: 0.465 accuracy: 0.796 [after 19 batches]\n",
            "val loss: 0.685 accuracy: 0.664 [after 4 batches]\n",
            "test accuracy: 0.836 recall: 0.658 precision: 0.972 f1: 0.782 [after 4 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▆▇▆▆▇▇██▇▇█▇▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ██▇▇█▇▆▆▆▅▅▅▅▄▅▄▃▂▂▂▃▃▃▃▃▂▁▂▃▂▂▂▁▂▁▁▁▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▁▆▇▆█▅▃▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▆▇█▂▁▄▂▂▇▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 39\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.83636\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.78228\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.97247\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.65814\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.79604\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.46546\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.66364\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.68494\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwise-sweep-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/qb6bpq52\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_184942-qb6bpq52/logs\u001b[0m\n",
            "2024-06-09 19:00:18,316 - wandb.wandb_agent - INFO - Cleaning up finished run: qb6bpq52\n",
            "2024-06-09 19:00:19,010 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 19:00:19,010 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 29\n",
            "\tfold: 6\n",
            "\tlearning_rate: 0.004017324926223572\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: None\n",
            "2024-06-09 19:00:19,012 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=29 --fold=6 --learning_rate=0.004017324926223572 --normalize=True --optimizer=sgd --resize=None\n",
            "2024-06-09 19:00:24,021 - wandb.wandb_agent - INFO - Running runs: ['3yk2tapq']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_190024-3yk2tapq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdecent-sweep-4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/3yk2tapq\u001b[0m\n",
            "FOLD 6\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 186, in forward\n",
            "    x = self.entry_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 92, in forward\n",
            "    x = self.block3(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 73, in forward\n",
            "    x2 =self.double_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/pooling.py\", line 164, in forward\n",
            "    return F.max_pool2d(input, self.kernel_size, self.stride,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_jit_internal.py\", line 497, in fn\n",
            "    return if_false(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 796, in _max_pool2d\n",
            "    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdecent-sweep-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/3yk2tapq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_190024-3yk2tapq/logs\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 63, in <module>\n",
            "    model_pipeline(default_config)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 186, in forward\n",
            "    x = self.entry_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 92, in forward\n",
            "    x = self.block3(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 73, in forward\n",
            "    x2 =self.double_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/pooling.py\", line 164, in forward\n",
            "    return F.max_pool2d(input, self.kernel_size, self.stride,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_jit_internal.py\", line 497, in fn\n",
            "    return if_false(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 796, in _max_pool2d\n",
            "    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 98.00 MiB. GPU \n",
            "2024-06-09 19:00:44,574 - wandb.wandb_agent - INFO - Cleaning up finished run: 3yk2tapq\n",
            "2024-06-09 19:00:45,202 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 19:00:45,202 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 13\n",
            "\tfold: 4\n",
            "\tlearning_rate: 0.0016747957488558048\n",
            "\tnormalize: False\n",
            "\toptimizer: adam\n",
            "\tresize: 300\n",
            "2024-06-09 19:00:45,204 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=13 --fold=4 --learning_rate=0.0016747957488558048 --normalize=False --optimizer=adam --resize=300\n",
            "2024-06-09 19:00:50,213 - wandb.wandb_agent - INFO - Running runs: ['x6rc790a']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_190051-x6rc790a\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mscarlet-sweep-5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/x6rc790a\u001b[0m\n",
            "FOLD 4\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.803 accuracy: 0.523 [after 19 batches]\n",
            "train loss: 0.737 accuracy: 0.531 [after 39 batches]\n",
            "train loss: 0.727 accuracy: 0.538 [after 59 batches]\n",
            "train loss: 0.727 accuracy: 0.519 [after 79 batches]\n",
            "val loss: 0.641 accuracy: 0.671 [after 19 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.642 accuracy: 0.558 [after 19 batches]\n",
            "train loss: 0.644 accuracy: 0.583 [after 39 batches]\n",
            "train loss: 0.676 accuracy: 0.547 [after 59 batches]\n",
            "train loss: 0.681 accuracy: 0.533 [after 79 batches]\n",
            "val loss: 8.416 accuracy: 0.343 [after 19 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.653 accuracy: 0.588 [after 19 batches]\n",
            "train loss: 0.663 accuracy: 0.594 [after 39 batches]\n",
            "train loss: 0.677 accuracy: 0.568 [after 59 batches]\n",
            "train loss: 0.661 accuracy: 0.579 [after 79 batches]\n",
            "val loss: 44.263 accuracy: 0.324 [after 19 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.670 accuracy: 0.600 [after 19 batches]\n",
            "train loss: 0.657 accuracy: 0.583 [after 39 batches]\n",
            "train loss: 0.640 accuracy: 0.600 [after 59 batches]\n",
            "train loss: 0.641 accuracy: 0.604 [after 79 batches]\n",
            "val loss: 0.670 accuracy: 0.671 [after 19 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.676 accuracy: 0.596 [after 19 batches]\n",
            "train loss: 0.639 accuracy: 0.606 [after 39 batches]\n",
            "train loss: 0.628 accuracy: 0.624 [after 59 batches]\n",
            "train loss: 0.621 accuracy: 0.624 [after 79 batches]\n",
            "val loss: 0.667 accuracy: 0.662 [after 19 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.625 accuracy: 0.585 [after 19 batches]\n",
            "train loss: 0.619 accuracy: 0.600 [after 39 batches]\n",
            "train loss: 0.613 accuracy: 0.618 [after 59 batches]\n",
            "train loss: 0.618 accuracy: 0.617 [after 79 batches]\n",
            "val loss: 1.307 accuracy: 0.426 [after 19 batches]\n",
            "test accuracy: 0.455 recall: 1.000 precision: 0.455 f1: 0.609 [after 17 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▂▁▄▅▃▂▆▆▄▅▆▅▆▇▆▇██▅▆██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▆▅▅▂▂▃▃▂▃▃▃▃▃▂▂▃▂▂▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy █▁▁██▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁▂█▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.45513\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.60911\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.45513\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.61731\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.61759\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.42578\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.30676\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mscarlet-sweep-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/x6rc790a\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_190051-x6rc790a/logs\u001b[0m\n",
            "2024-06-09 19:09:05,935 - wandb.wandb_agent - INFO - Cleaning up finished run: x6rc790a\n",
            "2024-06-09 19:09:06,582 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 19:09:06,582 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 11\n",
            "\tfold: 3\n",
            "\tlearning_rate: 0.0020258642909932677\n",
            "\tnormalize: True\n",
            "\toptimizer: adam\n",
            "\tresize: 150\n",
            "2024-06-09 19:09:06,584 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=11 --fold=3 --learning_rate=0.0020258642909932677 --normalize=True --optimizer=adam --resize=150\n",
            "2024-06-09 19:09:11,593 - wandb.wandb_agent - INFO - Running runs: ['pp7clts9']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_190913-pp7clts9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvocal-sweep-7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/pp7clts9\u001b[0m\n",
            "FOLD 3\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.879 accuracy: 0.515 [after 23 batches]\n",
            "train loss: 0.883 accuracy: 0.525 [after 47 batches]\n",
            "train loss: 0.821 accuracy: 0.509 [after 71 batches]\n",
            "train loss: 0.789 accuracy: 0.521 [after 95 batches]\n",
            "val loss: 0.706 accuracy: 0.378 [after 22 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.693 accuracy: 0.508 [after 23 batches]\n",
            "train loss: 0.688 accuracy: 0.536 [after 47 batches]\n",
            "train loss: 0.681 accuracy: 0.547 [after 71 batches]\n",
            "train loss: 0.690 accuracy: 0.546 [after 95 batches]\n",
            "val loss: 0.731 accuracy: 0.667 [after 22 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.624 accuracy: 0.667 [after 23 batches]\n",
            "train loss: 0.642 accuracy: 0.650 [after 47 batches]\n",
            "train loss: 0.656 accuracy: 0.633 [after 71 batches]\n",
            "train loss: 0.666 accuracy: 0.611 [after 95 batches]\n",
            "val loss: 0.804 accuracy: 0.668 [after 22 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.704 accuracy: 0.576 [after 23 batches]\n",
            "train loss: 0.721 accuracy: 0.534 [after 47 batches]\n",
            "train loss: 0.710 accuracy: 0.557 [after 71 batches]\n",
            "train loss: 0.709 accuracy: 0.565 [after 95 batches]\n",
            "val loss: 0.638 accuracy: 0.668 [after 22 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.687 accuracy: 0.576 [after 23 batches]\n",
            "train loss: 0.670 accuracy: 0.564 [after 47 batches]\n",
            "train loss: 0.664 accuracy: 0.605 [after 71 batches]\n",
            "train loss: 0.645 accuracy: 0.617 [after 95 batches]\n",
            "val loss: 0.795 accuracy: 0.606 [after 22 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.617 accuracy: 0.663 [after 23 batches]\n",
            "train loss: 0.616 accuracy: 0.667 [after 47 batches]\n",
            "train loss: 0.613 accuracy: 0.667 [after 71 batches]\n",
            "train loss: 0.614 accuracy: 0.669 [after 95 batches]\n",
            "val loss: 0.644 accuracy: 0.618 [after 22 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.556 accuracy: 0.731 [after 23 batches]\n",
            "train loss: 0.588 accuracy: 0.710 [after 47 batches]\n",
            "train loss: 0.592 accuracy: 0.713 [after 71 batches]\n",
            "train loss: 0.585 accuracy: 0.721 [after 95 batches]\n",
            "val loss: 0.979 accuracy: 0.333 [after 22 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.554 accuracy: 0.750 [after 23 batches]\n",
            "train loss: 0.567 accuracy: 0.716 [after 47 batches]\n",
            "train loss: 0.554 accuracy: 0.734 [after 71 batches]\n",
            "train loss: 0.559 accuracy: 0.732 [after 95 batches]\n",
            "val loss: 0.612 accuracy: 0.653 [after 22 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.451 accuracy: 0.811 [after 23 batches]\n",
            "train loss: 0.492 accuracy: 0.795 [after 47 batches]\n",
            "train loss: 0.513 accuracy: 0.780 [after 71 batches]\n",
            "train loss: 0.521 accuracy: 0.774 [after 95 batches]\n",
            "val loss: 1.173 accuracy: 0.333 [after 22 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.511 accuracy: 0.795 [after 23 batches]\n",
            "train loss: 0.529 accuracy: 0.786 [after 47 batches]\n",
            "train loss: 0.512 accuracy: 0.797 [after 71 batches]\n",
            "train loss: 0.515 accuracy: 0.794 [after 95 batches]\n",
            "val loss: 0.362 accuracy: 0.916 [after 22 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.474 accuracy: 0.799 [after 23 batches]\n",
            "train loss: 0.490 accuracy: 0.797 [after 47 batches]\n",
            "train loss: 0.507 accuracy: 0.785 [after 71 batches]\n",
            "train loss: 0.518 accuracy: 0.778 [after 95 batches]\n",
            "val loss: 0.576 accuracy: 0.799 [after 22 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.462 accuracy: 0.811 [after 23 batches]\n",
            "train loss: 0.478 accuracy: 0.799 [after 47 batches]\n",
            "train loss: 0.506 accuracy: 0.778 [after 71 batches]\n",
            "train loss: 0.503 accuracy: 0.775 [after 95 batches]\n",
            "val loss: 0.152 accuracy: 0.996 [after 22 batches]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "train loss: 0.536 accuracy: 0.773 [after 23 batches]\n",
            "train loss: 0.529 accuracy: 0.778 [after 47 batches]\n",
            "train loss: 0.530 accuracy: 0.780 [after 71 batches]\n",
            "train loss: 0.525 accuracy: 0.775 [after 95 batches]\n",
            "val loss: 0.572 accuracy: 0.688 [after 22 batches]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "train loss: 0.510 accuracy: 0.780 [after 23 batches]\n",
            "train loss: 0.507 accuracy: 0.786 [after 47 batches]\n",
            "train loss: 0.512 accuracy: 0.784 [after 71 batches]\n",
            "train loss: 0.500 accuracy: 0.787 [after 95 batches]\n",
            "val loss: 0.225 accuracy: 0.909 [after 22 batches]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "train loss: 0.495 accuracy: 0.788 [after 23 batches]\n",
            "train loss: 0.456 accuracy: 0.816 [after 47 batches]\n",
            "train loss: 0.454 accuracy: 0.814 [after 71 batches]\n",
            "train loss: 0.451 accuracy: 0.812 [after 95 batches]\n",
            "val loss: 0.311 accuracy: 0.868 [after 22 batches]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "train loss: 0.508 accuracy: 0.765 [after 23 batches]\n",
            "train loss: 0.523 accuracy: 0.756 [after 47 batches]\n",
            "train loss: 0.481 accuracy: 0.788 [after 71 batches]\n",
            "train loss: 0.472 accuracy: 0.787 [after 95 batches]\n",
            "val loss: 0.310 accuracy: 0.913 [after 22 batches]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "train loss: 0.442 accuracy: 0.807 [after 23 batches]\n",
            "train loss: 0.430 accuracy: 0.822 [after 47 batches]\n",
            "train loss: 0.421 accuracy: 0.826 [after 71 batches]\n",
            "train loss: 0.446 accuracy: 0.805 [after 95 batches]\n",
            "val loss: 0.695 accuracy: 0.489 [after 22 batches]\n",
            "test accuracy: 0.455 recall: 1.000 precision: 0.455 f1: 0.600 [after 20 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▁▁▂▄▄▂▁▂▂▃▄▄▆▅▆▆▆█▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇▆▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ██▇▅▅▄▄▅▆▅▅▅▄▄▃▃▃▃▃▁▂▂▃▂▂▂▁▂▃▃▂▂▂▂▁▂▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▅▅▅▄▄▁▄▁▇▆█▅▇▇▇▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▅▅▅▄▅▄▇▄█▂▄▁▄▁▂▂▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 67\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.45455\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.59981\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.45455\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.80492\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.44625\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.48944\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.69532\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mvocal-sweep-7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/pp7clts9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_190913-pp7clts9/logs\u001b[0m\n",
            "2024-06-09 19:29:26,699 - wandb.wandb_agent - INFO - Cleaning up finished run: pp7clts9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Terminating and syncing runs. Press ctrl-c to kill.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}