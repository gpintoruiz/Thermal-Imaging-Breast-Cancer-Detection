{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dftDiCQg3vYy"
      },
      "source": [
        "# <font color='#4C5FDA'>**Breast Cancer Detection Based on CNNs Using Thermal Imaging** </font>\n",
        "\n",
        "Original paper by Juan Pablo Zuluaga, Zeina Al Masry, Khaled Benaggoune, Safa Meraghni & Noureddine Zerhouni: [A CNN-based methodology for breast cancer diagnosis using thermal images](https://www.tandfonline.com/doi/full/10.1080/21681163.2020.1824685)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uraq36CkXB1T",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Instalar paquetes necesarios**\n",
        "\n",
        "%%capture\n",
        "! pip install torchmetrics\n",
        "! pip install wandb -Uq\n",
        "# ! pip install onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#ECA702'>**Clonamos nuestro repo**</font>\n",
        "\n",
        "Esto con el fin de traer todos los .py para poder entrenar 'localmente' en Colab y registrar las métricas en wandb."
      ],
      "metadata": {
        "id": "3eUcjX2Lv5g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/gpintoruiz/Thermal-Imaging-Breast-Cancer-Detection.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miS2W9Tdwbxq",
        "outputId": "0dc9acc0-7d5d-420d-dec4-f5a4c2802088"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Thermal-Imaging-Breast-Cancer-Detection'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 129 (delta 63), reused 68 (delta 24), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (129/129), 2.28 MiB | 5.84 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Thermal-Imaging-Breast-Cancer-Detection/notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMuOAySxx1Z1",
        "outputId": "f22db1eb-59b5-4b37-d21f-3a4a976b5f86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssFb9A5GyA_u",
        "outputId": "eddfb053-41f5-4b4f-9307-d9177fdf90f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01-gpr-data-exploration-cv2.ipynb  1.00-gpr-xception-from-scratch.ipynb  utils.py\n",
            "0.01-gpr-data-exploration-pil.ipynb  make_dataset.py                       validation.py\n",
            "0.02-gpr-experiments-base.ipynb      test.py                               xception-one-run.yaml\n",
            "0.03-gpr-initial-experiment.ipynb    train_one_run.py                      xception.py\n",
            "0.04-gpr-colab-experiments.ipynb     train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkrHeEps3vY3"
      },
      "source": [
        "## <font color='#ECA702'>**Configuración inicial para conectarnos con Kaggle**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJFg6k1x3vY5"
      },
      "source": [
        "1. Instalamos kaggle. Para poder usar comandos de Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hwqionQb3vY6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP3nl2Et3vY7"
      },
      "source": [
        "Subimos nuestro token de autenticación de Kaggle (si estamos en colab, sino colocarlo en la carpeta del proyecto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARP6wZsb3vY8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0VPkGso3vY9"
      },
      "source": [
        "1. Creamos los directorios de Kaggle\n",
        "2. Copiamos nuestro token en .kaggle\n",
        "3. Con `chmod 600` establecemos los permitos del token en 600, es decir, que solo yo tengo permisos de lectura y escritura sobre el archivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ed2nCVTu3vY-"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UhQ_SI9x3vZA"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGkGIazz3vZB"
      },
      "source": [
        "## <font color='#ECA702'>**Carga del dataset**</font>\n",
        "\n",
        "Traemos el dataset [Thermal Images for Breast Cancer Diagnosis DMR-IR](https://www.kaggle.com/datasets/asdeepak/thermal-images-for-breast-cancer-diagnosis-dmrir) desde kaggle.\n",
        "\n",
        "This dataset is a methodology for breast disease computer-aided diagnosis using dynamic thermography. The thermal images for breast tumors are classified according to DMR-IR standards.\n",
        "\n",
        "Two types of tumors are classified in this dataset one is benign another is malignant.\n",
        "- Benign: This type of tumor is usually well-defined and round or oval in shape. (non-cancerous tumor)\n",
        "- Malignant: This type of tumor is usually poorly defined and irregular with lobules. (cancerous tumor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "lmT0aOvG3vZD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! kaggle datasets download -d asdeepak/thermal-images-for-breast-cancer-diagnosis-dmrir\n",
        "! unzip thermal-images-for-breast-cancer-diagnosis-dmrir.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QToIa8zB3vZE"
      },
      "source": [
        "Después de descargar los datos. Debemos entender la estructura de las carpetas para poder trabajar con ellas de una mejor manera.\n",
        "1. La carpeta principal `Imagens e Matrizes da Tese de Thiago Alves Elias da Silva` son todos los datos `data`.\n",
        "2. La carpeta `12 Novos Casos de Testes` la podemos tomar como nuestro conjunto de prueba (`test`).\n",
        "3. Mientras que la carpeta `Desenvolvimento da Metodologia` será nuestro conjunto de entrenamiento (`train`).\n",
        "\n",
        "Luego dentro de nuestras carpetas de `train` y `test` encontramos dos categorías `DOENTES`y `SAUDAтХа├╝VEIS` o SAUDÁVEI. Los primeros son los casos malignos y los segundos benignos.\n",
        "\n",
        "Dentro de cada una de las carpetas de pacientes saludables y enfermos se encuentran carpetas con números, cada número representa un paciente. Y para cada paciente tendremos dos carpetas más, una para las imágenes **segmentadas** en escala de grises y la otra para la matrix o mapa de calor.\n",
        "\n",
        "Algo bueno de este dataset es que ya está dividido por pacientes, es decir, no tendremos imagenes del mismo paciente en el conjunto de entrenamiento y testeo. Por lo tanto, vamos a entrenar con N pacientes, y testear con K pacientes, que no son los mismos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#ECA702'>**Inicializamos el agende de wandb**</font>"
      ],
      "metadata": {
        "id": "5YTKbsdDyqSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='#52F17F'>**1. Nos logeamos en nuestra cuenta**</font>"
      ],
      "metadata": {
        "id": "nIskv2fHy4Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "St3agN3Rypzq",
        "outputId": "d8b669fa-0378-4eca-9896-beb4078246bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='#52F17F'>**2. Hacemos call del agente**</font>\n"
      ],
      "metadata": {
        "id": "w0r-E8Ufy9TQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El sweep que estoy probando acá es el [siguiente](https://github.com/gpintoruiz/Thermal-Imaging-Breast-Cancer-Detection/blob/main/notebooks/xception-one-run.yaml). Se pueden cambiar los parámetros a probar como tú quieras de acuerdo con la [documentación](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration) (recomiendo solo cambiar la arquitectura para que las comparaciones entre modelos sean equivalentes).\n",
        "\n",
        "Si no tienes ni idea qué es un sweep mira el siguiente [tutorial](https://www.youtube.com/watch?v=9zrmUIlScdY&t=1361s&ab_channel=Weights%26Biases).\n",
        "\n",
        "El comando `--count` sirve para decirle al agente cuántos runs hacer, aplica especialmente cuando el método del sweep es `bayes` o `random`\n"
      ],
      "metadata": {
        "id": "LxtyncVoCVL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wandb agent aiuis/dip-project/epbt9jh6 --count 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpG6dTZdy0R3",
        "outputId": "1e3ad9ba-c3f4-4913-8342-be42e8132b66"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
            "2024-06-09 22:49:05,841 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2024-06-09 22:49:06,898 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 22:49:06,899 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 46\n",
            "\tfold: 1\n",
            "\tlearning_rate: 0.002466508906685079\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: None\n",
            "2024-06-09 22:49:06,900 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=46 --fold=1 --learning_rate=0.002466508906685079 --normalize=True --optimizer=sgd --resize=None\n",
            "2024-06-09 22:49:11,912 - wandb.wandb_agent - INFO - Running runs: ['p2gwrs78']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_224916-p2gwrs78\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpleasant-sweep-30\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/p2gwrs78\u001b[0m\n",
            "FOLD 1\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 186, in forward\n",
            "    x = self.entry_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 91, in forward\n",
            "    x = self.block2(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 73, in forward\n",
            "    x2 =self.double_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\", line 103, in forward\n",
            "    return F.relu(input, inplace=self.inplace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1500, in relu\n",
            "    result = torch.relu(input)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 426.00 MiB. GPU \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpleasant-sweep-30\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/p2gwrs78\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_224916-p2gwrs78/logs\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 63, in <module>\n",
            "    model_pipeline(default_config)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 186, in forward\n",
            "    x = self.entry_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 91, in forward\n",
            "    x = self.block2(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 73, in forward\n",
            "    x2 =self.double_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\", line 103, in forward\n",
            "    return F.relu(input, inplace=self.inplace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1500, in relu\n",
            "    result = torch.relu(input)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 426.00 MiB. GPU \n",
            "2024-06-09 22:49:32,195 - wandb.wandb_agent - INFO - Cleaning up finished run: p2gwrs78\n",
            "2024-06-09 22:49:33,367 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 22:49:33,367 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 47\n",
            "\tfold: 2\n",
            "\tlearning_rate: 0.00327417591300011\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: 50\n",
            "2024-06-09 22:49:33,368 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=47 --fold=2 --learning_rate=0.00327417591300011 --normalize=True --optimizer=sgd --resize=50\n",
            "2024-06-09 22:49:38,374 - wandb.wandb_agent - INFO - Running runs: ['6mjtsa5j']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_224939-6mjtsa5j\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mresilient-sweep-31\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/6mjtsa5j\u001b[0m\n",
            "FOLD 2\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "train loss: 0.695 accuracy: 0.455 [after 4 batches]\n",
            "train loss: 0.689 accuracy: 0.517 [after 9 batches]\n",
            "train loss: 0.679 accuracy: 0.563 [after 14 batches]\n",
            "train loss: 0.667 accuracy: 0.590 [after 19 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "val loss: 0.654 accuracy: 0.657 [after 6 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.606 accuracy: 0.706 [after 4 batches]\n",
            "train loss: 0.595 accuracy: 0.728 [after 9 batches]\n",
            "train loss: 0.588 accuracy: 0.725 [after 14 batches]\n",
            "train loss: 0.586 accuracy: 0.719 [after 19 batches]\n",
            "val loss: 0.642 accuracy: 0.687 [after 6 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.650 accuracy: 0.702 [after 4 batches]\n",
            "train loss: 0.619 accuracy: 0.694 [after 9 batches]\n",
            "train loss: 0.605 accuracy: 0.689 [after 14 batches]\n",
            "train loss: 0.581 accuracy: 0.709 [after 19 batches]\n",
            "val loss: 0.493 accuracy: 0.767 [after 6 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.488 accuracy: 0.774 [after 4 batches]\n",
            "train loss: 0.480 accuracy: 0.774 [after 9 batches]\n",
            "train loss: 0.477 accuracy: 0.772 [after 14 batches]\n",
            "train loss: 0.490 accuracy: 0.766 [after 19 batches]\n",
            "val loss: 0.419 accuracy: 0.811 [after 6 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.532 accuracy: 0.766 [after 4 batches]\n",
            "train loss: 0.523 accuracy: 0.757 [after 9 batches]\n",
            "train loss: 0.509 accuracy: 0.765 [after 14 batches]\n",
            "train loss: 0.508 accuracy: 0.770 [after 19 batches]\n",
            "val loss: 0.405 accuracy: 0.862 [after 6 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.445 accuracy: 0.774 [after 4 batches]\n",
            "train loss: 0.479 accuracy: 0.777 [after 9 batches]\n",
            "train loss: 0.466 accuracy: 0.780 [after 14 batches]\n",
            "train loss: 0.449 accuracy: 0.786 [after 19 batches]\n",
            "val loss: 0.380 accuracy: 0.908 [after 6 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.458 accuracy: 0.800 [after 4 batches]\n",
            "train loss: 0.456 accuracy: 0.798 [after 9 batches]\n",
            "train loss: 0.461 accuracy: 0.790 [after 14 batches]\n",
            "train loss: 0.451 accuracy: 0.795 [after 19 batches]\n",
            "val loss: 0.356 accuracy: 0.897 [after 6 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.452 accuracy: 0.804 [after 4 batches]\n",
            "train loss: 0.416 accuracy: 0.817 [after 9 batches]\n",
            "train loss: 0.411 accuracy: 0.814 [after 14 batches]\n",
            "train loss: 0.412 accuracy: 0.817 [after 19 batches]\n",
            "val loss: 0.513 accuracy: 0.802 [after 6 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.407 accuracy: 0.855 [after 4 batches]\n",
            "train loss: 0.374 accuracy: 0.855 [after 9 batches]\n",
            "train loss: 0.389 accuracy: 0.838 [after 14 batches]\n",
            "train loss: 0.403 accuracy: 0.834 [after 19 batches]\n",
            "val loss: 0.339 accuracy: 0.850 [after 6 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.412 accuracy: 0.813 [after 4 batches]\n",
            "train loss: 0.404 accuracy: 0.823 [after 9 batches]\n",
            "train loss: 0.405 accuracy: 0.826 [after 14 batches]\n",
            "train loss: 0.424 accuracy: 0.817 [after 19 batches]\n",
            "val loss: 0.262 accuracy: 0.918 [after 6 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.371 accuracy: 0.843 [after 4 batches]\n",
            "train loss: 0.382 accuracy: 0.834 [after 9 batches]\n",
            "train loss: 0.405 accuracy: 0.817 [after 14 batches]\n",
            "train loss: 0.398 accuracy: 0.820 [after 19 batches]\n",
            "val loss: 0.373 accuracy: 0.826 [after 6 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.325 accuracy: 0.834 [after 4 batches]\n",
            "train loss: 0.306 accuracy: 0.864 [after 9 batches]\n",
            "train loss: 0.311 accuracy: 0.861 [after 14 batches]\n",
            "train loss: 0.331 accuracy: 0.857 [after 19 batches]\n",
            "val loss: 0.350 accuracy: 0.780 [after 6 batches]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "train loss: 0.354 accuracy: 0.847 [after 4 batches]\n",
            "train loss: 0.363 accuracy: 0.838 [after 9 batches]\n",
            "train loss: 0.381 accuracy: 0.837 [after 14 batches]\n",
            "train loss: 0.368 accuracy: 0.837 [after 19 batches]\n",
            "val loss: 0.450 accuracy: 0.800 [after 6 batches]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "train loss: 0.363 accuracy: 0.838 [after 4 batches]\n",
            "train loss: 0.358 accuracy: 0.838 [after 9 batches]\n",
            "train loss: 0.349 accuracy: 0.838 [after 14 batches]\n",
            "train loss: 0.341 accuracy: 0.846 [after 19 batches]\n",
            "val loss: 0.503 accuracy: 0.809 [after 6 batches]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "train loss: 0.386 accuracy: 0.834 [after 4 batches]\n",
            "train loss: 0.392 accuracy: 0.828 [after 9 batches]\n",
            "train loss: 0.383 accuracy: 0.833 [after 14 batches]\n",
            "train loss: 0.386 accuracy: 0.829 [after 19 batches]\n",
            "val loss: 0.591 accuracy: 0.683 [after 6 batches]\n",
            "test accuracy: 0.835 recall: 1.000 precision: 0.792 f1: 0.883 [after 5 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▃▃▃▄▅▅▅▆▇▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▅▆▆▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██▇▇▇█▇▇████████▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ██▇▆▆▆▇▆▄▄▄▅▅▅▄▄▄▄▄▄▃▃▂▃▃▃▃▂▃▃▁▁▂▂▂▂▂▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▄▅▆█▇▅▆█▆▄▅▅▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ██▅▄▄▃▃▅▂▁▃▃▄▅▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 59\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.83457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.88281\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.79204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.82872\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.3864\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.68298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.59137\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mresilient-sweep-31\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/6mjtsa5j\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_224939-6mjtsa5j/logs\u001b[0m\n",
            "2024-06-09 23:05:36,188 - wandb.wandb_agent - INFO - Cleaning up finished run: 6mjtsa5j\n",
            "2024-06-09 23:05:37,684 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 23:05:37,684 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 58\n",
            "\tfold: 7\n",
            "\tlearning_rate: 0.0028887009906956975\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: 300\n",
            "2024-06-09 23:05:37,686 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=58 --fold=7 --learning_rate=0.0028887009906956975 --normalize=True --optimizer=sgd --resize=300\n",
            "2024-06-09 23:05:42,696 - wandb.wandb_agent - INFO - Running runs: ['ajmoo68f']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_230544-ajmoo68f\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvivid-sweep-35\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/ajmoo68f\u001b[0m\n",
            "FOLD 7\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 187, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 124, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 113, in forward\n",
            "    x = self.triple_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\", line 175, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2509, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 78.00 MiB. GPU \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mvivid-sweep-35\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/ajmoo68f\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_230544-ajmoo68f/logs\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 63, in <module>\n",
            "    model_pipeline(default_config)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 187, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 124, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 113, in forward\n",
            "    x = self.triple_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\", line 175, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2509, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 78.00 MiB. GPU \n",
            "2024-06-09 23:06:08,029 - wandb.wandb_agent - INFO - Cleaning up finished run: ajmoo68f\n",
            "2024-06-09 23:06:09,725 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 23:06:09,725 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 27\n",
            "\tfold: 2\n",
            "\tlearning_rate: 0.007021217736718999\n",
            "\tnormalize: False\n",
            "\toptimizer: adam\n",
            "\tresize: None\n",
            "2024-06-09 23:06:09,727 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=27 --fold=2 --learning_rate=0.007021217736718999 --normalize=False --optimizer=adam --resize=None\n",
            "2024-06-09 23:06:14,733 - wandb.wandb_agent - INFO - Running runs: ['mjq2tfsd']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_230615-mjq2tfsd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mconfused-sweep-36\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/mjq2tfsd\u001b[0m\n",
            "FOLD 2\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 187, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 124, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 113, in forward\n",
            "    x = self.triple_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 37, in forward\n",
            "    x = self.depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 460, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mconfused-sweep-36\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/mjq2tfsd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_230615-mjq2tfsd/logs\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 63, in <module>\n",
            "    model_pipeline(default_config)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 187, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 124, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 113, in forward\n",
            "    x = self.triple_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 37, in forward\n",
            "    x = self.depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 460, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n",
            "    return F.conv2d(input, weight, bias, self.stride,\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU \n",
            "2024-06-09 23:06:34,989 - wandb.wandb_agent - INFO - Cleaning up finished run: mjq2tfsd\n",
            "2024-06-09 23:06:36,230 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 23:06:36,230 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 40\n",
            "\tfold: 3\n",
            "\tlearning_rate: 0.005998761057776602\n",
            "\tnormalize: True\n",
            "\toptimizer: adam\n",
            "\tresize: None\n",
            "2024-06-09 23:06:36,232 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=40 --fold=3 --learning_rate=0.005998761057776602 --normalize=True --optimizer=adam --resize=None\n",
            "2024-06-09 23:06:41,242 - wandb.wandb_agent - INFO - Running runs: ['1owfgt61']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_230642-1owfgt61\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlilac-sweep-37\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/1owfgt61\u001b[0m\n",
            "FOLD 3\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 186, in forward\n",
            "    x = self.entry_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 91, in forward\n",
            "    x = self.block2(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 73, in forward\n",
            "    x2 =self.double_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\", line 103, in forward\n",
            "    return F.relu(input, inplace=self.inplace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1500, in relu\n",
            "    result = torch.relu(input)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 740.00 MiB. GPU \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlilac-sweep-37\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/1owfgt61\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_230642-1owfgt61/logs\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 63, in <module>\n",
            "    model_pipeline(default_config)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 186, in forward\n",
            "    x = self.entry_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 91, in forward\n",
            "    x = self.block2(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 73, in forward\n",
            "    x2 =self.double_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\", line 103, in forward\n",
            "    return F.relu(input, inplace=self.inplace)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1500, in relu\n",
            "    result = torch.relu(input)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 740.00 MiB. GPU \n",
            "2024-06-09 23:07:01,500 - wandb.wandb_agent - INFO - Cleaning up finished run: 1owfgt61\n",
            "2024-06-09 23:07:02,845 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 23:07:02,846 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 31\n",
            "\tfold: 6\n",
            "\tlearning_rate: 0.005040247513723446\n",
            "\tnormalize: True\n",
            "\toptimizer: adam\n",
            "\tresize: 50\n",
            "2024-06-09 23:07:02,847 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=31 --fold=6 --learning_rate=0.005040247513723446 --normalize=True --optimizer=adam --resize=50\n",
            "2024-06-09 23:07:07,857 - wandb.wandb_agent - INFO - Running runs: ['92frf7pc']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_230709-92frf7pc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvivid-sweep-38\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/92frf7pc\u001b[0m\n",
            "FOLD 6\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "train loss: 1.285 accuracy: 0.540 [after 7 batches]\n",
            "train loss: 1.024 accuracy: 0.532 [after 15 batches]\n",
            "train loss: 0.928 accuracy: 0.520 [after 23 batches]\n",
            "train loss: 0.878 accuracy: 0.503 [after 31 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "val loss: 0.705 accuracy: 0.522 [after 8 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.734 accuracy: 0.508 [after 7 batches]\n",
            "train loss: 0.769 accuracy: 0.496 [after 15 batches]\n",
            "train loss: 0.768 accuracy: 0.496 [after 23 batches]\n",
            "train loss: 0.763 accuracy: 0.483 [after 31 batches]\n",
            "val loss: 34.896 accuracy: 0.483 [after 8 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.899 accuracy: 0.508 [after 7 batches]\n",
            "train loss: 0.845 accuracy: 0.498 [after 15 batches]\n",
            "train loss: 0.822 accuracy: 0.509 [after 23 batches]\n",
            "train loss: 0.827 accuracy: 0.506 [after 31 batches]\n",
            "val loss: 0.693 accuracy: 0.522 [after 8 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.715 accuracy: 0.520 [after 7 batches]\n",
            "train loss: 0.691 accuracy: 0.518 [after 15 batches]\n",
            "train loss: 0.705 accuracy: 0.500 [after 23 batches]\n",
            "train loss: 0.717 accuracy: 0.499 [after 31 batches]\n",
            "val loss: 0.683 accuracy: 0.597 [after 8 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.700 accuracy: 0.512 [after 7 batches]\n",
            "train loss: 0.695 accuracy: 0.514 [after 15 batches]\n",
            "train loss: 0.688 accuracy: 0.516 [after 23 batches]\n",
            "train loss: 0.683 accuracy: 0.533 [after 31 batches]\n",
            "val loss: 0.738 accuracy: 0.489 [after 8 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.656 accuracy: 0.540 [after 7 batches]\n",
            "train loss: 0.660 accuracy: 0.577 [after 15 batches]\n",
            "train loss: 0.682 accuracy: 0.543 [after 23 batches]\n",
            "train loss: 0.685 accuracy: 0.537 [after 31 batches]\n",
            "val loss: 0.726 accuracy: 0.597 [after 8 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.695 accuracy: 0.520 [after 7 batches]\n",
            "train loss: 0.686 accuracy: 0.504 [after 15 batches]\n",
            "train loss: 0.681 accuracy: 0.504 [after 23 batches]\n",
            "train loss: 0.682 accuracy: 0.501 [after 31 batches]\n",
            "val loss: 0.694 accuracy: 0.559 [after 8 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.695 accuracy: 0.544 [after 7 batches]\n",
            "train loss: 0.682 accuracy: 0.524 [after 15 batches]\n",
            "train loss: 0.674 accuracy: 0.532 [after 23 batches]\n",
            "train loss: 0.669 accuracy: 0.534 [after 31 batches]\n",
            "val loss: 0.693 accuracy: 0.559 [after 8 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.683 accuracy: 0.516 [after 7 batches]\n",
            "train loss: 0.665 accuracy: 0.556 [after 15 batches]\n",
            "train loss: 0.664 accuracy: 0.542 [after 23 batches]\n",
            "train loss: 0.661 accuracy: 0.539 [after 31 batches]\n",
            "val loss: 0.692 accuracy: 0.522 [after 8 batches]\n",
            "test accuracy: 0.559 recall: 0.000 precision: 0.000 f1: 0.000 [after 8 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▄▅▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▅▅▄▃▃▂▂▁▃▂▃▃▄▄▂▂▃▃▃▅▅█▅▅▄▃▃▂▆▄▅▅▃▆▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▅▄▃▂▂▂▂▄▃▃▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▃▁▃█▁█▆▆▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁█▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.55914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.53931\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.66064\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.52151\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.69222\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mvivid-sweep-38\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/92frf7pc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_230709-92frf7pc/logs\u001b[0m\n",
            "2024-06-09 23:17:05,966 - wandb.wandb_agent - INFO - Cleaning up finished run: 92frf7pc\n",
            "2024-06-09 23:17:07,462 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 23:17:07,462 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 31\n",
            "\tfold: 1\n",
            "\tlearning_rate: 0.00021096826954953267\n",
            "\tnormalize: False\n",
            "\toptimizer: sgd\n",
            "\tresize: 150\n",
            "2024-06-09 23:17:07,464 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=31 --fold=1 --learning_rate=0.00021096826954953267 --normalize=False --optimizer=sgd --resize=150\n",
            "2024-06-09 23:17:12,475 - wandb.wandb_agent - INFO - Running runs: ['88r5eltn']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_231714-88r5eltn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33molive-sweep-30\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/88r5eltn\u001b[0m\n",
            "FOLD 1\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.694 accuracy: 0.488 [after 7 batches]\n",
            "train loss: 0.689 accuracy: 0.504 [after 15 batches]\n",
            "train loss: 0.691 accuracy: 0.493 [after 23 batches]\n",
            "train loss: 0.687 accuracy: 0.511 [after 31 batches]\n",
            "val loss: 0.693 accuracy: 0.544 [after 8 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.671 accuracy: 0.544 [after 7 batches]\n",
            "train loss: 0.668 accuracy: 0.538 [after 15 batches]\n",
            "train loss: 0.674 accuracy: 0.528 [after 23 batches]\n",
            "train loss: 0.673 accuracy: 0.537 [after 31 batches]\n",
            "val loss: 0.644 accuracy: 0.761 [after 8 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.657 accuracy: 0.601 [after 7 batches]\n",
            "train loss: 0.680 accuracy: 0.544 [after 15 batches]\n",
            "train loss: 0.683 accuracy: 0.543 [after 23 batches]\n",
            "train loss: 0.679 accuracy: 0.552 [after 31 batches]\n",
            "val loss: 0.620 accuracy: 0.747 [after 8 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.666 accuracy: 0.565 [after 7 batches]\n",
            "train loss: 0.657 accuracy: 0.583 [after 15 batches]\n",
            "train loss: 0.657 accuracy: 0.565 [after 23 batches]\n",
            "train loss: 0.655 accuracy: 0.570 [after 31 batches]\n",
            "val loss: 0.589 accuracy: 0.837 [after 8 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.645 accuracy: 0.629 [after 7 batches]\n",
            "train loss: 0.651 accuracy: 0.625 [after 15 batches]\n",
            "train loss: 0.649 accuracy: 0.617 [after 23 batches]\n",
            "train loss: 0.641 accuracy: 0.617 [after 31 batches]\n",
            "val loss: 0.603 accuracy: 0.591 [after 8 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.635 accuracy: 0.581 [after 7 batches]\n",
            "train loss: 0.632 accuracy: 0.583 [after 15 batches]\n",
            "train loss: 0.624 accuracy: 0.595 [after 23 batches]\n",
            "train loss: 0.623 accuracy: 0.610 [after 31 batches]\n",
            "val loss: 0.535 accuracy: 0.740 [after 8 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.567 accuracy: 0.706 [after 7 batches]\n",
            "train loss: 0.579 accuracy: 0.665 [after 15 batches]\n",
            "train loss: 0.596 accuracy: 0.636 [after 23 batches]\n",
            "train loss: 0.595 accuracy: 0.635 [after 31 batches]\n",
            "val loss: 0.496 accuracy: 0.843 [after 8 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.585 accuracy: 0.681 [after 7 batches]\n",
            "train loss: 0.589 accuracy: 0.657 [after 15 batches]\n",
            "train loss: 0.585 accuracy: 0.642 [after 23 batches]\n",
            "train loss: 0.582 accuracy: 0.642 [after 31 batches]\n",
            "val loss: 0.457 accuracy: 0.861 [after 8 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.623 accuracy: 0.621 [after 7 batches]\n",
            "train loss: 0.584 accuracy: 0.665 [after 15 batches]\n",
            "train loss: 0.577 accuracy: 0.667 [after 23 batches]\n",
            "train loss: 0.574 accuracy: 0.671 [after 31 batches]\n",
            "val loss: 0.470 accuracy: 0.817 [after 8 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.557 accuracy: 0.690 [after 7 batches]\n",
            "train loss: 0.552 accuracy: 0.675 [after 15 batches]\n",
            "train loss: 0.559 accuracy: 0.660 [after 23 batches]\n",
            "train loss: 0.552 accuracy: 0.659 [after 31 batches]\n",
            "val loss: 0.521 accuracy: 0.783 [after 8 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.543 accuracy: 0.698 [after 7 batches]\n",
            "train loss: 0.541 accuracy: 0.712 [after 15 batches]\n",
            "train loss: 0.532 accuracy: 0.714 [after 23 batches]\n",
            "train loss: 0.542 accuracy: 0.693 [after 31 batches]\n",
            "val loss: 0.444 accuracy: 0.809 [after 8 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.549 accuracy: 0.665 [after 7 batches]\n",
            "train loss: 0.554 accuracy: 0.661 [after 15 batches]\n",
            "train loss: 0.566 accuracy: 0.663 [after 23 batches]\n",
            "train loss: 0.559 accuracy: 0.663 [after 31 batches]\n",
            "val loss: 0.395 accuracy: 0.849 [after 8 batches]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "train loss: 0.517 accuracy: 0.669 [after 7 batches]\n",
            "train loss: 0.521 accuracy: 0.700 [after 15 batches]\n",
            "train loss: 0.541 accuracy: 0.694 [after 23 batches]\n",
            "train loss: 0.546 accuracy: 0.676 [after 31 batches]\n",
            "val loss: 0.360 accuracy: 0.848 [after 8 batches]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "train loss: 0.571 accuracy: 0.645 [after 7 batches]\n",
            "train loss: 0.528 accuracy: 0.685 [after 15 batches]\n",
            "train loss: 0.527 accuracy: 0.688 [after 23 batches]\n",
            "train loss: 0.515 accuracy: 0.695 [after 31 batches]\n",
            "val loss: 0.345 accuracy: 0.902 [after 8 batches]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "train loss: 0.547 accuracy: 0.669 [after 7 batches]\n",
            "train loss: 0.534 accuracy: 0.694 [after 15 batches]\n",
            "train loss: 0.533 accuracy: 0.684 [after 23 batches]\n",
            "train loss: 0.525 accuracy: 0.696 [after 31 batches]\n",
            "val loss: 0.386 accuracy: 0.870 [after 8 batches]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "train loss: 0.519 accuracy: 0.738 [after 7 batches]\n",
            "train loss: 0.509 accuracy: 0.722 [after 15 batches]\n",
            "train loss: 0.509 accuracy: 0.712 [after 23 batches]\n",
            "train loss: 0.494 accuracy: 0.724 [after 31 batches]\n",
            "val loss: 0.367 accuracy: 0.865 [after 8 batches]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "train loss: 0.501 accuracy: 0.698 [after 7 batches]\n",
            "train loss: 0.496 accuracy: 0.708 [after 15 batches]\n",
            "train loss: 0.508 accuracy: 0.695 [after 23 batches]\n",
            "train loss: 0.501 accuracy: 0.706 [after 31 batches]\n",
            "val loss: 0.329 accuracy: 0.899 [after 8 batches]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "train loss: 0.469 accuracy: 0.714 [after 7 batches]\n",
            "train loss: 0.483 accuracy: 0.720 [after 15 batches]\n",
            "train loss: 0.495 accuracy: 0.696 [after 23 batches]\n",
            "train loss: 0.499 accuracy: 0.698 [after 31 batches]\n",
            "val loss: 0.460 accuracy: 0.798 [after 8 batches]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "train loss: 0.480 accuracy: 0.694 [after 7 batches]\n",
            "train loss: 0.471 accuracy: 0.742 [after 15 batches]\n",
            "train loss: 0.475 accuracy: 0.750 [after 23 batches]\n",
            "train loss: 0.490 accuracy: 0.744 [after 31 batches]\n",
            "val loss: 0.354 accuracy: 0.887 [after 8 batches]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "train loss: 0.472 accuracy: 0.734 [after 7 batches]\n",
            "train loss: 0.466 accuracy: 0.738 [after 15 batches]\n",
            "train loss: 0.496 accuracy: 0.728 [after 23 batches]\n",
            "train loss: 0.484 accuracy: 0.737 [after 31 batches]\n",
            "val loss: 0.325 accuracy: 0.900 [after 8 batches]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "train loss: 0.441 accuracy: 0.790 [after 7 batches]\n",
            "train loss: 0.446 accuracy: 0.764 [after 15 batches]\n",
            "train loss: 0.449 accuracy: 0.761 [after 23 batches]\n",
            "train loss: 0.451 accuracy: 0.763 [after 31 batches]\n",
            "val loss: 0.321 accuracy: 0.899 [after 8 batches]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "train loss: 0.455 accuracy: 0.710 [after 7 batches]\n",
            "train loss: 0.448 accuracy: 0.740 [after 15 batches]\n",
            "train loss: 0.448 accuracy: 0.749 [after 23 batches]\n",
            "train loss: 0.450 accuracy: 0.747 [after 31 batches]\n",
            "val loss: 0.458 accuracy: 0.785 [after 8 batches]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "train loss: 0.494 accuracy: 0.770 [after 7 batches]\n",
            "train loss: 0.500 accuracy: 0.748 [after 15 batches]\n",
            "train loss: 0.484 accuracy: 0.751 [after 23 batches]\n",
            "train loss: 0.468 accuracy: 0.761 [after 31 batches]\n",
            "val loss: 0.489 accuracy: 0.754 [after 8 batches]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "train loss: 0.422 accuracy: 0.806 [after 7 batches]\n",
            "train loss: 0.401 accuracy: 0.817 [after 15 batches]\n",
            "train loss: 0.393 accuracy: 0.819 [after 23 batches]\n",
            "train loss: 0.412 accuracy: 0.803 [after 31 batches]\n",
            "val loss: 0.381 accuracy: 0.836 [after 8 batches]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "train loss: 0.485 accuracy: 0.722 [after 7 batches]\n",
            "train loss: 0.481 accuracy: 0.746 [after 15 batches]\n",
            "train loss: 0.455 accuracy: 0.769 [after 23 batches]\n",
            "train loss: 0.440 accuracy: 0.779 [after 31 batches]\n",
            "val loss: 0.359 accuracy: 0.861 [after 8 batches]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "train loss: 0.360 accuracy: 0.851 [after 7 batches]\n",
            "train loss: 0.397 accuracy: 0.804 [after 15 batches]\n",
            "train loss: 0.391 accuracy: 0.810 [after 23 batches]\n",
            "train loss: 0.397 accuracy: 0.816 [after 31 batches]\n",
            "val loss: 0.324 accuracy: 0.896 [after 8 batches]\n",
            "test accuracy: 0.725 recall: 0.629 precision: 0.865 f1: 0.717 [after 7 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▄▅▄▄▅▄▅▅▆▆▅▅▅▆▆▆▆▅▆▆▇▇▆█▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ██▇██▇▇▇▇▇▆▆▆▆▅▅▅▅▅▅▅▄▅▄▄▄▃▄▃▃▄▃▃▃▄▂▂▄▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▅▅▇▂▅▇▇▆▆▆▇▇█▇▇█▆███▆▅▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▇▆▆▅▄▄▄▅▃▂▂▁▂▂▁▄▂▁▁▄▄▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 103\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.72465\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.71726\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.86471\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.62865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.81552\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.39723\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.89639\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.32385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33molive-sweep-30\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/88r5eltn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_231714-88r5eltn/logs\u001b[0m\n",
            "2024-06-09 23:47:06,590 - wandb.wandb_agent - INFO - Cleaning up finished run: 88r5eltn\n",
            "2024-06-09 23:47:08,282 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 23:47:08,282 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 16\n",
            "\tfold: 5\n",
            "\tlearning_rate: 0.008974205207923958\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: 150\n",
            "2024-06-09 23:47:08,284 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=16 --fold=5 --learning_rate=0.008974205207923958 --normalize=True --optimizer=sgd --resize=150\n",
            "2024-06-09 23:47:13,294 - wandb.wandb_agent - INFO - Running runs: ['x2qs3g2c']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_234715-x2qs3g2c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhardy-sweep-33\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/x2qs3g2c\u001b[0m\n",
            "FOLD 5\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.528 accuracy: 0.699 [after 15 batches]\n",
            "train loss: 0.432 accuracy: 0.775 [after 31 batches]\n",
            "train loss: 0.410 accuracy: 0.801 [after 47 batches]\n",
            "train loss: 0.354 accuracy: 0.829 [after 63 batches]\n",
            "val loss: 0.054 accuracy: 0.996 [after 15 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.272 accuracy: 0.891 [after 15 batches]\n",
            "train loss: 0.345 accuracy: 0.869 [after 31 batches]\n",
            "train loss: 0.279 accuracy: 0.897 [after 47 batches]\n",
            "train loss: 0.244 accuracy: 0.910 [after 63 batches]\n",
            "val loss: 0.266 accuracy: 0.858 [after 15 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.205 accuracy: 0.918 [after 15 batches]\n",
            "train loss: 0.183 accuracy: 0.924 [after 31 batches]\n",
            "train loss: 0.164 accuracy: 0.932 [after 47 batches]\n",
            "train loss: 0.154 accuracy: 0.937 [after 63 batches]\n",
            "val loss: 0.027 accuracy: 1.000 [after 15 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.207 accuracy: 0.930 [after 15 batches]\n",
            "train loss: 0.282 accuracy: 0.902 [after 31 batches]\n",
            "train loss: 0.219 accuracy: 0.924 [after 47 batches]\n",
            "train loss: 0.182 accuracy: 0.938 [after 63 batches]\n",
            "val loss: 0.004 accuracy: 1.000 [after 15 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.058 accuracy: 0.977 [after 15 batches]\n",
            "train loss: 0.038 accuracy: 0.984 [after 31 batches]\n",
            "train loss: 0.043 accuracy: 0.983 [after 47 batches]\n",
            "train loss: 0.039 accuracy: 0.984 [after 63 batches]\n",
            "val loss: 0.002 accuracy: 1.000 [after 15 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.044 accuracy: 0.980 [after 15 batches]\n",
            "train loss: 0.028 accuracy: 0.988 [after 31 batches]\n",
            "train loss: 0.023 accuracy: 0.992 [after 47 batches]\n",
            "train loss: 0.061 accuracy: 0.980 [after 63 batches]\n",
            "val loss: 0.275 accuracy: 0.842 [after 15 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.039 accuracy: 0.992 [after 15 batches]\n",
            "train loss: 0.057 accuracy: 0.984 [after 31 batches]\n",
            "train loss: 0.057 accuracy: 0.982 [after 47 batches]\n",
            "train loss: 0.062 accuracy: 0.981 [after 63 batches]\n",
            "val loss: 0.157 accuracy: 0.954 [after 15 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.036 accuracy: 0.984 [after 15 batches]\n",
            "train loss: 0.026 accuracy: 0.990 [after 31 batches]\n",
            "train loss: 0.035 accuracy: 0.990 [after 47 batches]\n",
            "train loss: 0.046 accuracy: 0.985 [after 63 batches]\n",
            "val loss: 0.130 accuracy: 0.933 [after 15 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.027 accuracy: 0.992 [after 15 batches]\n",
            "train loss: 0.031 accuracy: 0.986 [after 31 batches]\n",
            "train loss: 0.024 accuracy: 0.991 [after 47 batches]\n",
            "train loss: 0.020 accuracy: 0.993 [after 63 batches]\n",
            "val loss: 0.068 accuracy: 0.979 [after 15 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.014 accuracy: 0.996 [after 15 batches]\n",
            "train loss: 0.011 accuracy: 0.998 [after 31 batches]\n",
            "train loss: 0.010 accuracy: 0.999 [after 47 batches]\n",
            "train loss: 0.008 accuracy: 0.999 [after 63 batches]\n",
            "val loss: 0.403 accuracy: 0.867 [after 15 batches]\n",
            "test accuracy: 0.930 recall: 0.840 precision: 1.000 f1: 0.908 [after 14 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▃▄▅▅▆▆▆▆▆▇▆▆▆▇▇███████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▆▅▆▅▄▄▃▃▃▄▅▄▃▂▁▁▁▁▁▁▂▁▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy █▂███▁▆▅▇▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▂▆▁▁▁▆▄▃▂█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 39\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.93006\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.90825\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.83968\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.99902\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.00806\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.86667\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.40339\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhardy-sweep-33\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/x2qs3g2c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_234715-x2qs3g2c/logs\u001b[0m\n",
            "2024-06-09 23:57:01,475 - wandb.wandb_agent - INFO - Cleaning up finished run: x2qs3g2c\n",
            "2024-06-09 23:57:03,342 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-09 23:57:03,342 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 10\n",
            "\tfold: 5\n",
            "\tlearning_rate: 0.005104923202698088\n",
            "\tnormalize: False\n",
            "\toptimizer: sgd\n",
            "\tresize: 150\n",
            "2024-06-09 23:57:03,344 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=10 --fold=5 --learning_rate=0.005104923202698088 --normalize=False --optimizer=sgd --resize=150\n",
            "2024-06-09 23:57:08,354 - wandb.wandb_agent - INFO - Running runs: ['letaosf0']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240609_235710-letaosf0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33myoung-sweep-35\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/letaosf0\u001b[0m\n",
            "FOLD 5\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.711 accuracy: 0.558 [after 25 batches]\n",
            "train loss: 0.753 accuracy: 0.533 [after 51 batches]\n",
            "train loss: 0.724 accuracy: 0.573 [after 77 batches]\n",
            "train loss: 0.720 accuracy: 0.587 [after 103 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "val loss: 0.483 accuracy: 0.817 [after 24 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.660 accuracy: 0.619 [after 25 batches]\n",
            "train loss: 0.727 accuracy: 0.577 [after 51 batches]\n",
            "train loss: 0.751 accuracy: 0.559 [after 77 batches]\n",
            "train loss: 0.731 accuracy: 0.567 [after 103 batches]\n",
            "val loss: 0.497 accuracy: 0.788 [after 24 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.650 accuracy: 0.604 [after 25 batches]\n",
            "train loss: 0.671 accuracy: 0.567 [after 51 batches]\n",
            "train loss: 0.688 accuracy: 0.545 [after 77 batches]\n",
            "train loss: 0.687 accuracy: 0.555 [after 103 batches]\n",
            "val loss: 0.544 accuracy: 0.500 [after 24 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.672 accuracy: 0.635 [after 25 batches]\n",
            "train loss: 0.674 accuracy: 0.629 [after 51 batches]\n",
            "train loss: 0.668 accuracy: 0.612 [after 77 batches]\n",
            "train loss: 0.653 accuracy: 0.619 [after 103 batches]\n",
            "val loss: 3.790 accuracy: 0.483 [after 24 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.618 accuracy: 0.627 [after 25 batches]\n",
            "train loss: 0.614 accuracy: 0.640 [after 51 batches]\n",
            "train loss: 0.641 accuracy: 0.623 [after 77 batches]\n",
            "train loss: 0.636 accuracy: 0.631 [after 103 batches]\n",
            "val loss: 0.262 accuracy: 0.833 [after 24 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.665 accuracy: 0.596 [after 25 batches]\n",
            "train loss: 0.577 accuracy: 0.660 [after 51 batches]\n",
            "train loss: 0.595 accuracy: 0.645 [after 77 batches]\n",
            "train loss: 0.609 accuracy: 0.647 [after 103 batches]\n",
            "val loss: 0.160 accuracy: 0.946 [after 24 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.747 accuracy: 0.658 [after 25 batches]\n",
            "train loss: 0.669 accuracy: 0.650 [after 51 batches]\n",
            "train loss: 0.600 accuracy: 0.673 [after 77 batches]\n",
            "train loss: 0.607 accuracy: 0.670 [after 103 batches]\n",
            "val loss: 0.338 accuracy: 0.796 [after 24 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.564 accuracy: 0.723 [after 25 batches]\n",
            "train loss: 0.545 accuracy: 0.725 [after 51 batches]\n",
            "train loss: 0.546 accuracy: 0.695 [after 77 batches]\n",
            "train loss: 0.539 accuracy: 0.699 [after 103 batches]\n",
            "val loss: 0.159 accuracy: 0.913 [after 24 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.587 accuracy: 0.635 [after 25 batches]\n",
            "train loss: 0.562 accuracy: 0.667 [after 51 batches]\n",
            "train loss: 0.556 accuracy: 0.672 [after 77 batches]\n",
            "train loss: 0.550 accuracy: 0.684 [after 103 batches]\n",
            "val loss: 0.113 accuracy: 1.000 [after 24 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.630 accuracy: 0.685 [after 25 batches]\n",
            "train loss: 0.572 accuracy: 0.669 [after 51 batches]\n",
            "train loss: 0.540 accuracy: 0.685 [after 77 batches]\n",
            "train loss: 0.530 accuracy: 0.694 [after 103 batches]\n",
            "val loss: 0.123 accuracy: 0.987 [after 24 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.675 accuracy: 0.604 [after 25 batches]\n",
            "train loss: 0.611 accuracy: 0.644 [after 51 batches]\n",
            "train loss: 0.646 accuracy: 0.618 [after 77 batches]\n",
            "train loss: 0.618 accuracy: 0.641 [after 103 batches]\n",
            "val loss: 0.791 accuracy: 0.800 [after 24 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.582 accuracy: 0.738 [after 25 batches]\n",
            "train loss: 0.570 accuracy: 0.717 [after 51 batches]\n",
            "train loss: 0.561 accuracy: 0.708 [after 77 batches]\n",
            "train loss: 0.546 accuracy: 0.705 [after 103 batches]\n",
            "val loss: 0.066 accuracy: 0.983 [after 24 batches]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "train loss: 0.524 accuracy: 0.715 [after 25 batches]\n",
            "train loss: 0.514 accuracy: 0.688 [after 51 batches]\n",
            "train loss: 0.492 accuracy: 0.688 [after 77 batches]\n",
            "train loss: 0.488 accuracy: 0.695 [after 103 batches]\n",
            "val loss: 0.019 accuracy: 1.000 [after 24 batches]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "train loss: 0.496 accuracy: 0.696 [after 25 batches]\n",
            "train loss: 0.502 accuracy: 0.688 [after 51 batches]\n",
            "train loss: 0.535 accuracy: 0.679 [after 77 batches]\n",
            "train loss: 0.528 accuracy: 0.691 [after 103 batches]\n",
            "val loss: 0.418 accuracy: 0.817 [after 24 batches]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "train loss: 0.448 accuracy: 0.712 [after 25 batches]\n",
            "train loss: 0.489 accuracy: 0.715 [after 51 batches]\n",
            "train loss: 0.518 accuracy: 0.697 [after 77 batches]\n",
            "train loss: 0.517 accuracy: 0.703 [after 103 batches]\n",
            "val loss: 0.207 accuracy: 0.871 [after 24 batches]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "train loss: 0.435 accuracy: 0.765 [after 25 batches]\n",
            "train loss: 0.507 accuracy: 0.727 [after 51 batches]\n",
            "train loss: 0.527 accuracy: 0.712 [after 77 batches]\n",
            "train loss: 0.549 accuracy: 0.709 [after 103 batches]\n",
            "val loss: 8.152 accuracy: 0.492 [after 24 batches]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "train loss: 0.446 accuracy: 0.735 [after 25 batches]\n",
            "train loss: 0.481 accuracy: 0.692 [after 51 batches]\n",
            "train loss: 0.509 accuracy: 0.672 [after 77 batches]\n",
            "train loss: 0.510 accuracy: 0.675 [after 103 batches]\n",
            "val loss: 1.116 accuracy: 0.671 [after 24 batches]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "train loss: 0.593 accuracy: 0.615 [after 25 batches]\n",
            "train loss: 0.584 accuracy: 0.650 [after 51 batches]\n",
            "train loss: 0.559 accuracy: 0.678 [after 77 batches]\n",
            "train loss: 0.546 accuracy: 0.679 [after 103 batches]\n",
            "val loss: 1.511 accuracy: 0.579 [after 24 batches]\n",
            "test accuracy: 0.500 recall: 1.000 precision: 0.473 f1: 0.627 [after 22 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▂▁▃▂▂▂▁▄▃▄▄▃▅▄▅▅▇▆▄▅▆▆▃▄▄▇▆▆▆▆▅▆▆█▇▆▆▅▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▇█▇▇█▆▇▆▆▅▆▆▄▅▆▅▃▃▄▄▅▃▆▅▅▄▃▃▂▂▃▁▃▁▃▄▂▃▄▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▆▅▁▁▆▇▅▇██▅██▆▆▁▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁▁▁▄▁▁▁▁▁▁▂▁▁▁▁█▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 71\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.62721\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.47348\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.67885\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.54618\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.57917\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.51069\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33myoung-sweep-35\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/letaosf0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240609_235710-letaosf0/logs\u001b[0m\n",
            "2024-06-10 00:19:21,707 - wandb.wandb_agent - INFO - Cleaning up finished run: letaosf0\n",
            "2024-06-10 00:19:23,657 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 00:19:23,657 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 47\n",
            "\tfold: 6\n",
            "\tlearning_rate: 0.006896938063377756\n",
            "\tnormalize: True\n",
            "\toptimizer: adam\n",
            "\tresize: 50\n",
            "2024-06-10 00:19:23,660 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=47 --fold=6 --learning_rate=0.006896938063377756 --normalize=True --optimizer=adam --resize=50\n",
            "2024-06-10 00:19:28,670 - wandb.wandb_agent - INFO - Running runs: ['87sjqtyy']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_001930-87sjqtyy\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmisty-sweep-40\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/87sjqtyy\u001b[0m\n",
            "FOLD 6\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "train loss: 1.860 accuracy: 0.514 [after 5 batches]\n",
            "train loss: 1.532 accuracy: 0.548 [after 11 batches]\n",
            "train loss: 1.270 accuracy: 0.574 [after 17 batches]\n",
            "train loss: 1.085 accuracy: 0.594 [after 23 batches]\n",
            "val loss: 0.750 accuracy: 0.543 [after 5 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.602 accuracy: 0.567 [after 5 batches]\n",
            "train loss: 0.568 accuracy: 0.624 [after 11 batches]\n",
            "train loss: 0.538 accuracy: 0.676 [after 17 batches]\n",
            "train loss: 0.790 accuracy: 0.660 [after 23 batches]\n",
            "val loss: 18.916 accuracy: 0.460 [after 5 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.674 accuracy: 0.663 [after 5 batches]\n",
            "train loss: 0.625 accuracy: 0.691 [after 11 batches]\n",
            "train loss: 0.600 accuracy: 0.695 [after 17 batches]\n",
            "train loss: 0.549 accuracy: 0.730 [after 23 batches]\n",
            "val loss: 19.067 accuracy: 0.455 [after 5 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.420 accuracy: 0.823 [after 5 batches]\n",
            "train loss: 0.373 accuracy: 0.855 [after 11 batches]\n",
            "train loss: 0.298 accuracy: 0.883 [after 17 batches]\n",
            "train loss: 0.271 accuracy: 0.894 [after 23 batches]\n",
            "val loss: 0.354 accuracy: 0.862 [after 5 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.119 accuracy: 0.965 [after 5 batches]\n",
            "train loss: 0.094 accuracy: 0.970 [after 11 batches]\n",
            "train loss: 0.102 accuracy: 0.967 [after 17 batches]\n",
            "train loss: 0.119 accuracy: 0.960 [after 23 batches]\n",
            "val loss: 1.037 accuracy: 0.466 [after 5 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.079 accuracy: 0.965 [after 5 batches]\n",
            "train loss: 0.071 accuracy: 0.972 [after 11 batches]\n",
            "train loss: 0.062 accuracy: 0.975 [after 17 batches]\n",
            "train loss: 0.052 accuracy: 0.980 [after 23 batches]\n",
            "val loss: 2.021 accuracy: 0.553 [after 5 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.015 accuracy: 0.996 [after 5 batches]\n",
            "train loss: 0.035 accuracy: 0.989 [after 11 batches]\n",
            "train loss: 0.036 accuracy: 0.989 [after 17 batches]\n",
            "train loss: 0.191 accuracy: 0.949 [after 23 batches]\n",
            "val loss: 0.099 accuracy: 0.966 [after 5 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.158 accuracy: 0.986 [after 5 batches]\n",
            "train loss: 0.095 accuracy: 0.989 [after 11 batches]\n",
            "train loss: 0.073 accuracy: 0.989 [after 17 batches]\n",
            "train loss: 0.077 accuracy: 0.989 [after 23 batches]\n",
            "val loss: 1.618 accuracy: 0.678 [after 5 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.162 accuracy: 0.950 [after 5 batches]\n",
            "train loss: 0.148 accuracy: 0.957 [after 11 batches]\n",
            "train loss: 0.111 accuracy: 0.968 [after 17 batches]\n",
            "train loss: 0.095 accuracy: 0.974 [after 23 batches]\n",
            "val loss: 6.593 accuracy: 0.453 [after 5 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.070 accuracy: 0.965 [after 5 batches]\n",
            "train loss: 0.089 accuracy: 0.966 [after 11 batches]\n",
            "train loss: 0.074 accuracy: 0.972 [after 17 batches]\n",
            "train loss: 0.062 accuracy: 0.978 [after 23 batches]\n",
            "val loss: 1.235 accuracy: 0.718 [after 5 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.063 accuracy: 0.975 [after 5 batches]\n",
            "train loss: 0.039 accuracy: 0.986 [after 11 batches]\n",
            "train loss: 0.028 accuracy: 0.991 [after 17 batches]\n",
            "train loss: 0.251 accuracy: 0.948 [after 23 batches]\n",
            "val loss: 0.812 accuracy: 0.797 [after 5 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.004 accuracy: 1.000 [after 5 batches]\n",
            "train loss: 0.031 accuracy: 0.991 [after 11 batches]\n",
            "train loss: 0.036 accuracy: 0.987 [after 17 batches]\n",
            "train loss: 0.877 accuracy: 0.946 [after 23 batches]\n",
            "val loss: 9.637 accuracy: 0.598 [after 5 batches]\n",
            "test accuracy: 0.820 recall: 1.000 precision: 0.711 f1: 0.830 [after 5 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▂▃▄▄▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▂▂▂▃▃▃▄▄▅▆▆▆▇█▇▇█████▇███▇▇█▇█████▇██▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▅▃▃▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▂▂▁▁▂▂▁▁▁▁▁▁▁▂▁▁▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▂▁▁▇▁▂█▄▁▅▆▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁██▁▁▂▁▂▃▁▁▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.81981\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.82993\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.71081\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.94592\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.87707\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.59787\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 9.63703\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mmisty-sweep-40\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/87sjqtyy\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_001930-87sjqtyy/logs\u001b[0m\n",
            "2024-06-10 00:29:57,191 - wandb.wandb_agent - INFO - Cleaning up finished run: 87sjqtyy\n",
            "2024-06-10 00:29:59,217 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 00:29:59,217 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 14\n",
            "\tfold: 3\n",
            "\tlearning_rate: 0.000268801652673967\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: 300\n",
            "2024-06-10 00:29:59,219 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=14 --fold=3 --learning_rate=0.000268801652673967 --normalize=True --optimizer=sgd --resize=300\n",
            "2024-06-10 00:30:04,229 - wandb.wandb_agent - INFO - Running runs: ['vunubwgv']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_003005-vunubwgv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mconfused-sweep-42\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/vunubwgv\u001b[0m\n",
            "FOLD 3\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.669 accuracy: 0.609 [after 18 batches]\n",
            "train loss: 0.653 accuracy: 0.652 [after 37 batches]\n",
            "train loss: 0.636 accuracy: 0.660 [after 56 batches]\n",
            "train loss: 0.619 accuracy: 0.675 [after 75 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "val loss: 0.584 accuracy: 0.683 [after 18 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.526 accuracy: 0.741 [after 18 batches]\n",
            "train loss: 0.485 accuracy: 0.780 [after 37 batches]\n",
            "train loss: 0.456 accuracy: 0.806 [after 56 batches]\n",
            "train loss: 0.432 accuracy: 0.820 [after 75 batches]\n",
            "val loss: 0.340 accuracy: 0.897 [after 18 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.307 accuracy: 0.902 [after 18 batches]\n",
            "train loss: 0.304 accuracy: 0.898 [after 37 batches]\n",
            "train loss: 0.274 accuracy: 0.915 [after 56 batches]\n",
            "train loss: 0.258 accuracy: 0.923 [after 75 batches]\n",
            "val loss: 0.268 accuracy: 0.913 [after 18 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.165 accuracy: 0.966 [after 18 batches]\n",
            "train loss: 0.177 accuracy: 0.953 [after 37 batches]\n",
            "train loss: 0.170 accuracy: 0.945 [after 56 batches]\n",
            "train loss: 0.168 accuracy: 0.945 [after 75 batches]\n",
            "val loss: 0.261 accuracy: 0.861 [after 18 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.120 accuracy: 0.970 [after 18 batches]\n",
            "train loss: 0.118 accuracy: 0.970 [after 37 batches]\n",
            "train loss: 0.102 accuracy: 0.976 [after 56 batches]\n",
            "train loss: 0.098 accuracy: 0.979 [after 75 batches]\n",
            "val loss: 0.256 accuracy: 0.865 [after 18 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.087 accuracy: 0.981 [after 18 batches]\n",
            "train loss: 0.079 accuracy: 0.987 [after 37 batches]\n",
            "train loss: 0.078 accuracy: 0.986 [after 56 batches]\n",
            "train loss: 0.090 accuracy: 0.981 [after 75 batches]\n",
            "val loss: 0.213 accuracy: 0.917 [after 18 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.066 accuracy: 0.981 [after 18 batches]\n",
            "train loss: 0.057 accuracy: 0.987 [after 37 batches]\n",
            "train loss: 0.054 accuracy: 0.989 [after 56 batches]\n",
            "train loss: 0.061 accuracy: 0.988 [after 75 batches]\n",
            "val loss: 0.247 accuracy: 0.913 [after 18 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.115 accuracy: 0.977 [after 18 batches]\n",
            "train loss: 0.099 accuracy: 0.979 [after 37 batches]\n",
            "train loss: 0.083 accuracy: 0.982 [after 56 batches]\n",
            "train loss: 0.073 accuracy: 0.984 [after 75 batches]\n",
            "val loss: 0.327 accuracy: 0.917 [after 18 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.052 accuracy: 0.985 [after 18 batches]\n",
            "train loss: 0.048 accuracy: 0.989 [after 37 batches]\n",
            "train loss: 0.052 accuracy: 0.987 [after 56 batches]\n",
            "train loss: 0.051 accuracy: 0.989 [after 75 batches]\n",
            "val loss: 0.344 accuracy: 0.889 [after 18 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.044 accuracy: 0.985 [after 18 batches]\n",
            "train loss: 0.050 accuracy: 0.983 [after 37 batches]\n",
            "train loss: 0.080 accuracy: 0.974 [after 56 batches]\n",
            "train loss: 0.078 accuracy: 0.974 [after 75 batches]\n",
            "val loss: 0.293 accuracy: 0.917 [after 18 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.041 accuracy: 0.992 [after 18 batches]\n",
            "train loss: 0.053 accuracy: 0.987 [after 37 batches]\n",
            "train loss: 0.043 accuracy: 0.991 [after 56 batches]\n",
            "train loss: 0.040 accuracy: 0.993 [after 75 batches]\n",
            "val loss: 0.266 accuracy: 0.917 [after 18 batches]\n",
            "test accuracy: 0.885 recall: 0.760 precision: 0.972 f1: 0.843 [after 16 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▂▃▄▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▂▂▃▄▅▅▆▆▇█▇▇▇█████████████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ███▇▆▆▆▅▄▄▃▂▃▂▂▂▂▂▂▂▁▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▇█▆▆███▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▃▂▂▂▁▂▃▃▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 43\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.88482\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.8429\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.97188\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.75995\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.99342\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.04025\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.91667\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.26586\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mconfused-sweep-42\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/vunubwgv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_003005-vunubwgv/logs\u001b[0m\n",
            "2024-06-10 00:43:04,685 - wandb.wandb_agent - INFO - Cleaning up finished run: vunubwgv\n",
            "2024-06-10 00:43:06,520 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 00:43:06,521 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 11\n",
            "\tfold: 4\n",
            "\tlearning_rate: 0.009876441770099484\n",
            "\tnormalize: False\n",
            "\toptimizer: sgd\n",
            "\tresize: 300\n",
            "2024-06-10 00:43:06,523 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=11 --fold=4 --learning_rate=0.009876441770099484 --normalize=False --optimizer=sgd --resize=300\n",
            "2024-06-10 00:43:11,536 - wandb.wandb_agent - INFO - Running runs: ['ma0sb14a']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_004313-ma0sb14a\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mserene-sweep-47\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/ma0sb14a\u001b[0m\n",
            "FOLD 4\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.828 accuracy: 0.496 [after 23 batches]\n",
            "train loss: 0.768 accuracy: 0.523 [after 47 batches]\n",
            "train loss: 0.755 accuracy: 0.519 [after 71 batches]\n",
            "train loss: 0.715 accuracy: 0.548 [after 95 batches]\n",
            "val loss: 0.753 accuracy: 0.616 [after 22 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.611 accuracy: 0.655 [after 23 batches]\n",
            "train loss: 0.579 accuracy: 0.678 [after 47 batches]\n",
            "train loss: 0.576 accuracy: 0.670 [after 71 batches]\n",
            "train loss: 0.590 accuracy: 0.667 [after 95 batches]\n",
            "val loss: 0.250 accuracy: 0.891 [after 22 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.514 accuracy: 0.701 [after 23 batches]\n",
            "train loss: 0.540 accuracy: 0.676 [after 47 batches]\n",
            "train loss: 0.585 accuracy: 0.649 [after 71 batches]\n",
            "train loss: 0.573 accuracy: 0.662 [after 95 batches]\n",
            "val loss: 0.355 accuracy: 0.842 [after 22 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.520 accuracy: 0.720 [after 23 batches]\n",
            "train loss: 0.551 accuracy: 0.689 [after 47 batches]\n",
            "train loss: 0.573 accuracy: 0.683 [after 71 batches]\n",
            "train loss: 0.552 accuracy: 0.686 [after 95 batches]\n",
            "val loss: 0.509 accuracy: 0.750 [after 22 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.628 accuracy: 0.652 [after 23 batches]\n",
            "train loss: 0.563 accuracy: 0.672 [after 47 batches]\n",
            "train loss: 0.550 accuracy: 0.669 [after 71 batches]\n",
            "train loss: 0.547 accuracy: 0.674 [after 95 batches]\n",
            "val loss: 0.295 accuracy: 0.883 [after 22 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.504 accuracy: 0.727 [after 23 batches]\n",
            "train loss: 0.556 accuracy: 0.689 [after 47 batches]\n",
            "train loss: 0.566 accuracy: 0.692 [after 71 batches]\n",
            "train loss: 0.560 accuracy: 0.683 [after 95 batches]\n",
            "val loss: 0.072 accuracy: 0.992 [after 22 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.563 accuracy: 0.682 [after 23 batches]\n",
            "train loss: 0.544 accuracy: 0.703 [after 47 batches]\n",
            "train loss: 0.548 accuracy: 0.699 [after 71 batches]\n",
            "train loss: 0.542 accuracy: 0.705 [after 95 batches]\n",
            "val loss: 0.299 accuracy: 0.777 [after 22 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.542 accuracy: 0.697 [after 23 batches]\n",
            "train loss: 0.509 accuracy: 0.682 [after 47 batches]\n",
            "train loss: 0.538 accuracy: 0.691 [after 71 batches]\n",
            "train loss: 0.545 accuracy: 0.697 [after 95 batches]\n",
            "val loss: 3.212 accuracy: 0.580 [after 22 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.500 accuracy: 0.667 [after 23 batches]\n",
            "train loss: 0.477 accuracy: 0.670 [after 47 batches]\n",
            "train loss: 0.477 accuracy: 0.684 [after 71 batches]\n",
            "train loss: 0.507 accuracy: 0.671 [after 95 batches]\n",
            "val loss: 0.152 accuracy: 0.979 [after 22 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.546 accuracy: 0.712 [after 23 batches]\n",
            "train loss: 0.504 accuracy: 0.706 [after 47 batches]\n",
            "train loss: 0.505 accuracy: 0.726 [after 71 batches]\n",
            "train loss: 0.491 accuracy: 0.719 [after 95 batches]\n",
            "val loss: 0.420 accuracy: 0.803 [after 22 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.512 accuracy: 0.720 [after 23 batches]\n",
            "train loss: 0.521 accuracy: 0.699 [after 47 batches]\n",
            "train loss: 0.510 accuracy: 0.706 [after 71 batches]\n",
            "train loss: 0.507 accuracy: 0.711 [after 95 batches]\n",
            "val loss: 0.311 accuracy: 0.891 [after 22 batches]\n",
            "test accuracy: 0.859 recall: 0.820 precision: 0.885 f1: 0.832 [after 20 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▂▃▄▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▂▃▆▇▆▆▇▆▆█▇▇▇▆▆▆▆█▇▇▇▇▇▇▇▇▇▇▆▇▆█▇███▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▇▆▄▃▃▃▂▂▃▂▂▃▃▄▃▂▂▂▃▃▃▂▂▂▂▂▂▂▁▁▂▂▂▂▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▂▆▅▄▆█▄▁█▅▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▃▁▂▂▁▁▂█▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 43\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.85909\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.83198\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.885\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.81958\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.71117\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.50685\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.89073\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.31078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mserene-sweep-47\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/ma0sb14a\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_004313-ma0sb14a/logs\u001b[0m\n",
            "2024-06-10 00:58:44,523 - wandb.wandb_agent - INFO - Cleaning up finished run: ma0sb14a\n",
            "2024-06-10 00:58:46,856 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 00:58:46,857 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 36\n",
            "\tfold: 4\n",
            "\tlearning_rate: 0.007698331246292499\n",
            "\tnormalize: True\n",
            "\toptimizer: adam\n",
            "\tresize: 300\n",
            "2024-06-10 00:58:46,858 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=36 --fold=4 --learning_rate=0.007698331246292499 --normalize=True --optimizer=adam --resize=300\n",
            "2024-06-10 00:58:51,868 - wandb.wandb_agent - INFO - Running runs: ['al5im36s']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_005852-al5im36s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mjumping-sweep-48\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/al5im36s\u001b[0m\n",
            "FOLD 4\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.840 accuracy: 0.563 [after 6 batches]\n",
            "train loss: 0.705 accuracy: 0.605 [after 13 batches]\n",
            "train loss: 0.625 accuracy: 0.667 [after 20 batches]\n",
            "train loss: 0.574 accuracy: 0.710 [after 27 batches]\n",
            "val loss: 1.473 accuracy: 0.857 [after 7 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.340 accuracy: 0.857 [after 6 batches]\n",
            "train loss: 0.312 accuracy: 0.865 [after 13 batches]\n",
            "train loss: 0.283 accuracy: 0.886 [after 20 batches]\n",
            "train loss: 0.268 accuracy: 0.894 [after 27 batches]\n",
            "val loss: 0.405 accuracy: 0.857 [after 7 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.332 accuracy: 0.873 [after 6 batches]\n",
            "train loss: 0.292 accuracy: 0.899 [after 13 batches]\n",
            "train loss: 0.259 accuracy: 0.918 [after 20 batches]\n",
            "train loss: 0.266 accuracy: 0.913 [after 27 batches]\n",
            "val loss: 791.174 accuracy: 0.333 [after 7 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.261 accuracy: 0.901 [after 6 batches]\n",
            "train loss: 0.264 accuracy: 0.907 [after 13 batches]\n",
            "train loss: 0.254 accuracy: 0.906 [after 20 batches]\n",
            "train loss: 0.247 accuracy: 0.905 [after 27 batches]\n",
            "val loss: 26.206 accuracy: 0.516 [after 7 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.266 accuracy: 0.913 [after 6 batches]\n",
            "train loss: 0.265 accuracy: 0.907 [after 13 batches]\n",
            "train loss: 0.260 accuracy: 0.914 [after 20 batches]\n",
            "train loss: 0.286 accuracy: 0.900 [after 27 batches]\n",
            "val loss: 47.114 accuracy: 0.333 [after 7 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.287 accuracy: 0.901 [after 6 batches]\n",
            "train loss: 0.283 accuracy: 0.899 [after 13 batches]\n",
            "train loss: 0.250 accuracy: 0.915 [after 20 batches]\n",
            "train loss: 0.229 accuracy: 0.922 [after 27 batches]\n",
            "val loss: 0.681 accuracy: 0.897 [after 7 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.185 accuracy: 0.937 [after 6 batches]\n",
            "train loss: 0.153 accuracy: 0.946 [after 13 batches]\n",
            "train loss: 0.180 accuracy: 0.933 [after 20 batches]\n",
            "train loss: 0.156 accuracy: 0.939 [after 27 batches]\n",
            "val loss: 4.724 accuracy: 0.673 [after 7 batches]\n",
            "test accuracy: 0.631 recall: 0.202 precision: 0.857 f1: 0.318 [after 7 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▄▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▅▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ██▁▃▁█▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁▁█▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.63095\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.31799\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.85714\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.20183\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.93948\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.15562\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.67262\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 4.72435\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mjumping-sweep-48\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/al5im36s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_005852-al5im36s/logs\u001b[0m\n",
            "2024-06-10 01:07:08,438 - wandb.wandb_agent - INFO - Cleaning up finished run: al5im36s\n",
            "2024-06-10 01:07:10,583 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 01:07:10,583 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 43\n",
            "\tfold: 2\n",
            "\tlearning_rate: 0.0005969678476458226\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: 300\n",
            "2024-06-10 01:07:10,585 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=43 --fold=2 --learning_rate=0.0005969678476458226 --normalize=True --optimizer=sgd --resize=300\n",
            "2024-06-10 01:07:15,595 - wandb.wandb_agent - INFO - Running runs: ['ox8lp7rt']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_010717-ox8lp7rt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwandering-sweep-50\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/ox8lp7rt\u001b[0m\n",
            "FOLD 2\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.700 accuracy: 0.469 [after 5 batches]\n",
            "train loss: 0.696 accuracy: 0.488 [after 11 batches]\n",
            "train loss: 0.695 accuracy: 0.501 [after 17 batches]\n",
            "train loss: 0.693 accuracy: 0.524 [after 23 batches]\n",
            "val loss: 0.668 accuracy: 0.668 [after 6 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.670 accuracy: 0.612 [after 5 batches]\n",
            "train loss: 0.653 accuracy: 0.659 [after 11 batches]\n",
            "train loss: 0.635 accuracy: 0.682 [after 17 batches]\n",
            "train loss: 0.617 accuracy: 0.709 [after 23 batches]\n",
            "val loss: 0.657 accuracy: 0.670 [after 6 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.495 accuracy: 0.845 [after 5 batches]\n",
            "train loss: 0.478 accuracy: 0.849 [after 11 batches]\n",
            "train loss: 0.453 accuracy: 0.862 [after 17 batches]\n",
            "train loss: 0.427 accuracy: 0.873 [after 23 batches]\n",
            "val loss: 0.339 accuracy: 0.853 [after 6 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.356 accuracy: 0.880 [after 5 batches]\n",
            "train loss: 0.326 accuracy: 0.899 [after 11 batches]\n",
            "train loss: 0.302 accuracy: 0.912 [after 17 batches]\n",
            "train loss: 0.307 accuracy: 0.908 [after 23 batches]\n",
            "val loss: 0.155 accuracy: 0.992 [after 6 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.233 accuracy: 0.934 [after 5 batches]\n",
            "train loss: 0.251 accuracy: 0.928 [after 11 batches]\n",
            "train loss: 0.248 accuracy: 0.926 [after 17 batches]\n",
            "train loss: 0.238 accuracy: 0.930 [after 23 batches]\n",
            "val loss: 0.317 accuracy: 0.858 [after 6 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.223 accuracy: 0.934 [after 5 batches]\n",
            "train loss: 0.201 accuracy: 0.946 [after 11 batches]\n",
            "train loss: 0.188 accuracy: 0.951 [after 17 batches]\n",
            "train loss: 0.199 accuracy: 0.944 [after 23 batches]\n",
            "val loss: 0.298 accuracy: 0.842 [after 6 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.155 accuracy: 0.957 [after 5 batches]\n",
            "train loss: 0.151 accuracy: 0.965 [after 11 batches]\n",
            "train loss: 0.136 accuracy: 0.969 [after 17 batches]\n",
            "train loss: 0.138 accuracy: 0.964 [after 23 batches]\n",
            "val loss: 0.139 accuracy: 0.957 [after 6 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.096 accuracy: 0.992 [after 5 batches]\n",
            "train loss: 0.099 accuracy: 0.988 [after 11 batches]\n",
            "train loss: 0.111 accuracy: 0.975 [after 17 batches]\n",
            "train loss: 0.101 accuracy: 0.980 [after 23 batches]\n",
            "val loss: 0.594 accuracy: 0.736 [after 6 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.059 accuracy: 0.996 [after 5 batches]\n",
            "train loss: 0.055 accuracy: 0.996 [after 11 batches]\n",
            "train loss: 0.053 accuracy: 0.995 [after 17 batches]\n",
            "train loss: 0.061 accuracy: 0.989 [after 23 batches]\n",
            "val loss: 0.257 accuracy: 0.870 [after 6 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.027 accuracy: 1.000 [after 5 batches]\n",
            "train loss: 0.050 accuracy: 0.988 [after 11 batches]\n",
            "train loss: 0.061 accuracy: 0.986 [after 17 batches]\n",
            "train loss: 0.058 accuracy: 0.986 [after 23 batches]\n",
            "val loss: 0.261 accuracy: 0.876 [after 6 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.041 accuracy: 0.992 [after 5 batches]\n",
            "train loss: 0.034 accuracy: 0.996 [after 11 batches]\n",
            "train loss: 0.035 accuracy: 0.996 [after 17 batches]\n",
            "train loss: 0.040 accuracy: 0.993 [after 23 batches]\n",
            "val loss: 0.266 accuracy: 0.881 [after 6 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.049 accuracy: 0.996 [after 5 batches]\n",
            "train loss: 0.038 accuracy: 0.998 [after 11 batches]\n",
            "train loss: 0.036 accuracy: 0.999 [after 17 batches]\n",
            "train loss: 0.035 accuracy: 0.997 [after 23 batches]\n",
            "val loss: 0.394 accuracy: 0.811 [after 6 batches]\n",
            "test accuracy: 0.712 recall: 0.590 precision: 0.960 f1: 0.728 [after 6 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▂▃▄▄▅▅▆▇▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▁▂▃▄▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████████████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █████▇▇▆▆▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▅█▅▅▇▂▅▅▆▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ██▄▁▃▃▁▇▃▃▃▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 47\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.7124\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.72829\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.95979\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.58959\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.99709\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.03485\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.81116\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.39401\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwandering-sweep-50\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/ox8lp7rt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_010717-ox8lp7rt/logs\u001b[0m\n",
            "2024-06-10 01:20:56,681 - wandb.wandb_agent - INFO - Cleaning up finished run: ox8lp7rt\n",
            "2024-06-10 01:20:59,050 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 01:20:59,050 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 33\n",
            "\tfold: 5\n",
            "\tlearning_rate: 0.0012803673198531363\n",
            "\tnormalize: False\n",
            "\toptimizer: adam\n",
            "\tresize: 150\n",
            "2024-06-10 01:20:59,053 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=33 --fold=5 --learning_rate=0.0012803673198531363 --normalize=False --optimizer=adam --resize=150\n",
            "2024-06-10 01:21:04,065 - wandb.wandb_agent - INFO - Running runs: ['tdnzftsx']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_012106-tdnzftsx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mspring-sweep-51\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/tdnzftsx\u001b[0m\n",
            "FOLD 5\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.991 accuracy: 0.545 [after 7 batches]\n",
            "train loss: 0.730 accuracy: 0.627 [after 15 batches]\n",
            "train loss: 0.628 accuracy: 0.693 [after 23 batches]\n",
            "train loss: 0.560 accuracy: 0.730 [after 31 batches]\n",
            "val loss: 0.801 accuracy: 0.677 [after 8 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.344 accuracy: 0.833 [after 7 batches]\n",
            "train loss: 0.241 accuracy: 0.896 [after 15 batches]\n",
            "train loss: 0.229 accuracy: 0.899 [after 23 batches]\n",
            "train loss: 0.195 accuracy: 0.918 [after 31 batches]\n",
            "val loss: 1.082 accuracy: 0.735 [after 8 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.146 accuracy: 0.951 [after 7 batches]\n",
            "train loss: 0.120 accuracy: 0.960 [after 15 batches]\n",
            "train loss: 0.110 accuracy: 0.962 [after 23 batches]\n",
            "train loss: 0.114 accuracy: 0.959 [after 31 batches]\n",
            "val loss: 1.334 accuracy: 0.760 [after 8 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.072 accuracy: 0.962 [after 7 batches]\n",
            "train loss: 0.064 accuracy: 0.966 [after 15 batches]\n",
            "train loss: 0.064 accuracy: 0.971 [after 23 batches]\n",
            "train loss: 0.056 accuracy: 0.976 [after 31 batches]\n",
            "val loss: 1.167 accuracy: 0.688 [after 8 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.025 accuracy: 0.992 [after 7 batches]\n",
            "train loss: 0.031 accuracy: 0.991 [after 15 batches]\n",
            "train loss: 0.035 accuracy: 0.991 [after 23 batches]\n",
            "train loss: 0.050 accuracy: 0.989 [after 31 batches]\n",
            "val loss: 6.103 accuracy: 0.500 [after 8 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.161 accuracy: 0.943 [after 7 batches]\n",
            "train loss: 0.164 accuracy: 0.932 [after 15 batches]\n",
            "train loss: 0.130 accuracy: 0.946 [after 23 batches]\n",
            "train loss: 0.108 accuracy: 0.956 [after 31 batches]\n",
            "val loss: 3.540 accuracy: 0.587 [after 8 batches]\n",
            "test accuracy: 0.604 recall: 1.000 precision: 0.531 f1: 0.689 [after 7 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▃▄▆▆▇▇▇▇█▇████████▇▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▆▅▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▆▇█▆▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁▁▂▁█▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.6039\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.6888\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.53122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.95644\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.10753\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.58712\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 3.54045\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mspring-sweep-51\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/tdnzftsx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_012106-tdnzftsx/logs\u001b[0m\n",
            "2024-06-10 01:26:54,116 - wandb.wandb_agent - INFO - Cleaning up finished run: tdnzftsx\n",
            "2024-06-10 01:26:56,818 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 01:26:56,818 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 63\n",
            "\tfold: 5\n",
            "\tlearning_rate: 0.004832669951877852\n",
            "\tnormalize: True\n",
            "\toptimizer: adam\n",
            "\tresize: 300\n",
            "2024-06-10 01:26:56,820 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=63 --fold=5 --learning_rate=0.004832669951877852 --normalize=True --optimizer=adam --resize=300\n",
            "2024-06-10 01:27:01,828 - wandb.wandb_agent - INFO - Running runs: ['ol256y76']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_012702-ol256y76\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msparkling-sweep-53\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/ol256y76\u001b[0m\n",
            "FOLD 5\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 187, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 124, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 113, in forward\n",
            "    x = self.triple_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\", line 175, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2509, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 84.00 MiB. GPU \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msparkling-sweep-53\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/ol256y76\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_012702-ol256y76/logs\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 63, in <module>\n",
            "    model_pipeline(default_config)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 187, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 124, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 113, in forward\n",
            "    x = self.triple_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\", line 175, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2509, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 84.00 MiB. GPU \n",
            "2024-06-10 01:27:22,100 - wandb.wandb_agent - INFO - Cleaning up finished run: ol256y76\n",
            "2024-06-10 01:27:25,425 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 01:27:25,425 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 10\n",
            "\tfold: 2\n",
            "\tlearning_rate: 0.0023655723563896234\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: 150\n",
            "2024-06-10 01:27:25,427 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=10 --fold=2 --learning_rate=0.0023655723563896234 --normalize=True --optimizer=sgd --resize=150\n",
            "2024-06-10 01:27:30,438 - wandb.wandb_agent - INFO - Running runs: ['uofig8vk']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_012732-uofig8vk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexalted-sweep-54\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/uofig8vk\u001b[0m\n",
            "FOLD 2\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.690 accuracy: 0.577 [after 25 batches]\n",
            "train loss: 0.683 accuracy: 0.598 [after 51 batches]\n",
            "train loss: 0.684 accuracy: 0.606 [after 77 batches]\n",
            "train loss: 0.688 accuracy: 0.613 [after 103 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "val loss: 0.556 accuracy: 0.783 [after 24 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.678 accuracy: 0.596 [after 25 batches]\n",
            "train loss: 0.714 accuracy: 0.600 [after 51 batches]\n",
            "train loss: 0.693 accuracy: 0.617 [after 77 batches]\n",
            "train loss: 0.660 accuracy: 0.644 [after 103 batches]\n",
            "val loss: 0.624 accuracy: 0.733 [after 24 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.648 accuracy: 0.673 [after 25 batches]\n",
            "train loss: 0.608 accuracy: 0.710 [after 51 batches]\n",
            "train loss: 0.667 accuracy: 0.668 [after 77 batches]\n",
            "train loss: 0.645 accuracy: 0.675 [after 103 batches]\n",
            "val loss: 0.597 accuracy: 0.750 [after 24 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.606 accuracy: 0.685 [after 25 batches]\n",
            "train loss: 0.597 accuracy: 0.719 [after 51 batches]\n",
            "train loss: 0.580 accuracy: 0.724 [after 77 batches]\n",
            "train loss: 0.551 accuracy: 0.740 [after 103 batches]\n",
            "val loss: 0.484 accuracy: 0.842 [after 24 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.666 accuracy: 0.712 [after 25 batches]\n",
            "train loss: 0.604 accuracy: 0.723 [after 51 batches]\n",
            "train loss: 0.583 accuracy: 0.731 [after 77 batches]\n",
            "train loss: 0.576 accuracy: 0.736 [after 103 batches]\n",
            "val loss: 0.833 accuracy: 0.688 [after 24 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.655 accuracy: 0.688 [after 25 batches]\n",
            "train loss: 0.622 accuracy: 0.698 [after 51 batches]\n",
            "train loss: 0.571 accuracy: 0.732 [after 77 batches]\n",
            "train loss: 0.561 accuracy: 0.729 [after 103 batches]\n",
            "val loss: 0.606 accuracy: 0.633 [after 24 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.513 accuracy: 0.750 [after 25 batches]\n",
            "train loss: 0.548 accuracy: 0.744 [after 51 batches]\n",
            "train loss: 0.517 accuracy: 0.762 [after 77 batches]\n",
            "train loss: 0.520 accuracy: 0.766 [after 103 batches]\n",
            "val loss: 0.743 accuracy: 0.721 [after 24 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.471 accuracy: 0.785 [after 25 batches]\n",
            "train loss: 0.504 accuracy: 0.754 [after 51 batches]\n",
            "train loss: 0.502 accuracy: 0.769 [after 77 batches]\n",
            "train loss: 0.482 accuracy: 0.779 [after 103 batches]\n",
            "val loss: 0.620 accuracy: 0.704 [after 24 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.389 accuracy: 0.842 [after 25 batches]\n",
            "train loss: 0.438 accuracy: 0.806 [after 51 batches]\n",
            "train loss: 0.418 accuracy: 0.812 [after 77 batches]\n",
            "train loss: 0.419 accuracy: 0.813 [after 103 batches]\n",
            "val loss: 0.561 accuracy: 0.750 [after 24 batches]\n",
            "test accuracy: 0.805 recall: 0.979 precision: 0.778 f1: 0.862 [after 22 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▄▅▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▂▂▂▂▂▃▄▄▃▄▄▅▅▅▅▅▅▅▄▄▅▅▆▅▆▆▆▆▆▆█▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▇▇▇▇▇██▇▇▆▇▇▆▅▅▄▇▆▅▅▇▆▅▅▄▄▄▄▃▃▃▃▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▆▄▅█▃▁▄▃▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▂▄▃▁█▃▆▄▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.80455\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.86163\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.77751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.97944\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.81346\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.41898\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.75\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.5613\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mexalted-sweep-54\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/uofig8vk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_012732-uofig8vk/logs\u001b[0m\n",
            "2024-06-10 01:38:45,274 - wandb.wandb_agent - INFO - Cleaning up finished run: uofig8vk\n",
            "2024-06-10 01:38:47,724 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 01:38:47,724 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 51\n",
            "\tfold: 1\n",
            "\tlearning_rate: 0.0015206312455975765\n",
            "\tnormalize: True\n",
            "\toptimizer: sgd\n",
            "\tresize: 300\n",
            "2024-06-10 01:38:47,726 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=51 --fold=1 --learning_rate=0.0015206312455975765 --normalize=True --optimizer=sgd --resize=300\n",
            "2024-06-10 01:38:52,733 - wandb.wandb_agent - INFO - Running runs: ['mef9q0ru']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_013853-mef9q0ru\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mworldly-sweep-56\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/mef9q0ru\u001b[0m\n",
            "FOLD 1\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 187, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 124, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 113, in forward\n",
            "    x = self.triple_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\", line 175, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2509, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 68.00 MiB. GPU \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mworldly-sweep-56\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/mef9q0ru\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_013853-mef9q0ru/logs\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 63, in <module>\n",
            "    model_pipeline(default_config)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train_one_run.py\", line 53, in model_pipeline\n",
            "    train(model, train_loader, val_loader, criterion, optimizer, accuracy_fn, epochs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 94, in train\n",
            "    loss, accuracy = train_batch(images, labels, model, optimizer, criterion, accuracy_fn)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/train.py\", line 44, in train_batch\n",
            "    outputs = model(images)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1582, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 187, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 124, in forward\n",
            "    x = self.middle_flow(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/xception.py\", line 113, in forward\n",
            "    x = self.triple_depth_wise_conv(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\", line 175, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2509, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 68.00 MiB. GPU \n",
            "2024-06-10 01:39:18,091 - wandb.wandb_agent - INFO - Cleaning up finished run: mef9q0ru\n",
            "2024-06-10 01:39:20,433 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 01:39:20,433 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: False\n",
            "\tbatch_size: 22\n",
            "\tfold: 7\n",
            "\tlearning_rate: 0.00841723399097021\n",
            "\tnormalize: True\n",
            "\toptimizer: adam\n",
            "\tresize: 150\n",
            "2024-06-10 01:39:20,435 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=False --batch_size=22 --fold=7 --learning_rate=0.00841723399097021 --normalize=True --optimizer=adam --resize=150\n",
            "2024-06-10 01:39:25,446 - wandb.wandb_agent - INFO - Running runs: ['qng76c5i']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_013927-qng76c5i\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msolar-sweep-57\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/qng76c5i\u001b[0m\n",
            "FOLD 7\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 1.259 accuracy: 0.485 [after 11 batches]\n",
            "train loss: 1.006 accuracy: 0.523 [after 23 batches]\n",
            "train loss: 0.920 accuracy: 0.505 [after 35 batches]\n",
            "train loss: 0.879 accuracy: 0.511 [after 47 batches]\n",
            "val loss: 0.729 accuracy: 0.667 [after 11 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.603 accuracy: 0.549 [after 11 batches]\n",
            "train loss: 0.575 accuracy: 0.621 [after 23 batches]\n",
            "train loss: 0.536 accuracy: 0.687 [after 35 batches]\n",
            "train loss: 0.533 accuracy: 0.710 [after 47 batches]\n",
            "val loss: 0.656 accuracy: 0.667 [after 11 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.610 accuracy: 0.644 [after 11 batches]\n",
            "train loss: 0.650 accuracy: 0.617 [after 23 batches]\n",
            "train loss: 0.637 accuracy: 0.616 [after 35 batches]\n",
            "train loss: 0.611 accuracy: 0.634 [after 47 batches]\n",
            "val loss: 2.420 accuracy: 0.693 [after 11 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.606 accuracy: 0.739 [after 11 batches]\n",
            "train loss: 0.509 accuracy: 0.795 [after 23 batches]\n",
            "train loss: 0.462 accuracy: 0.821 [after 35 batches]\n",
            "train loss: 0.429 accuracy: 0.832 [after 47 batches]\n",
            "val loss: 0.345 accuracy: 0.804 [after 11 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.388 accuracy: 0.860 [after 11 batches]\n",
            "train loss: 0.291 accuracy: 0.898 [after 23 batches]\n",
            "train loss: 0.254 accuracy: 0.910 [after 35 batches]\n",
            "train loss: 0.241 accuracy: 0.914 [after 47 batches]\n",
            "val loss: 1.880 accuracy: 0.723 [after 11 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.132 accuracy: 0.962 [after 11 batches]\n",
            "train loss: 0.125 accuracy: 0.955 [after 23 batches]\n",
            "train loss: 0.136 accuracy: 0.947 [after 35 batches]\n",
            "train loss: 0.125 accuracy: 0.951 [after 47 batches]\n",
            "val loss: 1.007 accuracy: 0.739 [after 11 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.065 accuracy: 0.973 [after 11 batches]\n",
            "train loss: 0.093 accuracy: 0.972 [after 23 batches]\n",
            "train loss: 0.095 accuracy: 0.968 [after 35 batches]\n",
            "train loss: 0.092 accuracy: 0.970 [after 47 batches]\n",
            "val loss: 0.008 accuracy: 1.000 [after 11 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.055 accuracy: 0.977 [after 11 batches]\n",
            "train loss: 0.075 accuracy: 0.968 [after 23 batches]\n",
            "train loss: 0.077 accuracy: 0.968 [after 35 batches]\n",
            "train loss: 0.078 accuracy: 0.968 [after 47 batches]\n",
            "val loss: 0.028 accuracy: 0.992 [after 11 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.151 accuracy: 0.947 [after 11 batches]\n",
            "train loss: 0.231 accuracy: 0.920 [after 23 batches]\n",
            "train loss: 0.260 accuracy: 0.909 [after 35 batches]\n",
            "train loss: 0.289 accuracy: 0.896 [after 47 batches]\n",
            "val loss: 0.239 accuracy: 1.000 [after 11 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.302 accuracy: 0.902 [after 11 batches]\n",
            "train loss: 0.286 accuracy: 0.898 [after 23 batches]\n",
            "train loss: 0.319 accuracy: 0.872 [after 35 batches]\n",
            "train loss: 0.337 accuracy: 0.861 [after 47 batches]\n",
            "val loss: 0.214 accuracy: 1.000 [after 11 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.231 accuracy: 0.917 [after 11 batches]\n",
            "train loss: 0.181 accuracy: 0.934 [after 23 batches]\n",
            "train loss: 0.202 accuracy: 0.923 [after 35 batches]\n",
            "train loss: 0.178 accuracy: 0.935 [after 47 batches]\n",
            "val loss: 0.004 accuracy: 1.000 [after 11 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.039 accuracy: 0.992 [after 11 batches]\n",
            "train loss: 0.077 accuracy: 0.981 [after 23 batches]\n",
            "train loss: 0.081 accuracy: 0.976 [after 35 batches]\n",
            "train loss: 0.083 accuracy: 0.972 [after 47 batches]\n",
            "val loss: 0.103 accuracy: 0.945 [after 11 batches]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "train loss: 0.048 accuracy: 0.989 [after 11 batches]\n",
            "train loss: 0.034 accuracy: 0.991 [after 23 batches]\n",
            "train loss: 0.045 accuracy: 0.986 [after 35 batches]\n",
            "train loss: 0.044 accuracy: 0.986 [after 47 batches]\n",
            "val loss: 0.003 accuracy: 1.000 [after 11 batches]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "train loss: 0.017 accuracy: 0.996 [after 11 batches]\n",
            "train loss: 0.169 accuracy: 0.964 [after 23 batches]\n",
            "train loss: 0.161 accuracy: 0.955 [after 35 batches]\n",
            "train loss: 0.132 accuracy: 0.963 [after 47 batches]\n",
            "val loss: 0.023 accuracy: 0.992 [after 11 batches]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "train loss: 0.119 accuracy: 0.962 [after 11 batches]\n",
            "train loss: 0.091 accuracy: 0.972 [after 23 batches]\n",
            "train loss: 0.073 accuracy: 0.977 [after 35 batches]\n",
            "train loss: 0.063 accuracy: 0.980 [after 47 batches]\n",
            "val loss: 0.029 accuracy: 0.988 [after 11 batches]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "train loss: 0.103 accuracy: 0.973 [after 11 batches]\n",
            "train loss: 0.077 accuracy: 0.979 [after 23 batches]\n",
            "train loss: 0.060 accuracy: 0.984 [after 35 batches]\n",
            "train loss: 0.062 accuracy: 0.981 [after 47 batches]\n",
            "val loss: 0.089 accuracy: 0.967 [after 11 batches]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "train loss: 0.033 accuracy: 0.985 [after 11 batches]\n",
            "train loss: 0.033 accuracy: 0.989 [after 23 batches]\n",
            "train loss: 0.031 accuracy: 0.990 [after 35 batches]\n",
            "train loss: 0.028 accuracy: 0.991 [after 47 batches]\n",
            "val loss: 0.112 accuracy: 0.950 [after 11 batches]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "train loss: 0.015 accuracy: 0.992 [after 11 batches]\n",
            "train loss: 0.028 accuracy: 0.987 [after 23 batches]\n",
            "train loss: 0.036 accuracy: 0.987 [after 35 batches]\n",
            "train loss: 0.049 accuracy: 0.979 [after 47 batches]\n",
            "val loss: 0.962 accuracy: 0.833 [after 11 batches]\n",
            "test accuracy: 0.632 recall: 0.990 precision: 0.557 f1: 0.700 [after 10 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▂▁▃▄▃▃▄▆▆▇█▇▇████▇▇▇▆▇▇▇█████▇█████████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▆▄▄▅▄▄▄▃▂▂▂▂▁▁▁▁▂▂▃▃▂▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▂▄▂▃█████▇███▇▇▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▃▃█▂▆▄▁▁▂▂▁▁▁▁▁▁▁▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 71\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.63182\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.69984\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.55673\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 0.99\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.97917\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.04863\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.83347\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.96152\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msolar-sweep-57\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/qng76c5i\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_013927-qng76c5i/logs\u001b[0m\n",
            "2024-06-10 01:56:29,648 - wandb.wandb_agent - INFO - Cleaning up finished run: qng76c5i\n",
            "2024-06-10 01:56:31,913 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2024-06-10 01:56:31,913 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tarchitecture: xception\n",
            "\taugmented: True\n",
            "\tbatch_size: 63\n",
            "\tfold: 2\n",
            "\tlearning_rate: 0.005629341930013356\n",
            "\tnormalize: False\n",
            "\toptimizer: sgd\n",
            "\tresize: 50\n",
            "2024-06-10 01:56:31,915 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train_one_run.py --architecture=xception --augmented=True --batch_size=63 --fold=2 --learning_rate=0.005629341930013356 --normalize=False --optimizer=sgd --resize=50\n",
            "2024-06-10 01:56:36,923 - wandb.wandb_agent - INFO - Running runs: ['i761jl7r']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgpintoruiz\u001b[0m (\u001b[33maiuis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Thermal-Imaging-Breast-Cancer-Detection/notebooks/wandb/run-20240610_015637-i761jl7r\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdainty-sweep-59\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/sweeps/epbt9jh6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/i761jl7r\u001b[0m\n",
            "FOLD 2\n",
            "-------------------------------\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "train loss: 0.686 accuracy: 0.516 [after 3 batches]\n",
            "train loss: 0.659 accuracy: 0.560 [after 7 batches]\n",
            "train loss: 0.651 accuracy: 0.575 [after 11 batches]\n",
            "train loss: 0.659 accuracy: 0.572 [after 15 batches]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "val loss: 0.672 accuracy: 0.666 [after 4 batches]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "train loss: 0.570 accuracy: 0.667 [after 3 batches]\n",
            "train loss: 0.581 accuracy: 0.649 [after 7 batches]\n",
            "train loss: 0.562 accuracy: 0.663 [after 11 batches]\n",
            "train loss: 0.551 accuracy: 0.666 [after 15 batches]\n",
            "val loss: 0.635 accuracy: 0.672 [after 4 batches]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "train loss: 0.558 accuracy: 0.651 [after 3 batches]\n",
            "train loss: 0.561 accuracy: 0.657 [after 7 batches]\n",
            "train loss: 0.557 accuracy: 0.667 [after 11 batches]\n",
            "train loss: 0.540 accuracy: 0.676 [after 15 batches]\n",
            "val loss: 0.673 accuracy: 0.667 [after 4 batches]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "train loss: 0.569 accuracy: 0.683 [after 3 batches]\n",
            "train loss: 0.548 accuracy: 0.675 [after 7 batches]\n",
            "train loss: 0.551 accuracy: 0.676 [after 11 batches]\n",
            "train loss: 0.540 accuracy: 0.685 [after 15 batches]\n",
            "val loss: 0.374 accuracy: 0.850 [after 4 batches]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "train loss: 0.550 accuracy: 0.627 [after 3 batches]\n",
            "train loss: 0.507 accuracy: 0.683 [after 7 batches]\n",
            "train loss: 0.494 accuracy: 0.688 [after 11 batches]\n",
            "train loss: 0.514 accuracy: 0.676 [after 15 batches]\n",
            "val loss: 0.358 accuracy: 0.884 [after 4 batches]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "train loss: 0.541 accuracy: 0.635 [after 3 batches]\n",
            "train loss: 0.533 accuracy: 0.653 [after 7 batches]\n",
            "train loss: 0.523 accuracy: 0.661 [after 11 batches]\n",
            "train loss: 0.515 accuracy: 0.678 [after 15 batches]\n",
            "val loss: 0.411 accuracy: 0.797 [after 4 batches]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "train loss: 0.450 accuracy: 0.718 [after 3 batches]\n",
            "train loss: 0.519 accuracy: 0.708 [after 7 batches]\n",
            "train loss: 0.498 accuracy: 0.713 [after 11 batches]\n",
            "train loss: 0.504 accuracy: 0.702 [after 15 batches]\n",
            "val loss: 0.294 accuracy: 0.915 [after 4 batches]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "train loss: 0.518 accuracy: 0.683 [after 3 batches]\n",
            "train loss: 0.476 accuracy: 0.708 [after 7 batches]\n",
            "train loss: 0.496 accuracy: 0.704 [after 11 batches]\n",
            "train loss: 0.487 accuracy: 0.713 [after 15 batches]\n",
            "val loss: 0.272 accuracy: 0.883 [after 4 batches]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "train loss: 0.498 accuracy: 0.667 [after 3 batches]\n",
            "train loss: 0.487 accuracy: 0.657 [after 7 batches]\n",
            "train loss: 0.467 accuracy: 0.680 [after 11 batches]\n",
            "train loss: 0.469 accuracy: 0.684 [after 15 batches]\n",
            "val loss: 0.284 accuracy: 0.866 [after 4 batches]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "train loss: 0.412 accuracy: 0.774 [after 3 batches]\n",
            "train loss: 0.430 accuracy: 0.738 [after 7 batches]\n",
            "train loss: 0.444 accuracy: 0.735 [after 11 batches]\n",
            "train loss: 0.451 accuracy: 0.728 [after 15 batches]\n",
            "val loss: 0.641 accuracy: 0.831 [after 4 batches]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "train loss: 0.446 accuracy: 0.722 [after 3 batches]\n",
            "train loss: 0.471 accuracy: 0.710 [after 7 batches]\n",
            "train loss: 0.459 accuracy: 0.721 [after 11 batches]\n",
            "train loss: 0.453 accuracy: 0.716 [after 15 batches]\n",
            "val loss: 0.239 accuracy: 0.917 [after 4 batches]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "train loss: 0.427 accuracy: 0.726 [after 3 batches]\n",
            "train loss: 0.493 accuracy: 0.694 [after 7 batches]\n",
            "train loss: 0.471 accuracy: 0.690 [after 11 batches]\n",
            "train loss: 0.459 accuracy: 0.697 [after 15 batches]\n",
            "val loss: 0.270 accuracy: 0.917 [after 4 batches]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "train loss: 0.426 accuracy: 0.734 [after 3 batches]\n",
            "train loss: 0.430 accuracy: 0.708 [after 7 batches]\n",
            "train loss: 0.430 accuracy: 0.710 [after 11 batches]\n",
            "train loss: 0.419 accuracy: 0.713 [after 15 batches]\n",
            "val loss: 0.604 accuracy: 0.775 [after 4 batches]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "train loss: 0.414 accuracy: 0.702 [after 3 batches]\n",
            "train loss: 0.445 accuracy: 0.704 [after 7 batches]\n",
            "train loss: 0.443 accuracy: 0.714 [after 11 batches]\n",
            "train loss: 0.432 accuracy: 0.715 [after 15 batches]\n",
            "val loss: 0.053 accuracy: 1.000 [after 4 batches]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "train loss: 0.436 accuracy: 0.679 [after 3 batches]\n",
            "train loss: 0.462 accuracy: 0.708 [after 7 batches]\n",
            "train loss: 0.464 accuracy: 0.700 [after 11 batches]\n",
            "train loss: 0.459 accuracy: 0.701 [after 15 batches]\n",
            "val loss: 0.068 accuracy: 0.980 [after 4 batches]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "train loss: 0.442 accuracy: 0.679 [after 3 batches]\n",
            "train loss: 0.481 accuracy: 0.677 [after 7 batches]\n",
            "train loss: 0.452 accuracy: 0.709 [after 11 batches]\n",
            "train loss: 0.456 accuracy: 0.705 [after 15 batches]\n",
            "val loss: 0.664 accuracy: 0.708 [after 4 batches]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "train loss: 0.463 accuracy: 0.671 [after 3 batches]\n",
            "train loss: 0.441 accuracy: 0.694 [after 7 batches]\n",
            "train loss: 0.439 accuracy: 0.698 [after 11 batches]\n",
            "train loss: 0.442 accuracy: 0.694 [after 15 batches]\n",
            "val loss: 0.165 accuracy: 0.891 [after 4 batches]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "train loss: 0.471 accuracy: 0.738 [after 3 batches]\n",
            "train loss: 0.443 accuracy: 0.744 [after 7 batches]\n",
            "train loss: 0.464 accuracy: 0.714 [after 11 batches]\n",
            "train loss: 0.469 accuracy: 0.704 [after 15 batches]\n",
            "val loss: 0.049 accuracy: 0.980 [after 4 batches]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "train loss: 0.464 accuracy: 0.698 [after 3 batches]\n",
            "train loss: 0.446 accuracy: 0.712 [after 7 batches]\n",
            "train loss: 0.446 accuracy: 0.710 [after 11 batches]\n",
            "train loss: 0.436 accuracy: 0.715 [after 15 batches]\n",
            "val loss: 0.455 accuracy: 0.778 [after 4 batches]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "train loss: 0.440 accuracy: 0.675 [after 3 batches]\n",
            "train loss: 0.416 accuracy: 0.726 [after 7 batches]\n",
            "train loss: 0.424 accuracy: 0.721 [after 11 batches]\n",
            "train loss: 0.422 accuracy: 0.721 [after 15 batches]\n",
            "val loss: 0.574 accuracy: 0.799 [after 4 batches]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "train loss: 0.456 accuracy: 0.702 [after 3 batches]\n",
            "train loss: 0.475 accuracy: 0.700 [after 7 batches]\n",
            "train loss: 0.454 accuracy: 0.706 [after 11 batches]\n",
            "train loss: 0.439 accuracy: 0.719 [after 15 batches]\n",
            "val loss: 0.094 accuracy: 0.972 [after 4 batches]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "train loss: 0.381 accuracy: 0.750 [after 3 batches]\n",
            "train loss: 0.419 accuracy: 0.718 [after 7 batches]\n",
            "train loss: 0.427 accuracy: 0.712 [after 11 batches]\n",
            "train loss: 0.422 accuracy: 0.706 [after 15 batches]\n",
            "val loss: 0.182 accuracy: 0.876 [after 4 batches]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "train loss: 0.439 accuracy: 0.694 [after 3 batches]\n",
            "train loss: 0.409 accuracy: 0.714 [after 7 batches]\n",
            "train loss: 0.399 accuracy: 0.726 [after 11 batches]\n",
            "train loss: 0.406 accuracy: 0.729 [after 15 batches]\n",
            "val loss: 0.077 accuracy: 0.958 [after 4 batches]\n",
            "test accuracy: 0.924 recall: 1.000 precision: 0.897 f1: 0.946 [after 4 batches]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▆▅▅▆▆▄▆▅▆▇▆▇▆▆█▇▇▇▆▇▇▇▆▆▆▇▆▆▇▆▇▇▇▇█▇▆▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▇▅▅▅▅▅▅▄▄▄▄▄▄▄▃▂▃▃▂▃▂▂▂▂▃▂▃▂▂▃▃▂▂▂▃▁▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▁▅▆▄▆▆▅▄▆▆▃██▂▆█▃▄▇▅▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ███▅▄▅▄▄▄█▃▃▇▁▁█▂▁▆▇▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           step 91\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test_accuracy 0.92435\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        test_f1 0.94568\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test_precision 0.89749\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_recall 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.72917\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.40566\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.95845\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.07714\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdainty-sweep-59\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project/runs/i761jl7r\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/aiuis/dip-project\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240610_015637-i761jl7r/logs\u001b[0m\n",
            "2024-06-10 02:20:31,634 - wandb.wandb_agent - INFO - Cleaning up finished run: i761jl7r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Terminating and syncing runs. Press ctrl-c to kill.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}